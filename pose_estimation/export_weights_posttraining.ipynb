{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='5'\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import _init_paths\n",
    "from core.config import config\n",
    "from core.config import update_config\n",
    "from core.config import update_dir\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models\n",
    "\n",
    "import quantize_dorefa\n",
    "from quantize_iao import *\n",
    "# from quantize_iao_uint import *  #对feature map进行uint对称量化\n",
    "\n",
    "import numpy as np\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "np.set_printoptions(threshold = np.inf) \n",
    "\n",
    "# # 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_conv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConv2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv\n",
    "\n",
    "def bn_fuse_deconv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConvTranspose2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         output_padding=bn_conv.output_padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_module(module, device):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, QuantBNFuseConv2d):\n",
    "            bn_fused_conv = bn_fuse_conv(child, device)\n",
    "            module._modules[name] = bn_fused_conv\n",
    "        elif isinstance(child, QuantBNFuseConvTranspose2d):\n",
    "            bn_fused_deconv = bn_fuse_deconv(child, device)\n",
    "            module._modules[name] = bn_fused_deconv\n",
    "        else:\n",
    "            bn_fuse_module(child, device)\n",
    "\n",
    "\n",
    "def model_bn_fuse(model, inplace=False):\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "    device = next(model.parameters()).device\n",
    "    bn_fuse_module(model,device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device='', apex=False, batch_size=None):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    cpu_request = device.lower() == 'cpu'\n",
    "    if device and not cpu_request:  # if device requested other than 'cpu'\n",
    "        # os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity\n",
    "\n",
    "    cuda = False if cpu_request else torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        c = 1024 ** 2  # bytes to MB\n",
    "        ng = torch.cuda.device_count()\n",
    "        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
    "            assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)\n",
    "        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n",
    "        s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex\n",
    "        for i in range(0, ng):\n",
    "            if i == 1:\n",
    "                s = ' ' * len(s)\n",
    "            print(\"%sdevice%g _CudaDeviceProperties(name='%s', total_memory=%dMB)\" %\n",
    "                  (s, i, x[i].name, x[i].total_memory / c))\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    print('')  # skip a line\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "models.pose_mobilenet_relu.get_pose_net\n",
      "/home/ytwang/wyt_workspace/quantization/human-pose-estimation.pytorch/pose_estimation/../lib/core/config.py:196: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  exp_config = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "cfg='../experiments/coco/resnet50/mobile_quant_relu_int.yaml'   #MODEL_FILE:'output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt' 生成 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "update_config(cfg)\n",
    "# cudnn related setting\n",
    "cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n",
    "\n",
    "# for shufflenetv2\n",
    "shufflenetv2_spec = {'0.5': ([4, 8, 4], [24, 48, 96, 192, 1024]),\n",
    "                        '1.0': ([4, 8, 4], [24, 116, 232, 464, 1024]),\n",
    "                        '1.5': ([4, 8, 4], [24, 176, 352, 704, 1024]),\n",
    "                        '2.0': ([4, 8, 4], [24, 244, 488, 976, 2048])}\n",
    "stages_repeats, stages_out_channels = shufflenetv2_spec['1.0']\n",
    "print('models.'+config.MODEL.NAME+'.get_pose_net')\n",
    "model = eval('models.'+config.MODEL.NAME+'.get_pose_net')(\n",
    "        config, \n",
    "        stages_repeats, stages_out_channels,\n",
    "        is_train=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################### bnfuse model ############################################\n",
    "bnfuse_model = eval('models.pose_mobilenet_relu_bnfuse.get_pose_net')(\n",
    "    config, \n",
    "    stages_repeats, stages_out_channels,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2080 Ti', total_memory=11019MB)\n\n"
     ]
    }
   ],
   "source": [
    "gpus = [int(i) for i in config.GPUS.split(',')]\n",
    "device = select_device(config.GPUS, batch_size=config.TEST.BATCH_SIZE*len(gpus))\n",
    "\n",
    "model = model.to(device)\n",
    "# summary(model,input_size=(3, 256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a_bits= 8 \tw_bits= 8 \tq_type= 0 \tq_level= 0 \tdevice= cuda:0 \tweight_observer= 0 \tbn_fuse= 1 \tquant_inference= False\n"
     ]
    }
   ],
   "source": [
    "#print('*******************ori_model*******************\\n', model)\n",
    "if(config.QUANTIZATION.QUANT_METHOD == 1): # DoReFa\n",
    "    quantize_dorefa.prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS, quant_inference=config.QUANTIZATION.QUANT_INFERENCE, is_activate=False)\n",
    "else: #default quant_method == 0   IAO\n",
    "    prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS,q_type=config.QUANTIZATION.Q_TYPE, q_level=config.QUANTIZATION.Q_LEVEL, device=device,#device=next(model.parameters()).device, \n",
    "                        weight_observer=config.QUANTIZATION.WEIGHT_OBSERVER, bn_fuse=config.QUANTIZATION.BN_FUSE, quant_inference=config.QUANTIZATION.QUANT_INFERENCE)\n",
    "#print('\\n*******************quant_model*******************\\n', model)\n",
    "# print('\\n*******************Using quant_model in test*******************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.TEST.MODEL_FILE:\n",
    "#     # logger.info('=> loading model from {}'.format(config.TEST.MODEL_FILE))\n",
    "#     if(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint.pth.tar'):\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         #model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=torch.device('cuda'))['state_dict'])\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device)['state_dict'])\n",
    "#         #torch.save(model.module.state_dict(), 'output/coco_quan/mobile_quant_relu_w8a8_bnfuse0/checkpoint_nomodule.pth.tar')\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='model_best.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint_resave.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     else:  #final_state.pth.tar\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*******************For inference bn_fuse quant_model*******************\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# ********************* quant_bn_fused_model_inference **********************\n",
    "model.to(device)\n",
    "model_bn_fuse(model, inplace=True)  # bn融合\n",
    "# print('\\n*******************For inference bn_fuse quant_model*******************\\n', model)\n",
    "# ckpt = {'model': model.module.state_dict() if hasattr(model, 'module') else model.state_dict()}\n",
    "# torch.save(ckpt, '../output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt')\n",
    "print('*******************For inference bn_fuse quant_model*******************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "print(config.TEST.MODEL_FILE)\n",
    "model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device)['model'])  ##为什么还在'model'里面呀？\n",
    "# model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device))  ##为什么还在'model'里面呀？\n",
    "# model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model.state_dict:\n"
     ]
    }
   ],
   "source": [
    "remapped_state = {}\n",
    "print('Model.state_dict:')\n",
    "# ######################################## before #######################################\n",
    "# for n,param_tensor in enumerate(model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     # if(n<5):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     # print(model.state_dict()[param_tensor])\n",
    "\n",
    "# ######################################### after #######################################\n",
    "# for n,param_tensor in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',bnfuse_model.state_dict()[param_tensor].size())\n",
    "#     # if(n<4):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     print(model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor后 #############################\n",
    "# import re\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]=='features'):\n",
    "#         k[1]=str(int(k[1])+1)\n",
    "#         k[-2]=str((int(k[2][-1])-1)*3)\n",
    "#         k[2]='conv'\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     elif(k[0].startswith('deconv_layers')): #final_layer\n",
    "#         number=3*int(k[0][-1])\n",
    "#         remapped_state_key='deconv_layers.'+str(number)+'.'+k[-1] #weight/bias\n",
    "#     elif(k[0]=='conv1'): #final_layer\n",
    "#         remapped_state_key='features.0.0.'+k[-1] #weight/bias\n",
    "#     else: #final_layer  conv2\n",
    "#         remapped_state_key=state_key\n",
    "#     # print(n, state_key, remapped_state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor前 #############################\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]!='final_layer'):\n",
    "#         number = int(k[-2])//2*3 \n",
    "#         # print(number)\n",
    "#         k[-2]=str(number)\n",
    "#         # print(k)\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     else: #final_layer\n",
    "#         remapped_state_key=state_key\n",
    "#     print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 量化步骤 ##############################\n",
    "## 对称量化，因此zero_point均为0\n",
    "def quantize_tensor(weight, bias, wscale, ascale, num_bits=8):\n",
    "    qmin = -2**(num_bits-1)  #8bit [-128,127]\n",
    "    qmax = 2**(num_bits-1) - 1\n",
    "    q_weight= torch.round(weight/wscale).clamp_(qmin, qmax).type(torch.int32)\n",
    "    # print('0:', torch.max(torch.round(weight/wscale)), torch.min(torch.round(weight/wscale)))\n",
    "    q_bias= torch.round(bias/wscale.flatten()/ascale).type(torch.int32)  #bias没有进行截断\n",
    "    return q_weight, q_bias\n",
    "\n",
    "#反量化回浮点结果\n",
    "def dequantize_tensor(q_weight, q_bias, wscale, ascale):\n",
    "    return wscale*q_weight.float(), wscale.flatten()*ascale*q_bias.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model.state_dict:\nfeatures.0.0.weight features.0.2\ntorch.Size([16, 3, 3, 3]) conv_bias= torch.Size([16]) \nascale= tensor([0.0207]) torch.Size([1]) \nwscale= tensor([3.2806e-02, 3.5383e-07, 6.9110e-03, 1.2938e-02, 1.1094e-02, 4.6062e-02,\n        5.6726e-02, 4.9625e-02, 2.8893e-02, 2.8007e-05, 1.7617e-02, 1.5632e-06,\n        2.1120e-02, 2.3379e-07, 1.3260e-06, 4.4465e-02]) torch.Size([16, 1, 1, 1]) \noscale= tensor([0.9272]) torch.Size([1])\n###: M= tensor([7.3260e-04, 7.9017e-09, 1.5433e-04, 2.8893e-04, 2.4775e-04, 1.0286e-03,\n        1.2668e-03, 1.1082e-03, 6.4523e-04, 6.2544e-07, 3.9341e-04, 3.4910e-08,\n        4.7164e-04, 5.2209e-09, 2.9611e-08, 9.9298e-04])\ntorch.Size([16, 3, 3, 3]) torch.Size([16])\nconv_weight: tensor([[[[-1.7921e-01,  7.3493e-01,  2.8327e-01],\n          [ 3.1658e-02,  1.0855e+00,  2.7640e-01],\n          [ 1.1607e-01,  5.1940e-01,  3.0053e-01]],\n\n         [[-2.2042e-01, -4.3684e-01, -1.0990e+00],\n          [-1.3602e+00, -1.8952e+00, -4.0465e+00],\n          [-6.7470e-01,  1.8189e-01, -2.2336e+00]],\n\n         [[-1.7365e-01,  6.1672e-01,  3.5628e-01],\n          [-6.1048e-01,  2.9419e+00,  1.7364e+00],\n          [ 1.7706e-01,  3.3205e+00,  2.1474e+00]]],\n\n\n        [[[-1.4822e-05, -9.5277e-06, -1.7073e-05],\n          [-1.7501e-05, -3.0215e-05,  3.4911e-06],\n          [-2.7478e-05, -3.1973e-05,  1.0358e-05]],\n\n         [[ 5.6683e-08, -6.2256e-07,  6.6703e-06],\n          [-4.2640e-06, -2.3636e-05,  4.5760e-06],\n          [-7.8695e-06,  1.8608e-07,  4.2223e-06]],\n\n         [[ 3.2503e-05,  4.4813e-05,  3.3683e-05],\n          [ 1.5486e-05,  2.8259e-05,  2.5988e-05],\n          [ 1.3538e-05,  1.3542e-05,  4.2030e-05]]],\n\n\n        [[[ 6.4662e-02, -2.8969e-01, -1.8041e-01],\n          [-1.7222e-02, -8.4645e-01, -4.7034e-01],\n          [ 2.5451e-01, -1.6757e-01,  2.9974e-01]],\n\n         [[ 3.5879e-01,  2.0144e-01,  2.2877e-01],\n          [ 6.1542e-02, -7.8918e-01, -4.4781e-01],\n          [ 1.8424e-01,  1.0639e-01,  3.1992e-01]],\n\n         [[ 1.6270e-01,  9.2135e-02, -1.9797e-01],\n          [-5.6690e-02, -6.2024e-01, -4.3444e-01],\n          [-2.1235e-01, -4.9089e-01, -4.8161e-01]]],\n\n\n        [[[ 7.5742e-03, -2.5600e-01,  3.0317e-01],\n          [ 9.0896e-02, -2.0139e-01, -3.8204e-01],\n          [-8.3283e-02, -4.3013e-01, -5.8019e-01]],\n\n         [[ 3.6565e-02,  1.8387e-01,  1.1891e+00],\n          [ 4.3470e-01, -3.6708e-01, -7.9309e-01],\n          [ 2.9509e-01, -1.1274e+00, -1.5698e+00]],\n\n         [[-1.2066e-01,  2.8764e-01,  1.0456e+00],\n          [ 5.1799e-01, -2.0708e-01, -7.6669e-01],\n          [ 4.8074e-01, -8.9327e-01, -1.5837e+00]]],\n\n\n        [[[ 4.9103e-02, -1.0235e-01, -1.8420e-01],\n          [ 8.5128e-02,  2.2466e-01,  9.0830e-02],\n          [ 8.3695e-02,  5.6890e-02, -5.6582e-02]],\n\n         [[-6.6944e-01, -2.4013e-01, -2.0062e-01],\n          [-3.6079e-01,  1.1671e+00,  1.2605e+00],\n          [-4.9231e-01,  1.3005e-01,  4.6960e-02]],\n\n         [[ 5.9191e-01,  1.6689e-01,  8.0086e-01],\n          [ 2.6713e-01, -8.7909e-01, -7.3806e-01],\n          [ 7.3439e-01, -2.7486e-01,  2.3233e-01]]],\n\n\n        [[[-1.6108e-01, -1.8334e-01,  1.1788e-01],\n          [-5.1640e-01, -1.1388e+00, -1.8923e-01],\n          [ 6.8418e-01,  7.1658e-01,  3.2529e-01]],\n\n         [[-4.8550e-01, -3.3012e-01,  1.3311e+00],\n          [-5.6245e-01, -5.5642e+00, -1.4566e+00],\n          [ 2.0358e+00,  2.4543e+00,  2.8901e+00]],\n\n         [[ 9.8229e-03, -3.3810e-01,  6.9166e-01],\n          [-7.7013e-01, -4.0775e+00, -1.4067e+00],\n          [ 2.0891e+00,  1.2382e+00,  1.6708e+00]]],\n\n\n        [[[ 1.1997e+00, -2.0407e-01, -9.8837e-01],\n          [ 2.0941e+00,  2.6897e-02, -1.8246e+00],\n          [ 1.3779e+00, -5.4085e-02, -1.6043e+00]],\n\n         [[ 2.3261e+00,  4.5013e-01, -2.4839e+00],\n          [ 2.7801e+00,  2.2091e+00, -6.9971e+00],\n          [ 1.6253e+00,  2.9629e+00, -2.9562e+00]],\n\n         [[ 1.9762e+00,  4.0773e-01, -2.0454e+00],\n          [ 2.2640e+00,  1.9361e+00, -4.6838e+00],\n          [ 1.0588e+00,  2.0956e+00, -2.7309e+00]]],\n\n\n        [[[-1.1733e+00, -2.7729e+00, -1.7499e+00],\n          [ 3.5866e-01,  8.2942e-01,  2.6163e-01],\n          [ 8.7565e-01,  2.1464e+00,  1.2149e+00]],\n\n         [[-2.7854e+00, -6.0865e+00, -4.0350e+00],\n          [ 6.7674e-01,  2.2204e+00,  1.1464e+00],\n          [ 1.5138e+00,  4.7692e+00,  2.5181e+00]],\n\n         [[-1.9750e+00, -3.2870e+00, -2.2396e+00],\n          [ 1.4092e-01,  9.3969e-01,  2.3869e-01],\n          [ 1.4179e+00,  3.1532e+00,  1.6456e+00]]],\n\n\n        [[[-4.7871e-01, -8.7981e-01, -1.2766e+00],\n          [-8.1388e-01, -1.0796e+00, -1.8633e+00],\n          [-4.7098e-01, -1.3827e+00, -1.8523e+00]],\n\n         [[ 1.5711e-01,  1.4340e+00,  1.1901e+00],\n          [ 1.8079e-02,  2.8383e+00,  1.8065e+00],\n          [ 4.2325e-01,  3.5249e+00,  2.3787e+00]],\n\n         [[-1.4742e-01,  1.6849e-02, -3.0093e-01],\n          [-4.2318e-01, -3.8822e-01, -1.3402e+00],\n          [-9.6164e-02, -1.3735e-02, -5.6644e-01]]],\n\n\n        [[[-1.6447e-03, -1.8409e-03, -5.1892e-04],\n          [-1.7787e-03, -2.0455e-03, -2.0477e-03],\n          [-6.2765e-04, -8.3364e-04,  4.8243e-04]],\n\n         [[ 1.2635e-03, -4.9726e-04,  8.7828e-04],\n          [-5.3243e-04, -3.4650e-03, -2.4240e-03],\n          [ 1.1583e-03, -1.0884e-03,  1.0517e-03]],\n\n         [[ 1.4506e-03, -1.7678e-04,  2.0314e-03],\n          [-5.6690e-04, -3.1178e-03, -1.3729e-03],\n          [ 7.1481e-04, -9.1992e-04,  1.1404e-03]]],\n\n\n        [[[ 3.1301e-01,  1.2201e+00,  8.2837e-01],\n          [ 5.3810e-01,  2.0795e+00,  1.4958e+00],\n          [ 5.6250e-01,  2.1517e+00,  1.5210e+00]],\n\n         [[-2.5055e-01,  1.2587e-01, -2.1229e-01],\n          [-1.5278e-01,  5.7696e-01, -1.4731e-01],\n          [-2.2991e-01,  5.4600e-01,  1.5077e-02]],\n\n         [[-3.5617e-01, -6.5537e-01, -6.3880e-01],\n          [-1.0006e+00, -1.3598e+00, -1.8171e+00],\n          [-4.9149e-01, -8.6150e-01, -1.3855e+00]]],\n\n\n        [[[ 2.5149e-05, -1.2261e-05,  7.7667e-07],\n          [ 6.7261e-05, -1.6265e-05,  1.9869e-05],\n          [-1.4543e-05, -1.6220e-04, -1.0762e-04]],\n\n         [[ 3.6989e-05, -2.6363e-05, -5.7226e-05],\n          [ 6.0943e-05, -7.7950e-06,  1.4932e-05],\n          [-8.5327e-05, -1.9436e-04, -1.4946e-04]],\n\n         [[ 1.5198e-05, -1.3461e-05, -1.3127e-05],\n          [ 3.2630e-05,  6.8084e-07,  2.0728e-05],\n          [-2.5122e-05, -1.0187e-04, -6.5982e-05]]],\n\n\n        [[[-2.2736e-01, -2.8255e-01,  2.4754e-01],\n          [ 1.4710e-01,  2.5922e-01,  7.8402e-01],\n          [ 8.1816e-02,  1.0730e-02,  3.7887e-01]],\n\n         [[-4.5854e-01, -1.2932e+00, -5.6359e-01],\n          [ 4.6283e-01,  1.8193e+00,  2.2658e+00],\n          [-7.8963e-01, -1.2513e+00,  5.5860e-02]],\n\n         [[-6.1207e-01, -1.2115e+00, -8.2625e-01],\n          [ 5.9614e-01,  1.8612e+00,  2.2263e+00],\n          [-3.5473e-01, -2.3251e-01,  7.4126e-01]]],\n\n\n        [[[-1.7082e-05, -6.7861e-06, -2.9466e-05],\n          [ 5.6829e-06, -1.5180e-05, -2.4663e-05],\n          [-1.7183e-05, -1.1255e-05, -1.0335e-05]],\n\n         [[-5.0465e-06, -1.7906e-05, -1.3794e-05],\n          [ 1.2852e-05,  8.4162e-06, -2.8190e-06],\n          [-1.1875e-05,  5.2404e-06, -4.9197e-06]],\n\n         [[-1.0886e-05, -1.7122e-05, -1.2608e-05],\n          [-6.1066e-06, -2.7529e-05,  1.3082e-06],\n          [-1.8349e-05, -7.3425e-06, -1.0883e-05]]],\n\n\n        [[[ 3.4470e-05,  2.8592e-05,  6.1130e-06],\n          [-3.4086e-05, -1.0966e-04, -1.5394e-04],\n          [ 4.3749e-05,  1.2475e-05, -2.2999e-05]],\n\n         [[ 1.6148e-05, -2.1611e-06, -4.5601e-05],\n          [-7.8785e-05, -1.2939e-04, -1.6481e-04],\n          [-2.2955e-06, -7.6295e-06, -4.5257e-05]],\n\n         [[ 1.2335e-05, -9.2498e-06, -1.2562e-05],\n          [-3.9911e-05, -4.7529e-05, -8.9187e-05],\n          [-3.7376e-06,  2.1035e-05,  6.6711e-06]]],\n\n\n        [[[-1.7460e-01,  4.8523e-01,  6.0815e-01],\n          [ 4.5948e-01,  1.5254e-01,  5.1088e-01],\n          [ 4.6595e-01,  5.1054e-02,  9.5947e-02]],\n\n         [[ 6.5237e-01,  7.2784e-01,  7.3151e-01],\n          [ 7.8388e-01, -2.9308e+00, -2.0079e+00],\n          [-9.7896e-02, -5.2494e+00, -3.7617e+00]],\n\n         [[ 1.5342e-01, -3.1574e-01, -1.8108e-02],\n          [ 5.5608e-01, -6.2295e-01,  7.4962e-01],\n          [ 5.4856e-01, -5.7621e-01,  1.1804e+00]]]]) conv_bias: tensor([ 1.8194e+00, -4.6134e-04, -5.0178e+00,  6.3187e+00, -3.0287e+00,\n         5.9636e+00,  5.1741e+00,  3.8572e+00,  3.8592e+00, -7.6353e-02,\n         1.5772e+01, -3.8195e-03,  7.8675e+00, -2.0567e-04, -3.6811e-03,\n        -4.1723e+00])\ntorch.Size([16, 3, 3, 3]) torch.Size([16])\ntensor([  -5,   22,    9,    1,   33,    8,    4,   16,    9,   -7,  -13,  -33,\n         -41,  -58, -123,  -21,    6,  -68,   -5,   19,   11,  -19,   90,   53,\n           5,  101,   65,  -42,  -27,  -48,  -49,  -85,   10,  -78,  -90,   29,\n           0,   -2,   19,  -12,  -67,   13,  -22,    1,   12,   92,  127,   95,\n          44,   80,   73,   38,   38,  119,    9,  -42,  -26,   -2, -122,  -68,\n          37,  -24,   43,   52,   29,   33,    9, -114,  -65,   27,   15,   46,\n          24,   13,  -29,   -8,  -90,  -63,  -31,  -71,  -70,    1,  -20,   23,\n           7,  -16,  -30,   -6,  -33,  -45,    3,   14,   92,   34,  -28,  -61,\n          23,  -87, -121,   -9,   22,   81,   40,  -16,  -59,   37,  -69, -122,\n           4,   -9,  -17,    8,   20,    8,    8,    5,   -5,  -60,  -22,  -18,\n         -33,  105,  114,  -44,   12,    4,   53,   15,   72,   24,  -79,  -67,\n          66,  -25,   21,   -3,   -4,    3,  -11,  -25,   -4,   15,   16,    7,\n         -11,   -7,   29,  -12, -121,  -32,   44,   53,   63,    0,   -7,   15,\n         -17,  -89,  -31,   45,   27,   36,   21,   -4,  -17,   37,    0,  -32,\n          24,   -1,  -28,   41,    8,  -44,   49,   39, -123,   29,   52,  -52,\n          35,    7,  -36,   40,   34,  -83,   19,   37,  -48,  -24,  -56,  -35,\n           7,   17,    5,   18,   43,   24,  -56, -123,  -81,   14,   45,   23,\n          31,   96,   51,  -40,  -66,  -45,    3,   19,    5,   29,   64,   33,\n         -17,  -30,  -44,  -28,  -37,  -64,  -16,  -48,  -64,    5,   50,   41,\n           1,   98,   63,   15,  122,   82,   -5,    1,  -10,  -15,  -13,  -46,\n          -3,    0,  -20,  -59,  -66,  -19,  -64,  -73,  -73,  -22,  -30,   17,\n          45,  -18,   31,  -19, -124,  -87,   41,  -39,   38,   52,   -6,   73,\n         -20, -111,  -49,   26,  -33,   41,   18,   69,   47,   31,  118,   85,\n          32,  122,   86,  -14,    7,  -12,   -9,   33,   -8,  -13,   31,    1,\n         -20,  -37,  -36,  -57,  -77, -103,  -28,  -49,  -79,   16,   -8,    0,\n          43,  -10,   13,   -9, -104,  -69,   24,  -17,  -37,   39,   -5,   10,\n         -55, -124,  -96,   10,   -9,   -8,   21,    0,   13,  -16,  -65,  -42,\n         -11,  -13,   12,    7,   12,   37,    4,    1,   18,  -22,  -61,  -27,\n          22,   86,  107,  -37,  -59,    3,  -29,  -57,  -39,   28,   88,  105,\n         -17,  -11,   35,  -73,  -29, -126,   24,  -65, -105,  -73,  -48,  -44,\n         -22,  -77,  -59,   55,   36,  -12,  -51,   22,  -21,  -47,  -73,  -54,\n         -26, -118,    6,  -78,  -31,  -47,   26,   22,    5,  -26,  -83, -116,\n          33,    9,  -17,   12,   -2,  -34,  -59,  -98, -124,   -2,   -6,  -34,\n           9,   -7,   -9,  -30,  -36,  -67,   -3,   16,    5,   -4,   11,   14,\n          10,    3,   11,   10,    1,    2,   15,   16,   16,   18,  -66,  -45,\n          -2, -118,  -85,    3,   -7,    0,   13,  -14,   17,   12,  -13,   27],\n       dtype=torch.int32) tensor([   2678,  -62969,  -35066,   23586,  -13185,    6253,    4405,    3754,\n           6451, -131665,   43238, -118002,   17991,  -42486, -134076,   -4532],\n       dtype=torch.int32)\nweight: tensor(-0.0004) tensor(0.0269)   bias tensor(4.6534e-06) tensor(0.0003)\n['conv1']\n"
     ]
    }
   ],
   "source": [
    "############################################################## 定点结果导出 refactor后 ##############################################\n",
    "remapped_state = {}\n",
    "M_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M_key=[]  #存储M_list中的索引键值\n",
    "oscale_list={} #储存 浮点和整型的oscale\n",
    "ascale_list={} #储存 浮点和整型的ascale\n",
    "wscale_list={} #储存 浮点和整型的wscale\n",
    "\n",
    "count=0\n",
    "#导出权重和偏置至二进制文件中\n",
    "print('Model.state_dict:')\n",
    "for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "    #打印 key value字典\n",
    "    # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "    if(n==0): #if(n<4*2): if(n<=434):  n==16 or n==8\n",
    "        # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "        ############################################## 以下得到浮点权重值 ###################################################\n",
    "        if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "            layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "            next_layer=''\n",
    "            # print(layer_name+'.weight')\n",
    "            conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "            ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "            wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "            if(param_key.split('.')[0]!='final_layer'):\n",
    "                conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "                tmp=param_key.split('.')[0:-1]\n",
    "                tmp[-1]=str(int(tmp[-1])+2)\n",
    "                next_layer=('.').join(tmp)\n",
    "                print(param_key, next_layer)\n",
    "            else:\n",
    "                print('It is final_layer')\n",
    "                print(conv_weight.shape)\n",
    "            \n",
    "            if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "                oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况 包括网络第一层、InvertedResidual中包含relu的层、conv2以及deconv_layers\n",
    "                # print('0:',next_layer, oscale)\n",
    "            elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet 每个InvertedResidual模块的末尾（线性直通，无relu)   但features.17的oscale应该是conv2的ascale\n",
    "                tmp2=param_key.split('.')[0:-1]\n",
    "                tmp2[1]=str(int(tmp[1])+1)\n",
    "                tmp2[-1]='0'\n",
    "                next_layer=('.').join(tmp2)\n",
    "                oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "                # print('1:',next_layer,oscale)\n",
    "            elif(param_key.split('.')[1]=='17'): #features.17的oscale应该是final_layer的ascale\n",
    "                oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "                # print('2: conv2.0',oscale)\n",
    "            else:  #final_layer 不需要oscale    next_layer=='' 没有relu的都不需要oscale\n",
    "                oscale=torch.tensor([1.])\n",
    "                # print('3: final_layer',oscale)\n",
    "            print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(), wscale.shape, '\\noscale=',oscale,oscale.shape) #wscale.flatten()\n",
    "\n",
    "            ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "            if(param_key.split('.')[0]!='final_layer'):\n",
    "                #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "                # if(param_key.split('.')[0]=='features'and param_key.split('.')[1]=='0'): #第一层的M需要特殊处理，将输入ascale放入图像预处理中实现，输入网络的数据直接是[-128,127] 似乎又不需要...\n",
    "                #     M=wscale/oscale\n",
    "                # else:\n",
    "                #     M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "                M=wscale*ascale/oscale\n",
    "                # M0=(M*2**16).type(torch.int32)\n",
    "                # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n",
    "                print('###: M=',M.flatten())\n",
    "\n",
    "                print(conv_weight.shape, conv_bias.shape)\n",
    "                #*********************************************************************************************************************************\n",
    "                print('conv_weight:',conv_weight.squeeze(), 'conv_bias:',conv_bias.flatten())\n",
    "                \n",
    "                #计算权重和偏置int量化结果\n",
    "                q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "                print(q_weight.shape, q_bias.shape)\n",
    "                print(q_weight.flatten(), q_bias.flatten())\n",
    "                \n",
    "                #反量化回浮点数的结果\n",
    "                dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "                # print(dq_weight, dq_bias)\n",
    "                print('weight:', torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight),'  bias', torch.mean(conv_bias-dq_bias), torch.max(conv_bias-dq_bias))\n",
    "                # print(conv_bias-dq_bias)\n",
    "            else: #final_layer\n",
    "                #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "                M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "                # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "                # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "                print('final_layer: M=',M.flatten())\n",
    "\n",
    "                #计算权重和偏置int量化结果\n",
    "                q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "                # print(q_weight, q_bias)\n",
    "                # print(q_bias.shape)\n",
    "                #反量化回浮点数的结果\n",
    "                dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "                # print(dq_weight, dq_bias)\n",
    "                print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "                # print(conv_bias-dq_bias)\n",
    "\n",
    "            # ################################################### 浮点权重导出 ################################################################\n",
    "            # q_weight=conv_weight #浮点权重的导出，需要控制变量呀！！！\n",
    "            # q_bias=conv_bias\n",
    "            # ################################################################################################################################\n",
    "\n",
    "            ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "            k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "            if(k[0]=='features' and k[1]=='0'): # 网络第一层  features.0. -> conv1\n",
    "                remapped_state_key = 'conv1.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            elif(k[0]=='features'): #除了第一层外的其他层\n",
    "                k[1]=str(int(k[1])-1)\n",
    "                number = int(k[-2])//3 + 1 \n",
    "                # print(number)\n",
    "                k[2]=k[2]+str(number)\n",
    "                # k[3]='0' #现在不需要了\n",
    "                # print(k)\n",
    "                remapped_state_key=('.').join(k[0:3])+'.weight' #进行重映射\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='deconv_layers'): #deconv_layers0-5\n",
    "                number = int(k[-2])//3\n",
    "                remapped_state_key=k[0]+str(number)+'.weight' #进行重映射\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[k[0]+str(number)+'.bias']=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='final_layer'): #final_layer\n",
    "                remapped_state_key=param_key\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "            else: #conv2 无需进行重映射\n",
    "                remapped_state_key = 'conv2.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "            # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "            if(remapped_state_key.split('.')[0]=='features'):\n",
    "                M_key_name = '.'.join(remapped_state_key.split('.')[0:-1])\n",
    "            else:\n",
    "                M_key_name = remapped_state_key.split('.')[0]\n",
    "            M_key.append(M_key_name)\n",
    "            M_list[M_key_name] = torch.squeeze(M, dim=-1) #M.flatten() torch.Size([16]);  M通道量化torch.Size([16, 1, 1, 1]) -> 需要转换为torch.Size([16, 1, 1])\n",
    "            oscale_list[M_key_name]=oscale  #储存 浮点和整形的Oscale 是一个值  类似这样oscale= tensor([0.9272]) torch.Size([1])\n",
    "            ascale_list[M_key_name]=ascale  \n",
    "            wscale_list[M_key_name]=torch.squeeze(M, dim=-1)  \n",
    "            count += 1\n",
    "\n",
    "#保存M结果\n",
    "print(M_key)\n",
    "# np.save('../output/weights_quan/M_refactor.npy', M_list)\n",
    "# np.save('../output/weights_quan/oscale.npy', oscale_list)\n",
    "# np.save('../output/weights_quan/ascale.npy', ascale_list)\n",
    "# np.save('../output/weights_quan/wscale.npy', wscale_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## 定点结果导出 refactor前 ##############################################\n",
    "# remapped_state = {}\n",
    "# M_list = {}\n",
    "# count=0\n",
    "# #导出权重和偏置至二进制文件中\n",
    "# print('Model.state_dict:')\n",
    "# for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "#     #打印 key value字典\n",
    "#     # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "#     if(n<=434): #if(n<4*2): if(n<=434):\n",
    "#         # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "#         ############################################## 以下得到浮点权重值 ###################################################\n",
    "#         if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "#             layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "#             next_layer=''\n",
    "#             # print(layer_name+'.weight')\n",
    "#             conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "#             ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "#             wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "#                 tmp=param_key.split('.')[0:-1]\n",
    "#                 tmp[-1]=str(int(tmp[-1])+2)\n",
    "#                 next_layer=('.').join(tmp)\n",
    "#             else:\n",
    "#                 print('It is final_layer')\n",
    "#                 print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 # print('0:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet InvertedResidual模块的末尾\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 # print('1:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#             elif(param_key.split('.')[1]!='17'):\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#             # print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(),wscale.shape, '\\noscale=',oscale,oscale.shape)\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "                \n",
    "#             ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "#             k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#             if(k[0]!='final_layer'):\n",
    "#                 number = int(k[-2])//3*2 \n",
    "#                 # print(number)\n",
    "#                 k[-2]=str(number)\n",
    "#                 # print(k)\n",
    "#                 remapped_state_key=('.').join(k) #进行重映射\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             else: #final_layer\n",
    "#                 remapped_state_key=param_key\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#             # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#             # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "#             M_list[remapped_state_key] = M.flatten()\n",
    "#             count += 1\n",
    "\n",
    "# #保存M结果\n",
    "# np.save('../output/weights_quan/M.npy', M_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 ['conv1', 'features.0.conv1', 'features.0.conv2', 'features.1.conv1', 'features.1.conv2', 'features.1.conv3', 'features.2.conv1', 'features.2.conv2', 'features.2.conv3', 'features.3.conv1', 'features.3.conv2', 'features.3.conv3', 'features.4.conv1', 'features.4.conv2', 'features.4.conv3', 'features.5.conv1', 'features.5.conv2', 'features.5.conv3', 'features.6.conv1', 'features.6.conv2', 'features.6.conv3', 'features.7.conv1', 'features.7.conv2', 'features.7.conv3', 'features.8.conv1', 'features.8.conv2', 'features.8.conv3', 'features.9.conv1', 'features.9.conv2', 'features.9.conv3', 'features.10.conv1', 'features.10.conv2', 'features.10.conv3', 'features.11.conv1', 'features.11.conv2', 'features.11.conv3', 'features.12.conv1', 'features.12.conv2', 'features.12.conv3', 'features.13.conv1', 'features.13.conv2', 'features.13.conv3', 'features.14.conv1', 'features.14.conv2', 'features.14.conv3', 'features.15.conv1', 'features.15.conv2', 'features.15.conv3', 'features.16.conv1', 'features.16.conv2', 'features.16.conv3', 'conv2', 'deconv_layers0', 'deconv_layers1', 'deconv_layers2', 'deconv_layers3', 'deconv_layers4', 'deconv_layers5', 'final_layer']\n0 conv1 tensor([0.0207]) tensor([2.6400])\n1 features.0.conv1 tensor([0.9150]) tensor([116.6561])\n2 features.0.conv2 tensor([1.0306]) tensor([131.3981])\n3 features.1.conv1 tensor([0.2329]) tensor([29.6970])\n4 features.1.conv2 tensor([0.5631]) tensor([71.8014])\n5 features.1.conv3 tensor([0.1964]) tensor([25.0420])\n6 features.2.conv1 tensor([0.0793]) tensor([10.1045])\n7 features.2.conv2 tensor([0.1950]) tensor([24.8615])\n8 features.2.conv3 tensor([0.1028]) tensor([13.1009])\n9 features.3.conv1 tensor([0.1077]) tensor([13.7298])\n10 features.3.conv2 tensor([0.1491]) tensor([19.0112])\n11 features.3.conv3 tensor([0.0807]) tensor([10.2866])\n12 features.4.conv1 tensor([0.0530]) tensor([6.7617])\n13 features.4.conv2 tensor([0.0814]) tensor([10.3801])\n14 features.4.conv3 tensor([0.0649]) tensor([8.2754])\n15 features.5.conv1 tensor([0.0798]) tensor([10.1757])\n16 features.5.conv2 tensor([0.0897]) tensor([11.4407])\n17 features.5.conv3 tensor([0.0575]) tensor([7.3307])\n18 features.6.conv1 tensor([0.1163]) tensor([14.8286])\n19 features.6.conv2 tensor([0.1305]) tensor([16.6396])\n20 features.6.conv3 tensor([0.1375]) tensor([17.5360])\n21 features.7.conv1 tensor([0.0756]) tensor([9.6384])\n22 features.7.conv2 tensor([0.0695]) tensor([8.8645])\n23 features.7.conv3 tensor([0.0597]) tensor([7.6130])\n24 features.8.conv1 tensor([0.0789]) tensor([10.0614])\n25 features.8.conv2 tensor([0.1304]) tensor([16.6309])\n26 features.8.conv3 tensor([0.0749]) tensor([9.5488])\n27 features.9.conv1 tensor([0.0803]) tensor([10.2323])\n28 features.9.conv2 tensor([0.0740]) tensor([9.4385])\n29 features.9.conv3 tensor([0.0664]) tensor([8.4680])\n30 features.10.conv1 tensor([0.1039]) tensor([13.2485])\n31 features.10.conv2 tensor([0.1494]) tensor([19.0457])\n32 features.10.conv3 tensor([0.0903]) tensor([11.5114])\n33 features.11.conv1 tensor([0.0526]) tensor([6.7110])\n34 features.11.conv2 tensor([0.1421]) tensor([18.1171])\n35 features.11.conv3 tensor([0.2762]) tensor([35.2111])\n36 features.12.conv1 tensor([0.0864]) tensor([11.0108])\n37 features.12.conv2 tensor([0.1265]) tensor([16.1330])\n38 features.12.conv3 tensor([0.1929]) tensor([24.5971])\n39 features.13.conv1 tensor([0.1177]) tensor([15.0068])\n40 features.13.conv2 tensor([0.2718]) tensor([34.6569])\n41 features.13.conv3 tensor([0.1603]) tensor([20.4352])\n42 features.14.conv1 tensor([0.0647]) tensor([8.2471])\n43 features.14.conv2 tensor([0.1755]) tensor([22.3733])\n44 features.14.conv3 tensor([0.0797]) tensor([10.1665])\n45 features.15.conv1 tensor([0.0697]) tensor([8.8865])\n46 features.15.conv2 tensor([0.0990]) tensor([12.6213])\n47 features.15.conv3 tensor([0.1316]) tensor([16.7802])\n48 features.16.conv1 tensor([0.1201]) tensor([15.3142])\n49 features.16.conv2 tensor([0.1572]) tensor([20.0388])\n50 features.16.conv3 tensor([0.2168]) tensor([27.6383])\n51 conv2 tensor([0.1345]) tensor([17.1447])\n52 deconv_layers0 tensor([0.3039]) tensor([38.7426])\n53 deconv_layers1 tensor([0.2949]) tensor([37.5984])\n54 deconv_layers2 tensor([0.4139]) tensor([52.7759])\n55 deconv_layers3 tensor([0.3231]) tensor([41.1914])\n56 deconv_layers4 tensor([0.2333]) tensor([29.7409])\n57 deconv_layers5 tensor([0.8468]) tensor([107.9702])\n58 final_layer tensor([0.0653]) tensor([8.3283])\n0 conv1 tensor([0.9272])\n1 features.0.conv1 tensor([1.1763])\n2 features.0.conv2 tensor([0.2329])\n3 features.1.conv1 tensor([0.8437])\n4 features.1.conv2 tensor([0.2438])\n5 features.1.conv3 tensor([0.0793])\n6 features.2.conv1 tensor([0.2108])\n7 features.2.conv2 tensor([0.1052])\n8 features.2.conv3 tensor([0.1077])\n9 features.3.conv1 tensor([0.1516])\n10 features.3.conv2 tensor([0.0934])\n11 features.3.conv3 tensor([0.0530])\n12 features.4.conv1 tensor([0.0838])\n13 features.4.conv2 tensor([0.0668])\n14 features.4.conv3 tensor([0.0798])\n15 features.5.conv1 tensor([0.0928])\n16 features.5.conv2 tensor([0.0592])\n17 features.5.conv3 tensor([0.1163])\n18 features.6.conv1 tensor([0.1325])\n19 features.6.conv2 tensor([0.1453])\n20 features.6.conv3 tensor([0.0756])\n21 features.7.conv1 tensor([0.0861])\n22 features.7.conv2 tensor([0.0635])\n23 features.7.conv3 tensor([0.0789])\n24 features.8.conv1 tensor([0.1342])\n25 features.8.conv2 tensor([0.0755])\n26 features.8.conv3 tensor([0.0803])\n27 features.9.conv1 tensor([0.0765])\n28 features.9.conv2 tensor([0.0704])\n29 features.9.conv3 tensor([0.1039])\n30 features.10.conv1 tensor([0.1528])\n31 features.10.conv2 tensor([0.0926])\n32 features.10.conv3 tensor([0.0526])\n33 features.11.conv1 tensor([0.1472])\n34 features.11.conv2 tensor([0.3166])\n35 features.11.conv3 tensor([0.0864])\n36 features.12.conv1 tensor([0.1324])\n37 features.12.conv2 tensor([0.2029])\n38 features.12.conv3 tensor([0.1177])\n39 features.13.conv1 tensor([0.2830])\n40 features.13.conv2 tensor([0.1674])\n41 features.13.conv3 tensor([0.0647])\n42 features.14.conv1 tensor([0.1897])\n43 features.14.conv2 tensor([0.0910])\n44 features.14.conv3 tensor([0.0697])\n45 features.15.conv1 tensor([0.1046])\n46 features.15.conv2 tensor([0.1388])\n47 features.15.conv3 tensor([0.1201])\n48 features.16.conv1 tensor([0.1613])\n49 features.16.conv2 tensor([0.2314])\n50 features.16.conv3 tensor([0.1345])\n51 conv2 tensor([0.3217])\n52 deconv_layers0 tensor([0.3050])\n53 deconv_layers1 tensor([0.4940])\n54 deconv_layers2 tensor([0.3350])\n55 deconv_layers3 tensor([0.2363])\n56 deconv_layers4 tensor([0.8969])\n57 deconv_layers5 tensor([0.2014])\n58 final_layer tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(len(M_key), M_key)\n",
    "#读取M结果\n",
    "# M_load = np.load('../output/weights_quan/M_refactor.npy', allow_pickle=True) #M_list\n",
    "# # print('M_list=',M_load.item())\n",
    "# print('conv1', M_load.item()['conv1'])#, M_load.item()[key])\n",
    "# for n,key in enumerate(M_key):\n",
    "#     if(n<=150):\n",
    "#         print(n, key, M_load.item()[key].shape)#, M_load.item()[key])\n",
    "\n",
    "# #读取oscale结果\n",
    "# ascale_load = np.load('../output/weights_quan/ascale.npy', allow_pickle=True) \n",
    "# # print('M_list=',M_load.item())\n",
    "# for n,key in enumerate(M_key):\n",
    "#     if(n<=150):\n",
    "#         print(n, key, ascale_load.item()[key], ascale_load.item()[key]*127.5)  #ascale_load.item()[key]*127.5 浮点结果\n",
    "\n",
    "#读取oscale结果\n",
    "oscale_load = np.load('../output/weights_quan/oscale.npy', allow_pickle=True) \n",
    "# print('M_list=',M_load.item())\n",
    "for n,key in enumerate(M_key):\n",
    "    if(n<=150):\n",
    "        print(n, key, oscale_load.item()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['conv1.weight', 'conv1.bias', 'features.0.conv1.weight', 'features.0.conv1.bias', 'features.0.conv2.weight', 'features.0.conv2.bias', 'features.1.conv1.weight', 'features.1.conv1.bias', 'features.1.conv2.weight', 'features.1.conv2.bias', 'features.1.conv3.weight', 'features.1.conv3.bias', 'features.2.conv1.weight', 'features.2.conv1.bias', 'features.2.conv2.weight', 'features.2.conv2.bias', 'features.2.conv3.weight', 'features.2.conv3.bias', 'features.3.conv1.weight', 'features.3.conv1.bias', 'features.3.conv2.weight', 'features.3.conv2.bias', 'features.3.conv3.weight', 'features.3.conv3.bias', 'features.4.conv1.weight', 'features.4.conv1.bias', 'features.4.conv2.weight', 'features.4.conv2.bias', 'features.4.conv3.weight', 'features.4.conv3.bias', 'features.5.conv1.weight', 'features.5.conv1.bias', 'features.5.conv2.weight', 'features.5.conv2.bias', 'features.5.conv3.weight', 'features.5.conv3.bias', 'features.6.conv1.weight', 'features.6.conv1.bias', 'features.6.conv2.weight', 'features.6.conv2.bias', 'features.6.conv3.weight', 'features.6.conv3.bias', 'features.7.conv1.weight', 'features.7.conv1.bias', 'features.7.conv2.weight', 'features.7.conv2.bias', 'features.7.conv3.weight', 'features.7.conv3.bias', 'features.8.conv1.weight', 'features.8.conv1.bias', 'features.8.conv2.weight', 'features.8.conv2.bias', 'features.8.conv3.weight', 'features.8.conv3.bias', 'features.9.conv1.weight', 'features.9.conv1.bias', 'features.9.conv2.weight', 'features.9.conv2.bias', 'features.9.conv3.weight', 'features.9.conv3.bias', 'features.10.conv1.weight', 'features.10.conv1.bias', 'features.10.conv2.weight', 'features.10.conv2.bias', 'features.10.conv3.weight', 'features.10.conv3.bias', 'features.11.conv1.weight', 'features.11.conv1.bias', 'features.11.conv2.weight', 'features.11.conv2.bias', 'features.11.conv3.weight', 'features.11.conv3.bias', 'features.12.conv1.weight', 'features.12.conv1.bias', 'features.12.conv2.weight', 'features.12.conv2.bias', 'features.12.conv3.weight', 'features.12.conv3.bias', 'features.13.conv1.weight', 'features.13.conv1.bias', 'features.13.conv2.weight', 'features.13.conv2.bias', 'features.13.conv3.weight', 'features.13.conv3.bias', 'features.14.conv1.weight', 'features.14.conv1.bias', 'features.14.conv2.weight', 'features.14.conv2.bias', 'features.14.conv3.weight', 'features.14.conv3.bias', 'features.15.conv1.weight', 'features.15.conv1.bias', 'features.15.conv2.weight', 'features.15.conv2.bias', 'features.15.conv3.weight', 'features.15.conv3.bias', 'features.16.conv1.weight', 'features.16.conv1.bias', 'features.16.conv2.weight', 'features.16.conv2.bias', 'features.16.conv3.weight', 'features.16.conv3.bias', 'conv2.weight', 'conv2.bias', 'deconv_layers0.weight', 'deconv_layers0.bias', 'deconv_layers1.weight', 'deconv_layers1.bias', 'deconv_layers2.weight', 'deconv_layers2.bias', 'deconv_layers3.weight', 'deconv_layers3.bias', 'deconv_layers4.weight', 'deconv_layers4.bias', 'deconv_layers5.weight', 'deconv_layers5.bias', 'final_layer.weight'])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "#导入量化后的int权重  但由于模型中的weight/bias参数仍是float32类型的，因此保存时还是会保存成float32.\n",
    "print(remapped_state.keys())\n",
    "bnfuse_model.load_state_dict(remapped_state)\n",
    "#修改权重数据的类型为int32\n",
    "# for n,param_key in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_key,'\\t',bnfuse_model.state_dict()[param_key].size())\n",
    "#     eval('bnfuse_model.'+param_key+'.data.type(torch.int32)')\n",
    "\n",
    "# for name, module in bnfuse_model.named_modules():\n",
    "#     if type(module) in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n",
    "#         print(name, module)\n",
    "#         if(name=='final_layer'): #没有bias\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#         else: #conv2d, ConvTranspose2d\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#             eval('bnfuse_model.'+name+'.bias.data.type(torch.int32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {'model': bnfuse_model.module.state_dict() if hasattr(bnfuse_model, 'module') else bnfuse_model.state_dict()}\n",
    "# torch.save(ckpt, '../output/weights_quan/float_mobilenetpose_nobn_refactor_likeint5_8.pt')\n",
    "# torch.save(ckpt, '../output/weights_quan/float_mobilenetpose_nobn_refactor.pt')\n",
    "torch.save(ckpt, '../output/weights_quan/int_mobilenetpose_nobn_refactor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './0_weight.bin'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-73bb369dc102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##一维的数据排列 ->[kernel_size1*kernel_size0*out_ch*in_ch]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#是weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './0_weight.bin'"
     ]
    }
   ],
   "source": [
    "###验证导出的权重是否正确\n",
    "######################以下是bnfuse后的权重，与bnfuse_model.pt对应######################\n",
    "###格式： [out_ch,in_ch,kernel_size1,kernel_size0]\n",
    "###第一个是普通卷积\n",
    "weight0_shape=[8,3,3,3]\n",
    "bias0_shape=[8]\n",
    "###接下来是深度可分离卷积\n",
    "weight1_shape=[8,8,1,1]\n",
    "bias1_shape=[8]\n",
    "weight2_shape=[8,1,3,3]\n",
    "bias2_shape=[8]\n",
    "weight3_shape=[4,8,1,1]\n",
    "bias3_shape=[4]\n",
    "\n",
    "list=[[8,3,3,3],[8],[8,8,1,1],[8],[8,1,3,3],[8],[4,8,1,1],[4]]\n",
    "\n",
    "#####################给的bin文件是权重重排后的结果######################\n",
    "###格式： [kernel_size1,kernel_size0,out_ch,in_ch]\n",
    "#以下可以将.bin文件中的权重返回.pt的数据排列，以便进行数据比对\n",
    "result_path='./'\n",
    "filename=['0_weight','0_bias','1_weight','1_bias','2_weight','2_bias','3_weight','3_bias']\n",
    "for i in range(4*2):\n",
    "    b=np.fromfile(result_path+filename[i]+'.bin',dtype=np.float32) ##一维的数据排列 ->[kernel_size1*kernel_size0*out_ch*in_ch]\n",
    "    if(i==1):\n",
    "        if(filename[i].split('_')[-1]=='weight'): #是weight\n",
    "            b=b.reshape(list[i][2]*list[i][3],list[i][0],list[i][1]) #->[kernel_size1*kernel_size0,out_ch,in_ch]\n",
    "            print(b.shape)\n",
    "            b=b.transpose(1,2,0) #->[out_ch,in_ch,kernel_size1*kernel_size0]\n",
    "            print(b.shape)\n",
    "            b=b.reshape(list[i][0],list[i][1],list[i][2],list[i][3]) #->[out_ch,in_ch,kernel_size1,kernel_size0]\n",
    "            print(b.shape)\n",
    "            print(b)\n",
    "        else: #否则是bias 无需转换，直接打印\n",
    "            print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b=np.fromfile(result_path+filename+'.bin',dtype=np.float32)\n",
    "# b=np.loadtxt(result_path+filename+'.txt', dtype=np.float32, delimiter='  ')\n",
    "# print(b.shape)\n",
    "# b=b.reshape(9,8,3)\n",
    "# print(b.shape)\n",
    "# b=b.transpose(1,2,0) #[9,8,3]->[8,3,9]\n",
    "# print(b.shape)\n",
    "# b=b.reshape(8,3,3,3)\n",
    "# print(b.shape)\n",
    "# print(b)\n",
    "\n",
    "# print(a.shape)\n",
    "# print(a.shape[0],a.shape[1],a.shape[2],a.shape[3])\n",
    "# #a_resize=a.reshape([8,3,-1])\n",
    "# a_reshape=a.reshape([a.shape[0],a.shape[1],-1])\n",
    "# print(a_reshape.shape)\n",
    "# a_reshape=a_reshape.transpose(2,0,1) #[8,3,9]->[9,8,3]\n",
    "# print(a_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.reshape(8,3,9).transpose(2,1,0).shape #一次可以置换三个\n",
    "#a.swapaxes(1,2)  #swapaxes只能两两置换 对于swapaxes来说，括号内的两参数，交换位置和不交换，实际结果相同。\n",
    "\n",
    "# import cv2\n",
    "# image=cv2.imread('/home/ytwang/dataset/VOC/images/val/000001.jpg')\n",
    "# cv2.imshow(\"image\",image)\n",
    "# cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 352, 256])\n",
      "(3, 352, 256)\n",
      "(352, 256, 3)\n",
      "(90112, 3)\n"
     ]
    }
   ],
   "source": [
    "##################################### 权重、feature map导出 ##################################\n",
    "x0=torch.randn(1,3,352,256)\n",
    "print(x0.shape) #[1,3,352,256]\n",
    "x=x0.numpy().squeeze()\n",
    "print(x.shape) #(3, 352, 256)\n",
    "x=x.transpose(1,2,0)\n",
    "print(x.shape) #(352,256,3)\n",
    "#x.astype(np.float32).tofile(result_path+'input000001_352x256.bin') # 二进制文件导出\n",
    "x=x.reshape([-1,x.shape[-1]]) #(90112,3)=(352*256,3)\n",
    "print(x.shape)\n",
    "#np.savetxt(result_path+'input.txt', x, fmt=\"%f\", delimiter=',\\t') # .txt文件导出\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./input.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5952c7f42c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# b=torch.tensor(b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# type(b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m',\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ./input.txt not found."
     ]
    }
   ],
   "source": [
    "#b=np.fromfile(result_path+'input000001_352x256.bin',dtype=np.float32)\n",
    "# b=np.loadtxt(result_path+'input.txt', dtype=np.float32, delimiter=',\\t')\n",
    "# print(b.shape)\n",
    "# b=b.reshape(352,256,3).transpose(2,0,1)\n",
    "# print(b.shape)\n",
    "# b=b.reshape(1,3,352,256)\n",
    "# print(b.shape)\n",
    "# b=torch.tensor(b)\n",
    "# type(b)\n",
    "b=np.loadtxt(result_path+'input.txt', dtype=np.float32, delimiter=',\\t')\n",
    "print(b.shape)\n",
    "print(x0.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 352, 256)\n",
      "(1, 3, 352, 256)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "b=b.reshape(x0.shape[2],x0.shape[3],x0.shape[1]).transpose(2,0,1)\n",
    "print(b.shape)\n",
    "b=np.expand_dims(b,0) #[3,352,256]->[1,3,352,256]\n",
    "print(b.shape)\n",
    "x=torch.tensor(b)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.81406e-01, -8.01516e-01,  3.28198e-01, -7.11460e-01, -9.71156e-01, -2.00248e-02, -1.63450e+00,  9.71171e-01, -7.00366e-01,  1.60246e+00, -1.78841e+00,  3.63713e-01,  9.74244e-01,  1.33411e+00, -3.02220e-01, -4.06555e-01,  1.37745e-01, -6.31540e-01, -1.46539e+00,  2.76620e-01,  1.23277e+00, -1.40946e+00,\n",
      "        -1.51452e+00,  1.19671e-01,  4.42624e-01, -7.22908e-01, -4.04740e-01, -4.34604e-01,  2.06584e-01, -1.40587e+00,  5.70870e-01,  1.07140e-01,  1.61050e+00,  1.02770e+00, -3.53327e-01,  2.04177e-01,  2.31896e-02,  1.28927e+00, -5.48752e-01,  1.42098e-01,  9.03911e-01, -1.20936e+00,  9.63919e-01, -1.01060e+00,\n",
      "         8.54119e-02, -1.53845e-01, -1.54839e+00, -6.02410e-02, -2.57630e+00,  3.93433e-01,  2.39229e-01, -1.21564e-01, -1.15327e+00,  1.26149e+00,  1.45040e+00, -9.07547e-01,  1.31109e+00, -1.65926e-02, -3.07028e-01,  6.83296e-01, -5.43891e-01,  6.85024e-01,  4.69522e-01,  1.76420e+00,  1.61510e+00,  1.25916e+00,\n",
      "        -1.77150e-01,  2.47641e+00, -1.46249e+00, -2.80018e-01,  6.05029e-02,  5.44458e-01,  2.67465e+00, -1.15849e+00, -5.81707e-01,  4.31597e-01,  2.43441e+00,  2.65851e-01, -1.14425e+00,  2.25560e+00,  1.77140e+00, -7.80207e-01,  6.53952e-02, -4.71407e-01,  1.57048e+00, -9.17719e-01,  8.72730e-01,  9.31447e-01,\n",
      "         1.77649e+00,  1.00942e+00, -7.18745e-01, -2.77709e-01,  4.57021e-01, -2.23519e-01,  2.16158e-01, -1.34371e+00, -4.63412e-01,  1.40616e+00, -8.71267e-01, -2.05477e+00,  1.82690e+00, -9.64664e-02, -1.37564e+00, -3.15296e-02,  5.58746e-01, -1.83691e-01, -6.04123e-01,  7.66482e-01, -4.03121e-01,  2.93691e-01,\n",
      "         8.41161e-01,  2.42523e+00,  6.69548e-01,  1.93673e-01, -1.15198e+00,  4.01391e-01,  8.17872e-02,  1.89599e-01, -6.67637e-01,  1.04472e+00,  2.86891e-01,  8.68945e-01, -1.60576e+00, -3.06741e-04,  2.12187e+00,  4.09882e-01, -1.14038e+00, -1.76595e+00, -1.55876e-01, -1.18096e+00, -4.73746e-02,  1.64630e-01,\n",
      "        -6.96033e-01,  1.18123e+00,  2.55200e+00, -1.55258e+00, -5.07378e-02,  5.74990e-02, -3.31153e-01,  4.79517e-01,  8.98099e-02,  1.49721e+00,  4.27276e-02,  2.24228e+00,  9.64584e-01,  1.06729e+00,  1.11198e+00,  1.89691e+00, -1.34978e+00,  1.04962e+00,  1.25740e+00,  4.83465e-01,  9.00648e-01,  6.93612e-01,\n",
      "        -1.32085e+00,  6.34273e-01,  7.02246e-02,  3.29396e-01,  1.19658e+00, -3.61629e-01,  1.82372e+00, -8.15059e-01,  5.58395e-02, -1.26954e+00, -2.69953e-01, -6.87627e-01, -8.86588e-01,  6.98368e-01, -1.29626e+00, -6.38218e-01,  6.82183e-01, -4.30375e-01, -2.36423e+00,  4.34401e-01,  1.94347e+00, -1.14078e-01,\n",
      "         4.15629e-01,  9.99400e-01,  9.41885e-01,  5.68806e-01, -2.00598e-01,  7.16138e-01, -9.69121e-01, -1.51415e-01,  4.58058e-01,  6.32497e-01,  1.45279e+00, -2.12141e-01,  8.35680e-01,  6.50394e-01, -5.15632e-01, -5.69040e-01,  3.28715e-01,  1.17557e+00,  1.30806e+00,  1.56861e+00, -2.23972e+00,  4.16258e-01,\n",
      "         4.63714e-02, -3.72965e-01, -8.45543e-01, -8.86618e-01,  2.44351e-01,  9.09960e-01, -3.21582e-01, -1.45642e+00, -1.95657e+00, -3.57767e-01, -1.44434e+00,  8.81572e-01,  3.95041e-01,  7.41678e-01,  1.47994e+00,  7.90388e-01, -6.65410e-01, -3.59384e-01,  6.13578e-01,  8.22294e-01, -5.26904e-01,  8.14695e-01,\n",
      "        -3.81759e-01,  2.36013e+00, -3.96305e-01,  1.66977e+00, -2.11435e-01,  5.07760e-01, -3.10217e-01,  4.52953e-01, -7.71852e-01, -3.66158e-01,  3.17449e-01, -2.57260e+00, -8.96794e-01,  9.62500e-02,  1.30173e+00, -7.75847e-01, -1.38639e+00, -5.68825e-01,  4.10178e-01,  5.41091e-01, -1.68622e-02,  6.18837e-01,\n",
      "        -8.45662e-01, -6.51488e-01, -6.27133e-01,  7.43690e-01,  2.00255e-01, -3.11761e-01,  1.16924e+00,  1.51702e+00,  1.01467e+00, -2.29272e+00, -5.90515e-01, -6.66745e-01,  1.29226e+00, -2.55041e+00])\n"
     ]
    }
   ],
   "source": [
    "# (x0==b).all()\n",
    "print(x0[0][0][0]) #torch.Size([1, 3, 352, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### 验证bnfuse后结果是否正确 #########################################\n",
    "bn_weight=torch.tensor([\n",
    "    0.9627532362937927,\n",
    "    1.6811193227767944,\n",
    "    1.0011911392211914,\n",
    "    1.6239477396011353,\n",
    "    1.3853583335876465,\n",
    "    0.9575594663619995,\n",
    "    1.325505256652832,\n",
    "    0.9901668429374695])\n",
    "bn_bias=torch.tensor([\n",
    "    -24.751123428344727,\n",
    "    16.553693771362305,\n",
    "    -21.58427619934082,\n",
    "    13.835719108581543,\n",
    "    7.6066741943359375,\n",
    "    12.691370010375977,\n",
    "    24.215431213378906,\n",
    "    -10.96066665649414])\n",
    "bn_running_mean=torch.tensor([\n",
    "    0.2264288365840912,\n",
    "    -1.438944935798645,\n",
    "    -0.012756695970892906,\n",
    "    -1.1514368057250977,\n",
    "    2.064279079437256,\n",
    "    -1.000322699546814,\n",
    "    1.4272160530090332,\n",
    "    -0.01157035119831562])\n",
    "bn_running_var=torch.tensor([\n",
    "    0.018602905794978142,\n",
    "    0.7236471176147461,\n",
    "    0.0006818491965532303,\n",
    "    0.586725652217865,\n",
    "    1.5904737710952759,\n",
    "    0.3611223101615906,\n",
    "    0.7421475648880005,\n",
    "    0.00004840613837586716])\n",
    "bn_eps=torch.tensor([1E-4]).expand([8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-26.34514,  19.39717, -21.12751,  16.27666,   5.33914,  14.28511,  22.01961, -10.02023])\n",
      "tensor([-26.34514,  19.39717, -21.12751,  16.27666,   5.33914,  14.28511,  22.01961, -10.02023])\n"
     ]
    }
   ],
   "source": [
    "# bn.bias=\n",
    "# bn.weight=\n",
    "# bn.running_mean=\n",
    "# bn.running_var=\n",
    "# bn.eps=\n",
    "afterfuse_b=bn_bias-bn_weight*bn_running_mean/(torch.sqrt(bn_running_var+bn_eps))\n",
    "print(afterfuse_b)\n",
    "\n",
    "b_bn = bn_bias - bn_weight.mul(bn_running_mean).div(torch.sqrt(bn_running_var + bn_eps))\n",
    "print(b_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### torch_utils.py中的bu_fuse实现 ##########################\n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "    # https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
    "    with torch.no_grad():\n",
    "        # init\n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                    conv.out_channels,\n",
    "                                    kernel_size=conv.kernel_size,\n",
    "                                    stride=conv.stride,\n",
    "                                    padding=conv.padding,\n",
    "                                    groups=conv.groups,\n",
    "                                    bias=True)\n",
    "\n",
    "        # prepare filters\n",
    "        w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "        fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.size()))\n",
    "\n",
    "        # prepare spatial bias\n",
    "        if conv.bias is not None:\n",
    "            b_conv = conv.bias\n",
    "        else:\n",
    "            b_conv = torch.zeros(conv.weight.size(0))\n",
    "        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "        fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
    "\n",
    "        return fusedconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab342d38c9089b27086ef2ff31e6c6863533cd1c8642810b45a542962dd3699"
  },
  "kernelspec": {
   "name": "python383jvsc74a57bd0aab342d38c9089b27086ef2ff31e6c6863533cd1c8642810b45a542962dd3699",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}