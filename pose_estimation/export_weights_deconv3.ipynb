{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import _init_paths\n",
    "from core.config import config\n",
    "from core.config import update_config\n",
    "from core.config import update_dir\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models\n",
    "\n",
    "import quantize_dorefa\n",
    "# from quantize_iao import *\n",
    "from quantize_iao_deconv3 import *\n",
    "# from quantize_iao_uint import *  #对feature map进行uint对称量化\n",
    "\n",
    "import numpy as np\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "np.set_printoptions(threshold = np.inf) \n",
    "\n",
    "# # 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_conv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConv2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv\n",
    "\n",
    "def bn_fuse_deconv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConvTranspose2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         output_padding=bn_conv.output_padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_module(module, device):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, QuantBNFuseConv2d):\n",
    "            bn_fused_conv = bn_fuse_conv(child, device)\n",
    "            module._modules[name] = bn_fused_conv\n",
    "        elif isinstance(child, QuantBNFuseConvTranspose2d):\n",
    "            bn_fused_deconv = bn_fuse_deconv(child, device)\n",
    "            module._modules[name] = bn_fused_deconv\n",
    "        else:\n",
    "            bn_fuse_module(child, device)\n",
    "\n",
    "\n",
    "def model_bn_fuse(model, inplace=False):\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "    device = next(model.parameters()).device\n",
    "    bn_fuse_module(model,device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device='', apex=False, batch_size=None):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    cpu_request = device.lower() == 'cpu'\n",
    "    if device and not cpu_request:  # if device requested other than 'cpu'\n",
    "        # os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity\n",
    "\n",
    "    cuda = False if cpu_request else torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        c = 1024 ** 2  # bytes to MB\n",
    "        ng = torch.cuda.device_count()\n",
    "        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
    "            assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)\n",
    "        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n",
    "        s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex\n",
    "        for i in range(0, ng):\n",
    "            if i == 1:\n",
    "                s = ' ' * len(s)\n",
    "            print(\"%sdevice%g _CudaDeviceProperties(name='%s', total_memory=%dMB)\" %\n",
    "                  (s, i, x[i].name, x[i].total_memory / c))\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    print('')  # skip a line\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "models.pose_mobilenet_relu.get_pose_net\n",
      "/home/ytwang/wyt_workspace/quantization/human-pose-estimation.pytorch/pose_estimation/../lib/core/config.py:196: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  exp_config = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "# cfg='../experiments/coco/resnet50/mobile_quant_relu_int.yaml'   #MODEL_FILE:'output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt' 生成 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "cfg='../experiments/coco/resnet50/mobile_quant_relu_int_deconv3.yaml'   #MODEL_FILE:'output/weights_quan_deconv3/int8_mobilenet8_relu_bnfuse_deconv3_float.pt' 生成 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "update_config(cfg)\n",
    "# cudnn related setting\n",
    "cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n",
    "\n",
    "# for shufflenetv2\n",
    "shufflenetv2_spec = {'0.5': ([4, 8, 4], [24, 48, 96, 192, 1024]),\n",
    "                        '1.0': ([4, 8, 4], [24, 116, 232, 464, 1024]),\n",
    "                        '1.5': ([4, 8, 4], [24, 176, 352, 704, 1024]),\n",
    "                        '2.0': ([4, 8, 4], [24, 244, 488, 976, 2048])}\n",
    "stages_repeats, stages_out_channels = shufflenetv2_spec['1.0']\n",
    "print('models.'+config.MODEL.NAME+'.get_pose_net')\n",
    "model = eval('models.'+config.MODEL.NAME+'.get_pose_net')(\n",
    "        config, \n",
    "        stages_repeats, stages_out_channels,\n",
    "        is_train=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################### bnfuse model ############################################\n",
    "bnfuse_model = eval('models.pose_mobilenet_relu_bnfuse.get_pose_net')(\n",
    "    config, \n",
    "    stages_repeats, stages_out_channels,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32510MB)\n\n"
     ]
    }
   ],
   "source": [
    "gpus = [int(i) for i in config.GPUS.split(',')]\n",
    "device = select_device(config.GPUS, batch_size=config.TEST.BATCH_SIZE*len(gpus))\n",
    "\n",
    "model = model.to(device)\n",
    "# summary(model,input_size=(3, 256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a_bits= 8 \tw_bits= 8 \tq_type= 0 \tq_level= 0 \tdevice= cuda:0 \tweight_observer= 0 \tbn_fuse= 1 \tquant_inference= False\n"
     ]
    }
   ],
   "source": [
    "#print('*******************ori_model*******************\\n', model)\n",
    "if(config.QUANTIZATION.QUANT_METHOD == 1): # DoReFa\n",
    "    quantize_dorefa.prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS, quant_inference=config.QUANTIZATION.QUANT_INFERENCE, is_activate=False)\n",
    "else: #default quant_method == 0   IAO\n",
    "    prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS,q_type=config.QUANTIZATION.Q_TYPE, q_level=config.QUANTIZATION.Q_LEVEL, device=device,#device=next(model.parameters()).device, \n",
    "                        weight_observer=config.QUANTIZATION.WEIGHT_OBSERVER, bn_fuse=config.QUANTIZATION.BN_FUSE, quant_inference=config.QUANTIZATION.QUANT_INFERENCE)\n",
    "#print('\\n*******************quant_model*******************\\n', model)\n",
    "# print('\\n*******************Using quant_model in test*******************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_list=torch.load('../'+config.TEST.MODEL_FILE)\n",
    "# print(checkpoint_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=> loading model from output/coco_quan/mobile_quant_relu_int_deconv3_w8a8_bnfuse1/final_state_140.pth.tar\n"
     ]
    }
   ],
   "source": [
    "if config.TEST.MODEL_FILE:\n",
    "    # logger.info('=> loading model from {}'.format(config.TEST.MODEL_FILE))\n",
    "    print('=> loading model from {}'.format(config.TEST.MODEL_FILE))\n",
    "    if(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint.pth.tar'):\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "        #model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=torch.device('cuda'))['state_dict'])\n",
    "        model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device)['state_dict'])\n",
    "        #torch.save(model.module.state_dict(), 'output/coco_quan/mobile_quant_relu_w8a8_bnfuse0/checkpoint_nomodule.pth.tar')\n",
    "    elif(config.TEST.MODEL_FILE.split('/')[-1]=='model_best_140.pth.tar'):  #multiGPU has model.module.\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "        model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device))\n",
    "    elif(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint_resave.pth.tar'):  #multiGPU has model.module.\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "        model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "    else:  #final_state.pth.tar\n",
    "        model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device))\n",
    "        # model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*******************For inference bn_fuse quant_model*******************\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# ********************* quant_bn_fused_model_inference **********************\n",
    "model.to(device)\n",
    "model_bn_fuse(model, inplace=True)  # bn融合\n",
    "# print('\\n*******************For inference bn_fuse quant_model*******************\\n', model)\n",
    "# ckpt = {'model': model.module.state_dict() if hasattr(model, 'module') else model.state_dict()}\n",
    "# torch.save(ckpt, '../output/weights_quan_deconv3/int8_mobilenet8_relu_bnfuse_inference.pt')\n",
    "print('*******************For inference bn_fuse quant_model*******************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output/coco_quan/mobile_quant_relu_int_deconv3_w8a8_bnfuse1/final_state_140.pth.tar\n"
     ]
    }
   ],
   "source": [
    "print(config.TEST.MODEL_FILE)\n",
    "# model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device)['model'])  ##为什么还在'model'里面呀？\n",
    "# model.load_state_dict(torch.load('../output/weights_quan_deconv3/int8_mobilenet8_relu_bnfuse_inference.pt',map_location=device)['model'])  ##为什么还在'model'里面呀？\n",
    "# model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device))  ##为什么还在'model'里面呀？\n",
    "# model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model.state_dict:\n",
      "0 features.0.0.weight \t torch.Size([16, 3, 3, 3])\n",
      "1 features.0.0.bias \t torch.Size([16])\n",
      "2 features.0.0.activation_quantizer.scale \t torch.Size([1])\n",
      "3 features.0.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "4 features.0.0.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "5 features.0.0.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "6 features.1.conv.0.weight \t torch.Size([16, 1, 3, 3])\n",
      "7 features.1.conv.0.bias \t torch.Size([16])\n",
      "8 features.1.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "9 features.1.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "10 features.1.conv.0.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "11 features.1.conv.0.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "12 features.1.conv.3.weight \t torch.Size([8, 16, 1, 1])\n",
      "13 features.1.conv.3.bias \t torch.Size([8])\n",
      "14 features.1.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "15 features.1.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "16 features.1.conv.3.weight_quantizer.scale \t torch.Size([8, 1, 1, 1])\n",
      "17 features.1.conv.3.weight_quantizer.zero_point \t torch.Size([8, 1, 1, 1])\n",
      "18 features.2.conv.0.weight \t torch.Size([48, 8, 1, 1])\n",
      "19 features.2.conv.0.bias \t torch.Size([48])\n",
      "20 features.2.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "21 features.2.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "22 features.2.conv.0.weight_quantizer.scale \t torch.Size([48, 1, 1, 1])\n",
      "23 features.2.conv.0.weight_quantizer.zero_point \t torch.Size([48, 1, 1, 1])\n",
      "24 features.2.conv.3.weight \t torch.Size([48, 1, 3, 3])\n",
      "25 features.2.conv.3.bias \t torch.Size([48])\n",
      "26 features.2.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "27 features.2.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "28 features.2.conv.3.weight_quantizer.scale \t torch.Size([48, 1, 1, 1])\n",
      "29 features.2.conv.3.weight_quantizer.zero_point \t torch.Size([48, 1, 1, 1])\n",
      "30 features.2.conv.6.weight \t torch.Size([16, 48, 1, 1])\n",
      "31 features.2.conv.6.bias \t torch.Size([16])\n",
      "32 features.2.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "33 features.2.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "34 features.2.conv.6.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "35 features.2.conv.6.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "36 features.3.conv.0.weight \t torch.Size([96, 16, 1, 1])\n",
      "37 features.3.conv.0.bias \t torch.Size([96])\n",
      "38 features.3.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "39 features.3.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "40 features.3.conv.0.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "41 features.3.conv.0.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "42 features.3.conv.3.weight \t torch.Size([96, 1, 3, 3])\n",
      "43 features.3.conv.3.bias \t torch.Size([96])\n",
      "44 features.3.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "45 features.3.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "46 features.3.conv.3.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "47 features.3.conv.3.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "48 features.3.conv.6.weight \t torch.Size([16, 96, 1, 1])\n",
      "49 features.3.conv.6.bias \t torch.Size([16])\n",
      "50 features.3.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "51 features.3.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "52 features.3.conv.6.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "53 features.3.conv.6.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "54 features.4.conv.0.weight \t torch.Size([96, 16, 1, 1])\n",
      "55 features.4.conv.0.bias \t torch.Size([96])\n",
      "56 features.4.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "57 features.4.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "58 features.4.conv.0.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "59 features.4.conv.0.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "60 features.4.conv.3.weight \t torch.Size([96, 1, 3, 3])\n",
      "61 features.4.conv.3.bias \t torch.Size([96])\n",
      "62 features.4.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "63 features.4.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "64 features.4.conv.3.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "65 features.4.conv.3.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "66 features.4.conv.6.weight \t torch.Size([16, 96, 1, 1])\n",
      "67 features.4.conv.6.bias \t torch.Size([16])\n",
      "68 features.4.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "69 features.4.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "70 features.4.conv.6.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "71 features.4.conv.6.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "72 features.5.conv.0.weight \t torch.Size([96, 16, 1, 1])\n",
      "73 features.5.conv.0.bias \t torch.Size([96])\n",
      "74 features.5.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "75 features.5.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "76 features.5.conv.0.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "77 features.5.conv.0.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "78 features.5.conv.3.weight \t torch.Size([96, 1, 3, 3])\n",
      "79 features.5.conv.3.bias \t torch.Size([96])\n",
      "80 features.5.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "81 features.5.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "82 features.5.conv.3.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "83 features.5.conv.3.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "84 features.5.conv.6.weight \t torch.Size([16, 96, 1, 1])\n",
      "85 features.5.conv.6.bias \t torch.Size([16])\n",
      "86 features.5.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "87 features.5.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "88 features.5.conv.6.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "89 features.5.conv.6.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "90 features.6.conv.0.weight \t torch.Size([96, 16, 1, 1])\n",
      "91 features.6.conv.0.bias \t torch.Size([96])\n",
      "92 features.6.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "93 features.6.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "94 features.6.conv.0.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "95 features.6.conv.0.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "96 features.6.conv.3.weight \t torch.Size([96, 1, 3, 3])\n",
      "97 features.6.conv.3.bias \t torch.Size([96])\n",
      "98 features.6.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "99 features.6.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "100 features.6.conv.3.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "101 features.6.conv.3.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "102 features.6.conv.6.weight \t torch.Size([16, 96, 1, 1])\n",
      "103 features.6.conv.6.bias \t torch.Size([16])\n",
      "104 features.6.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "105 features.6.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "106 features.6.conv.6.weight_quantizer.scale \t torch.Size([16, 1, 1, 1])\n",
      "107 features.6.conv.6.weight_quantizer.zero_point \t torch.Size([16, 1, 1, 1])\n",
      "108 features.7.conv.0.weight \t torch.Size([96, 16, 1, 1])\n",
      "109 features.7.conv.0.bias \t torch.Size([96])\n",
      "110 features.7.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "111 features.7.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "112 features.7.conv.0.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "113 features.7.conv.0.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "114 features.7.conv.3.weight \t torch.Size([96, 1, 3, 3])\n",
      "115 features.7.conv.3.bias \t torch.Size([96])\n",
      "116 features.7.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "117 features.7.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "118 features.7.conv.3.weight_quantizer.scale \t torch.Size([96, 1, 1, 1])\n",
      "119 features.7.conv.3.weight_quantizer.zero_point \t torch.Size([96, 1, 1, 1])\n",
      "120 features.7.conv.6.weight \t torch.Size([32, 96, 1, 1])\n",
      "121 features.7.conv.6.bias \t torch.Size([32])\n",
      "122 features.7.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "123 features.7.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "124 features.7.conv.6.weight_quantizer.scale \t torch.Size([32, 1, 1, 1])\n",
      "125 features.7.conv.6.weight_quantizer.zero_point \t torch.Size([32, 1, 1, 1])\n",
      "126 features.8.conv.0.weight \t torch.Size([192, 32, 1, 1])\n",
      "127 features.8.conv.0.bias \t torch.Size([192])\n",
      "128 features.8.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "129 features.8.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "130 features.8.conv.0.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "131 features.8.conv.0.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "132 features.8.conv.3.weight \t torch.Size([192, 1, 3, 3])\n",
      "133 features.8.conv.3.bias \t torch.Size([192])\n",
      "134 features.8.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "135 features.8.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "136 features.8.conv.3.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "137 features.8.conv.3.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "138 features.8.conv.6.weight \t torch.Size([32, 192, 1, 1])\n",
      "139 features.8.conv.6.bias \t torch.Size([32])\n",
      "140 features.8.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "141 features.8.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "142 features.8.conv.6.weight_quantizer.scale \t torch.Size([32, 1, 1, 1])\n",
      "143 features.8.conv.6.weight_quantizer.zero_point \t torch.Size([32, 1, 1, 1])\n",
      "144 features.9.conv.0.weight \t torch.Size([192, 32, 1, 1])\n",
      "145 features.9.conv.0.bias \t torch.Size([192])\n",
      "146 features.9.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "147 features.9.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "148 features.9.conv.0.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "149 features.9.conv.0.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "150 features.9.conv.3.weight \t torch.Size([192, 1, 3, 3])\n",
      "151 features.9.conv.3.bias \t torch.Size([192])\n",
      "152 features.9.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "153 features.9.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "154 features.9.conv.3.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "155 features.9.conv.3.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "156 features.9.conv.6.weight \t torch.Size([32, 192, 1, 1])\n",
      "157 features.9.conv.6.bias \t torch.Size([32])\n",
      "158 features.9.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "159 features.9.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "160 features.9.conv.6.weight_quantizer.scale \t torch.Size([32, 1, 1, 1])\n",
      "161 features.9.conv.6.weight_quantizer.zero_point \t torch.Size([32, 1, 1, 1])\n",
      "162 features.10.conv.0.weight \t torch.Size([192, 32, 1, 1])\n",
      "163 features.10.conv.0.bias \t torch.Size([192])\n",
      "164 features.10.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "165 features.10.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "166 features.10.conv.0.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "167 features.10.conv.0.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "168 features.10.conv.3.weight \t torch.Size([192, 1, 3, 3])\n",
      "169 features.10.conv.3.bias \t torch.Size([192])\n",
      "170 features.10.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "171 features.10.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "172 features.10.conv.3.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "173 features.10.conv.3.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "174 features.10.conv.6.weight \t torch.Size([32, 192, 1, 1])\n",
      "175 features.10.conv.6.bias \t torch.Size([32])\n",
      "176 features.10.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "177 features.10.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "178 features.10.conv.6.weight_quantizer.scale \t torch.Size([32, 1, 1, 1])\n",
      "179 features.10.conv.6.weight_quantizer.zero_point \t torch.Size([32, 1, 1, 1])\n",
      "180 features.11.conv.0.weight \t torch.Size([192, 32, 1, 1])\n",
      "181 features.11.conv.0.bias \t torch.Size([192])\n",
      "182 features.11.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "183 features.11.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "184 features.11.conv.0.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "185 features.11.conv.0.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "186 features.11.conv.3.weight \t torch.Size([192, 1, 3, 3])\n",
      "187 features.11.conv.3.bias \t torch.Size([192])\n",
      "188 features.11.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "189 features.11.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "190 features.11.conv.3.weight_quantizer.scale \t torch.Size([192, 1, 1, 1])\n",
      "191 features.11.conv.3.weight_quantizer.zero_point \t torch.Size([192, 1, 1, 1])\n",
      "192 features.11.conv.6.weight \t torch.Size([48, 192, 1, 1])\n",
      "193 features.11.conv.6.bias \t torch.Size([48])\n",
      "194 features.11.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "195 features.11.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "196 features.11.conv.6.weight_quantizer.scale \t torch.Size([48, 1, 1, 1])\n",
      "197 features.11.conv.6.weight_quantizer.zero_point \t torch.Size([48, 1, 1, 1])\n",
      "198 features.12.conv.0.weight \t torch.Size([288, 48, 1, 1])\n",
      "199 features.12.conv.0.bias \t torch.Size([288])\n",
      "200 features.12.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "201 features.12.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "202 features.12.conv.0.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "203 features.12.conv.0.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "204 features.12.conv.3.weight \t torch.Size([288, 1, 3, 3])\n",
      "205 features.12.conv.3.bias \t torch.Size([288])\n",
      "206 features.12.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "207 features.12.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "208 features.12.conv.3.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "209 features.12.conv.3.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "210 features.12.conv.6.weight \t torch.Size([48, 288, 1, 1])\n",
      "211 features.12.conv.6.bias \t torch.Size([48])\n",
      "212 features.12.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "213 features.12.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "214 features.12.conv.6.weight_quantizer.scale \t torch.Size([48, 1, 1, 1])\n",
      "215 features.12.conv.6.weight_quantizer.zero_point \t torch.Size([48, 1, 1, 1])\n",
      "216 features.13.conv.0.weight \t torch.Size([288, 48, 1, 1])\n",
      "217 features.13.conv.0.bias \t torch.Size([288])\n",
      "218 features.13.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "219 features.13.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "220 features.13.conv.0.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "221 features.13.conv.0.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "222 features.13.conv.3.weight \t torch.Size([288, 1, 3, 3])\n",
      "223 features.13.conv.3.bias \t torch.Size([288])\n",
      "224 features.13.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "225 features.13.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "226 features.13.conv.3.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "227 features.13.conv.3.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "228 features.13.conv.6.weight \t torch.Size([48, 288, 1, 1])\n",
      "229 features.13.conv.6.bias \t torch.Size([48])\n",
      "230 features.13.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "231 features.13.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "232 features.13.conv.6.weight_quantizer.scale \t torch.Size([48, 1, 1, 1])\n",
      "233 features.13.conv.6.weight_quantizer.zero_point \t torch.Size([48, 1, 1, 1])\n",
      "234 features.14.conv.0.weight \t torch.Size([288, 48, 1, 1])\n",
      "235 features.14.conv.0.bias \t torch.Size([288])\n",
      "236 features.14.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "237 features.14.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "238 features.14.conv.0.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "239 features.14.conv.0.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "240 features.14.conv.3.weight \t torch.Size([288, 1, 3, 3])\n",
      "241 features.14.conv.3.bias \t torch.Size([288])\n",
      "242 features.14.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "243 features.14.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "244 features.14.conv.3.weight_quantizer.scale \t torch.Size([288, 1, 1, 1])\n",
      "245 features.14.conv.3.weight_quantizer.zero_point \t torch.Size([288, 1, 1, 1])\n",
      "246 features.14.conv.6.weight \t torch.Size([80, 288, 1, 1])\n",
      "247 features.14.conv.6.bias \t torch.Size([80])\n",
      "248 features.14.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "249 features.14.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "250 features.14.conv.6.weight_quantizer.scale \t torch.Size([80, 1, 1, 1])\n",
      "251 features.14.conv.6.weight_quantizer.zero_point \t torch.Size([80, 1, 1, 1])\n",
      "252 features.15.conv.0.weight \t torch.Size([480, 80, 1, 1])\n",
      "253 features.15.conv.0.bias \t torch.Size([480])\n",
      "254 features.15.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "255 features.15.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "256 features.15.conv.0.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "257 features.15.conv.0.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "258 features.15.conv.3.weight \t torch.Size([480, 1, 3, 3])\n",
      "259 features.15.conv.3.bias \t torch.Size([480])\n",
      "260 features.15.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "261 features.15.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "262 features.15.conv.3.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "263 features.15.conv.3.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "264 features.15.conv.6.weight \t torch.Size([80, 480, 1, 1])\n",
      "265 features.15.conv.6.bias \t torch.Size([80])\n",
      "266 features.15.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "267 features.15.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "268 features.15.conv.6.weight_quantizer.scale \t torch.Size([80, 1, 1, 1])\n",
      "269 features.15.conv.6.weight_quantizer.zero_point \t torch.Size([80, 1, 1, 1])\n",
      "270 features.16.conv.0.weight \t torch.Size([480, 80, 1, 1])\n",
      "271 features.16.conv.0.bias \t torch.Size([480])\n",
      "272 features.16.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "273 features.16.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "274 features.16.conv.0.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "275 features.16.conv.0.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "276 features.16.conv.3.weight \t torch.Size([480, 1, 3, 3])\n",
      "277 features.16.conv.3.bias \t torch.Size([480])\n",
      "278 features.16.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "279 features.16.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "280 features.16.conv.3.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "281 features.16.conv.3.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "282 features.16.conv.6.weight \t torch.Size([80, 480, 1, 1])\n",
      "283 features.16.conv.6.bias \t torch.Size([80])\n",
      "284 features.16.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "285 features.16.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "286 features.16.conv.6.weight_quantizer.scale \t torch.Size([80, 1, 1, 1])\n",
      "287 features.16.conv.6.weight_quantizer.zero_point \t torch.Size([80, 1, 1, 1])\n",
      "288 features.17.conv.0.weight \t torch.Size([480, 80, 1, 1])\n",
      "289 features.17.conv.0.bias \t torch.Size([480])\n",
      "290 features.17.conv.0.activation_quantizer.scale \t torch.Size([1])\n",
      "291 features.17.conv.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "292 features.17.conv.0.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "293 features.17.conv.0.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "294 features.17.conv.3.weight \t torch.Size([480, 1, 3, 3])\n",
      "295 features.17.conv.3.bias \t torch.Size([480])\n",
      "296 features.17.conv.3.activation_quantizer.scale \t torch.Size([1])\n",
      "297 features.17.conv.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "298 features.17.conv.3.weight_quantizer.scale \t torch.Size([480, 1, 1, 1])\n",
      "299 features.17.conv.3.weight_quantizer.zero_point \t torch.Size([480, 1, 1, 1])\n",
      "300 features.17.conv.6.weight \t torch.Size([160, 480, 1, 1])\n",
      "301 features.17.conv.6.bias \t torch.Size([160])\n",
      "302 features.17.conv.6.activation_quantizer.scale \t torch.Size([1])\n",
      "303 features.17.conv.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "304 features.17.conv.6.weight_quantizer.scale \t torch.Size([160, 1, 1, 1])\n",
      "305 features.17.conv.6.weight_quantizer.zero_point \t torch.Size([160, 1, 1, 1])\n",
      "306 conv2.0.weight \t torch.Size([128, 160, 1, 1])\n",
      "307 conv2.0.bias \t torch.Size([128])\n",
      "308 conv2.0.activation_quantizer.scale \t torch.Size([1])\n",
      "309 conv2.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "310 conv2.0.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "311 conv2.0.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "312 deconv_layers.0.weight \t torch.Size([128, 1, 3, 3])\n",
      "313 deconv_layers.0.bias \t torch.Size([128])\n",
      "314 deconv_layers.0.activation_quantizer.scale \t torch.Size([1])\n",
      "315 deconv_layers.0.activation_quantizer.zero_point \t torch.Size([1])\n",
      "316 deconv_layers.0.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "317 deconv_layers.0.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "318 deconv_layers.3.weight \t torch.Size([128, 128, 1, 1])\n",
      "319 deconv_layers.3.bias \t torch.Size([128])\n",
      "320 deconv_layers.3.activation_quantizer.scale \t torch.Size([1])\n",
      "321 deconv_layers.3.activation_quantizer.zero_point \t torch.Size([1])\n",
      "322 deconv_layers.3.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "323 deconv_layers.3.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "324 deconv_layers.6.weight \t torch.Size([128, 1, 3, 3])\n",
      "325 deconv_layers.6.bias \t torch.Size([128])\n",
      "326 deconv_layers.6.activation_quantizer.scale \t torch.Size([1])\n",
      "327 deconv_layers.6.activation_quantizer.zero_point \t torch.Size([1])\n",
      "328 deconv_layers.6.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "329 deconv_layers.6.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "330 deconv_layers.9.weight \t torch.Size([128, 128, 1, 1])\n",
      "331 deconv_layers.9.bias \t torch.Size([128])\n",
      "332 deconv_layers.9.activation_quantizer.scale \t torch.Size([1])\n",
      "333 deconv_layers.9.activation_quantizer.zero_point \t torch.Size([1])\n",
      "334 deconv_layers.9.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "335 deconv_layers.9.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "336 deconv_layers.12.weight \t torch.Size([128, 1, 3, 3])\n",
      "337 deconv_layers.12.bias \t torch.Size([128])\n",
      "338 deconv_layers.12.activation_quantizer.scale \t torch.Size([1])\n",
      "339 deconv_layers.12.activation_quantizer.zero_point \t torch.Size([1])\n",
      "340 deconv_layers.12.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "341 deconv_layers.12.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "342 deconv_layers.15.weight \t torch.Size([128, 128, 1, 1])\n",
      "343 deconv_layers.15.bias \t torch.Size([128])\n",
      "344 deconv_layers.15.activation_quantizer.scale \t torch.Size([1])\n",
      "345 deconv_layers.15.activation_quantizer.zero_point \t torch.Size([1])\n",
      "346 deconv_layers.15.weight_quantizer.scale \t torch.Size([128, 1, 1, 1])\n",
      "347 deconv_layers.15.weight_quantizer.zero_point \t torch.Size([128, 1, 1, 1])\n",
      "348 final_layer.weight \t torch.Size([17, 128, 1, 1])\n",
      "349 final_layer.activation_quantizer.scale \t torch.Size([1])\n",
      "350 final_layer.activation_quantizer.zero_point \t torch.Size([1])\n",
      "351 final_layer.weight_quantizer.scale \t torch.Size([17, 1, 1, 1])\n",
      "352 final_layer.weight_quantizer.zero_point \t torch.Size([17, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "remapped_state = {}\n",
    "print('Model.state_dict:')\n",
    "######################################## before #######################################\n",
    "for n,param_tensor in enumerate(model.state_dict()):\n",
    "    #打印 key value字典\n",
    "    print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "    # if(n<5):\n",
    "    #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "    #     # print(model.state_dict()[param_tensor])\n",
    "\n",
    "# ######################################### after #######################################\n",
    "# for n,param_tensor in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',bnfuse_model.state_dict()[param_tensor].size())\n",
    "#     # if(n<4):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     print(model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor后 #############################\n",
    "# import re\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]=='features'):\n",
    "#         k[1]=str(int(k[1])+1)\n",
    "#         k[-2]=str((int(k[2][-1])-1)*3)\n",
    "#         k[2]='conv'\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     elif(k[0].startswith('deconv_layers')): #final_layer\n",
    "#         number=3*int(k[0][-1])\n",
    "#         remapped_state_key='deconv_layers.'+str(number)+'.'+k[-1] #weight/bias\n",
    "#     elif(k[0]=='conv1'): #final_layer\n",
    "#         remapped_state_key='features.0.0.'+k[-1] #weight/bias\n",
    "#     else: #final_layer  conv2\n",
    "#         remapped_state_key=state_key\n",
    "#     # print(n, state_key, remapped_state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor前 #############################\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]!='final_layer'):\n",
    "#         number = int(k[-2])//2*3 \n",
    "#         # print(number)\n",
    "#         k[-2]=str(number)\n",
    "#         # print(k)\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     else: #final_layer\n",
    "#         remapped_state_key=state_key\n",
    "#     print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 量化步骤 ##############################\n",
    "## 对称量化，因此zero_point均为0\n",
    "def quantize_tensor(weight, bias, wscale, ascale, num_bits=8):\n",
    "    qmin = -2**(num_bits-1)  #8bit [-128,127]\n",
    "    qmax = 2**(num_bits-1) - 1\n",
    "    q_weight= torch.round(weight/wscale).clamp_(qmin, qmax).type(torch.int32)\n",
    "    # print('0:', torch.max(torch.round(weight/wscale)), torch.min(torch.round(weight/wscale)))\n",
    "    q_bias= torch.round(bias/wscale.flatten()/ascale).type(torch.int32)  #bias没有进行截断\n",
    "    return q_weight, q_bias\n",
    "\n",
    "#反量化回浮点结果\n",
    "def dequantize_tensor(q_weight, q_bias, wscale, ascale):\n",
    "    return wscale*q_weight.float(), wscale.flatten()*ascale*q_bias.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model.state_dict:\n",
      "It is final_layer\n",
      "torch.Size([17, 128, 1, 1])\n",
      "['conv1', 'features.0.conv1', 'features.0.conv2', 'features.1.conv1', 'features.1.conv2', 'features.1.conv3', 'features.2.conv1', 'features.2.conv2', 'features.2.conv3', 'features.3.conv1', 'features.3.conv2', 'features.3.conv3', 'features.4.conv1', 'features.4.conv2', 'features.4.conv3', 'features.5.conv1', 'features.5.conv2', 'features.5.conv3', 'features.6.conv1', 'features.6.conv2', 'features.6.conv3', 'features.7.conv1', 'features.7.conv2', 'features.7.conv3', 'features.8.conv1', 'features.8.conv2', 'features.8.conv3', 'features.9.conv1', 'features.9.conv2', 'features.9.conv3', 'features.10.conv1', 'features.10.conv2', 'features.10.conv3', 'features.11.conv1', 'features.11.conv2', 'features.11.conv3', 'features.12.conv1', 'features.12.conv2', 'features.12.conv3', 'features.13.conv1', 'features.13.conv2', 'features.13.conv3', 'features.14.conv1', 'features.14.conv2', 'features.14.conv3', 'features.15.conv1', 'features.15.conv2', 'features.15.conv3', 'features.16.conv1', 'features.16.conv2', 'features.16.conv3', 'conv2', 'deconv_layers0', 'deconv_layers1', 'deconv_layers2', 'deconv_layers3', 'deconv_layers4', 'deconv_layers5', 'final_layer']\n"
     ]
    }
   ],
   "source": [
    "############################################################## 定点结果导出 类似后训练量化，直接从权重中得到最大值最小值 ##############################################\n",
    "remapped_state = {}\n",
    "M_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M_key=[]  #存储M_list中的索引键值\n",
    "oscale_list={} #储存 浮点和整型的oscale\n",
    "ascale_list={} #储存 浮点和整型的ascale\n",
    "wscale_list={} #储存 浮点和整型的wscale\n",
    "\n",
    "count=0\n",
    "#导出权重和偏置至二进制文件中\n",
    "print('Model.state_dict:')\n",
    "for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "    #打印 key value字典\n",
    "    # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "    if(n<=434): #if(n<4*2): if(n<=434):  n==16 or n==8\n",
    "        # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "        ############################################## 以下得到浮点权重值 ###################################################\n",
    "        if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "            layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "            next_layer=''\n",
    "            # print(layer_name+'.weight')\n",
    "            conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "            ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "            ######################################## 使用量化感知训练的权重 #########################################\n",
    "            wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "            ######################################## 直接后训练量化得到权重 #########################################\n",
    "            # inter = torch.flatten(conv_weight, start_dim=1)  #变成两维 [out_channel,-1]\n",
    "            # frange=torch.max(torch.abs(inter), 1)[0].reshape(-1,1,1,1)\n",
    "            # wscale=torch.clamp(frange/127.5, min=1.1920928955078125e-7)  #1.1920928955078125e-7*127.5=1.519918441772461e-05\n",
    "            # print('frange=',frange.flatten(),'\\nwscale=',wscale.flatten())\n",
    "\n",
    "            if(param_key.split('.')[0]!='final_layer' and param_key.split('.')[1]!='final_layer'): #'module.final_layer.bias'\n",
    "                conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "                tmp=param_key.split('.')[0:-1]\n",
    "                tmp[-1]=str(int(tmp[-1])+2)\n",
    "                next_layer=('.').join(tmp)\n",
    "                # print(param_key, next_layer)\n",
    "            else:\n",
    "                print('It is final_layer')\n",
    "                print(conv_weight.shape)\n",
    "            \n",
    "            # if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "            #     oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况 包括网络第一层、InvertedResidual中包含relu的层、conv2以及deconv_layers\n",
    "            #     # print('0:',next_layer, oscale)\n",
    "            # elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet 每个InvertedResidual模块的末尾（线性直通，无relu)   但features.17的oscale应该是conv2的ascale\n",
    "            #     tmp2=param_key.split('.')[0:-1]\n",
    "            #     tmp2[1]=str(int(tmp[1])+1)\n",
    "            #     tmp2[-1]='0'\n",
    "            #     next_layer=('.').join(tmp2)\n",
    "            #     oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "            #     # print('1:',next_layer,oscale)\n",
    "            # elif(param_key.split('.')[1]=='17'): #features.17的oscale应该是final_layer的ascale\n",
    "            #     oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "            #     # print('2: conv2.0',oscale)\n",
    "            # else:  #final_layer 不需要oscale    next_layer=='' 没有relu的都不需要oscale\n",
    "            #     oscale=torch.tensor([1.])\n",
    "            #     # print('3: final_layer',oscale)\n",
    "            # print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(), wscale.shape, '\\noscale=',oscale,oscale.shape) #wscale.flatten()\n",
    "\n",
    "            # ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "            # if(param_key.split('.')[0]!='final_layer'):\n",
    "            #     #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "            #     # if(param_key.split('.')[0]=='features'and param_key.split('.')[1]=='0'): #第一层的M需要特殊处理，将输入ascale放入图像预处理中实现，输入网络的数据直接是[-128,127] 似乎又不需要...\n",
    "            #     #     M=wscale/oscale\n",
    "            #     # else:\n",
    "            #     #     M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "            #     M=wscale*ascale/oscale\n",
    "            #     # M0=(M*2**16).type(torch.int32)\n",
    "            #     # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n",
    "            #     # print('###: M=',M.flatten())\n",
    "\n",
    "            #     # print(conv_weight.shape, conv_bias.shape)\n",
    "            #     # #*********************************************************************************************************************************\n",
    "            #     # print('conv_weight:',conv_weight.squeeze(), 'conv_bias:',conv_bias.flatten())\n",
    "                \n",
    "            #     #计算权重和偏置int量化结果\n",
    "            #     q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "            #     # print(q_weight.shape, q_bias.shape)\n",
    "            #     # print('q_weight:',q_weight.flatten(), 'q_bias:',q_bias.flatten())\n",
    "                \n",
    "            #     #反量化回浮点数的结果\n",
    "            #     dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "            #     # print(dq_weight, dq_bias)\n",
    "            #     # print('**量化误差**   weight:', torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight),'  bias', torch.mean(conv_bias-dq_bias), torch.max(conv_bias-dq_bias))\n",
    "            #     # print(conv_bias-dq_bias)\n",
    "            # else: #final_layer\n",
    "            #     #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "            #     M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "            #     # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "            #     # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "            #     # print('final_layer: M=',M.flatten())\n",
    "\n",
    "            #     #计算权重和偏置int量化结果\n",
    "            #     q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "            #     # print(q_weight, q_bias)\n",
    "            #     # print(q_bias.shape)\n",
    "            #     #反量化回浮点数的结果\n",
    "            #     dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "            #     # print(dq_weight, dq_bias)\n",
    "            #     # print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "            #     # print(conv_bias-dq_bias)\n",
    "\n",
    "            ################################################### 浮点权重导出 ################################################################\n",
    "            q_weight=conv_weight #浮点权重的导出，需要控制变量呀！！！\n",
    "            q_bias=conv_bias\n",
    "            ################################################################################################################################\n",
    "\n",
    "            ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "            k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']    k = param_key[7:].split('.')如果有module.的话  如果没有module的话直接用k = param_key.split('.')\n",
    "            if(k[0]=='features' and k[1]=='0'): # 网络第一层  features.0. -> conv1\n",
    "                remapped_state_key = 'conv1.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            elif(k[0]=='features'): #除了第一层外的其他层\n",
    "                k[1]=str(int(k[1])-1)\n",
    "                number = int(k[-2])//3 + 1 \n",
    "                # print(number)\n",
    "                k[2]=k[2]+str(number)\n",
    "                # k[3]='0' #现在不需要了\n",
    "                # print(k)\n",
    "                remapped_state_key=('.').join(k[0:3])+'.weight' #进行重映射\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='deconv_layers'): #deconv_layers0-5\n",
    "                number = int(k[-2])//3\n",
    "                remapped_state_key=k[0]+str(number)+'.weight' #进行重映射\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[k[0]+str(number)+'.bias']=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='final_layer'): #final_layer\n",
    "                remapped_state_key='final_layer.weight' #param_key\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "            else: #conv2 无需进行重映射\n",
    "                remapped_state_key = 'conv2.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            # print(n, remapped_state_key, model.state_dict()[remapped_state_key].shape)\n",
    "            # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "            if(remapped_state_key.split('.')[0]=='features'):\n",
    "                M_key_name = '.'.join(remapped_state_key.split('.')[0:-1])\n",
    "            else:\n",
    "                M_key_name = remapped_state_key.split('.')[0]\n",
    "            M_key.append(M_key_name)\n",
    "#             M_list[M_key_name] = torch.squeeze(M, dim=-1) #M.flatten() torch.Size([16]);  M通道量化torch.Size([16, 1, 1, 1]) -> 需要转换为torch.Size([16, 1, 1])\n",
    "#             oscale_list[M_key_name]=oscale  #储存 浮点和整形的Oscale 是一个值  类似这样oscale= tensor([0.9272]) torch.Size([1])\n",
    "            ascale_list[M_key_name]=ascale  \n",
    "            wscale_list[M_key_name]=torch.squeeze(wscale, dim=-1)  \n",
    "            count += 1\n",
    "\n",
    "#保存M结果\n",
    "print(M_key)\n",
    "parent_path='../output/weights_quan_deconv3/'\n",
    "# np.save('../output/weights_quan/M_key.npy', M_key) #这才真的是list\n",
    "np.save(parent_path+'M_key.npy', M_key) #这才真的是list\n",
    "\n",
    "#其实下面的都是假的list,实际是字典\n",
    "# np.save(parent_path+'M_refactor_noreluq.npy', M_list)\n",
    "# np.save(parent_path+'oscale.npy', oscale_list)\n",
    "np.save(parent_path+'ascale.npy', ascale_list)\n",
    "\n",
    "# np.save(parent_path+'M_refactor.npy', M_list)\n",
    "np.save(parent_path+'wscale.npy', wscale_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#127.0 **量化误差**   weight: tensor(0.0004) tensor(0.0275)   bias tensor(7.7337e-06) tensor(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################## 定点结果导出 refactor后 ##############################################\n",
    "# remapped_state = {}\n",
    "# M_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "# M_key=[]  #存储M_list中的索引键值\n",
    "# oscale_list={} #储存 浮点和整型的oscale\n",
    "# ascale_list={} #储存 浮点和整型的ascale\n",
    "# wscale_list={} #储存 浮点和整型的wscale\n",
    "\n",
    "# count=0\n",
    "# #导出权重和偏置至二进制文件中\n",
    "# print('Model.state_dict:')\n",
    "# for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "#     #打印 key value字典\n",
    "#     # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "#     if(n<=434): #if(n<4*2): if(n<=434):  n==16 or n==8\n",
    "#         # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "#         ############################################## 以下得到浮点权重值 ###################################################\n",
    "#         if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "#             layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "#             next_layer=''\n",
    "#             # print(layer_name+'.weight')\n",
    "#             conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "#             ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "#             wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "#                 tmp=param_key.split('.')[0:-1]\n",
    "#                 tmp[-1]=str(int(tmp[-1])+2)\n",
    "#                 next_layer=('.').join(tmp)\n",
    "#                 print(param_key, next_layer)\n",
    "#             else:\n",
    "#                 print('It is final_layer')\n",
    "#                 print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况 包括网络第一层、InvertedResidual中包含relu的层、conv2以及deconv_layers\n",
    "#                 # print('0:',next_layer, oscale)\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet 每个InvertedResidual模块的末尾（线性直通，无relu)   但features.17的oscale应该是conv2的ascale\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('1:',next_layer,oscale)\n",
    "#             elif(param_key.split('.')[1]=='17'): #features.17的oscale应该是final_layer的ascale\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('2: conv2.0',oscale)\n",
    "#             else:  #final_layer 不需要oscale    next_layer=='' 没有relu的都不需要oscale\n",
    "#                 oscale=torch.tensor([1.])\n",
    "#                 # print('3: final_layer',oscale)\n",
    "#             print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(), wscale.shape, '\\noscale=',oscale,oscale.shape) #wscale.flatten()\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 # if(param_key.split('.')[0]=='features'and param_key.split('.')[1]=='0'): #第一层的M需要特殊处理，将输入ascale放入图像预处理中实现，输入网络的数据直接是[-128,127] 似乎又不需要...\n",
    "#                 #     M=wscale/oscale\n",
    "#                 # else:\n",
    "#                 #     M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M=wscale*ascale/oscale\n",
    "#                 # M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n",
    "#                 print('###: M=',M.flatten())\n",
    "\n",
    "#                 print(conv_weight.shape, conv_bias.shape)\n",
    "#                 #*********************************************************************************************************************************\n",
    "#                 print('conv_weight:',conv_weight.squeeze(), 'conv_bias:',conv_bias.flatten())\n",
    "                \n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 print(q_weight.shape, q_bias.shape)\n",
    "#                 print(q_weight.flatten(), q_bias.flatten())\n",
    "                \n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print('weight:', torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight),'  bias', torch.mean(conv_bias-dq_bias), torch.max(conv_bias-dq_bias))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "#                 print('final_layer: M=',M.flatten())\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "#             # ################################################### 浮点权重导出 ################################################################\n",
    "#             # q_weight=conv_weight #浮点权重的导出，需要控制变量呀！！！\n",
    "#             # q_bias=conv_bias\n",
    "#             # ################################################################################################################################\n",
    "\n",
    "#             ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "#             k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#             if(k[0]=='features' and k[1]=='0'): # 网络第一层  features.0. -> conv1\n",
    "#                 remapped_state_key = 'conv1.weight'\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#             elif(k[0]=='features'): #除了第一层外的其他层\n",
    "#                 k[1]=str(int(k[1])-1)\n",
    "#                 number = int(k[-2])//3 + 1 \n",
    "#                 # print(number)\n",
    "#                 k[2]=k[2]+str(number)\n",
    "#                 # k[3]='0' #现在不需要了\n",
    "#                 # print(k)\n",
    "#                 remapped_state_key=('.').join(k[0:3])+'.weight' #进行重映射\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             elif(k[0]=='deconv_layers'): #deconv_layers0-5\n",
    "#                 number = int(k[-2])//3\n",
    "#                 remapped_state_key=k[0]+str(number)+'.weight' #进行重映射\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[k[0]+str(number)+'.bias']=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             elif(k[0]=='final_layer'): #final_layer\n",
    "#                 remapped_state_key=param_key\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#             else: #conv2 无需进行重映射\n",
    "#                 remapped_state_key = 'conv2.weight'\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#             # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#             # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "#             if(remapped_state_key.split('.')[0]=='features'):\n",
    "#                 M_key_name = '.'.join(remapped_state_key.split('.')[0:-1])\n",
    "#             else:\n",
    "#                 M_key_name = remapped_state_key.split('.')[0]\n",
    "#             M_key.append(M_key_name)\n",
    "#             M_list[M_key_name] = torch.squeeze(M, dim=-1) #M.flatten() torch.Size([16]);  M通道量化torch.Size([16, 1, 1, 1]) -> 需要转换为torch.Size([16, 1, 1])\n",
    "#             oscale_list[M_key_name]=oscale  #储存 浮点和整形的Oscale 是一个值  类似这样oscale= tensor([0.9272]) torch.Size([1])\n",
    "#             ascale_list[M_key_name]=ascale  \n",
    "#             wscale_list[M_key_name]=torch.squeeze(M, dim=-1)  \n",
    "#             count += 1\n",
    "\n",
    "# #保存M结果\n",
    "# print(M_key)\n",
    "# np.save('../output/weights_quan/M_refactor.npy', M_list)\n",
    "# np.save('../output/weights_quan/oscale.npy', oscale_list)\n",
    "# np.save('../output/weights_quan/ascale.npy', ascale_list)\n",
    "# np.save('../output/weights_quan/wscale.npy', wscale_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## 定点结果导出 refactor前 ##############################################\n",
    "# remapped_state = {}\n",
    "# M_list = {}\n",
    "# count=0\n",
    "# #导出权重和偏置至二进制文件中\n",
    "# print('Model.state_dict:')\n",
    "# for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "#     #打印 key value字典\n",
    "#     # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "#     if(n<=434): #if(n<4*2): if(n<=434):\n",
    "#         # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "#         ############################################## 以下得到浮点权重值 ###################################################\n",
    "#         if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "#             layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "#             next_layer=''\n",
    "#             # print(layer_name+'.weight')\n",
    "#             conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "#             ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "#             wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "#                 tmp=param_key.split('.')[0:-1]\n",
    "#                 tmp[-1]=str(int(tmp[-1])+2)\n",
    "#                 next_layer=('.').join(tmp)\n",
    "#             else:\n",
    "#                 print('It is final_layer')\n",
    "#                 print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 # print('0:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet InvertedResidual模块的末尾\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 # print('1:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#             elif(param_key.split('.')[1]!='17'):\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#             # print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(),wscale.shape, '\\noscale=',oscale,oscale.shape)\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "                \n",
    "#             ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "#             k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#             if(k[0]!='final_layer'):\n",
    "#                 number = int(k[-2])//3*2 \n",
    "#                 # print(number)\n",
    "#                 k[-2]=str(number)\n",
    "#                 # print(k)\n",
    "#                 remapped_state_key=('.').join(k) #进行重映射\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             else: #final_layer\n",
    "#                 remapped_state_key=param_key\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#             # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#             # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "#             M_list[remapped_state_key] = M.flatten()\n",
    "#             count += 1\n",
    "\n",
    "# #保存M结果\n",
    "# np.save('../output/weights_quan/M.npy', M_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\nconv1\n2\nfeatures.0.conv1\n"
     ]
    }
   ],
   "source": [
    "# print(len(M_key), type(M_key), M_key)\n",
    "import numpy as np\n",
    "import torch\n",
    "Mkey_load = np.load('../output/weights_quan/M_key.npy', allow_pickle=True)  #type:<class 'numpy.ndarray'>\n",
    "Mkey_load=list(Mkey_load)\n",
    "print(type(Mkey_load))\n",
    "print(Mkey_load[0])\n",
    "print(Mkey_load.index('features.0.conv2'))\n",
    "print(Mkey_load[Mkey_load.index('features.0.conv2')-1])\n",
    "\n",
    "\n",
    "# 59 ['conv1', 'features.0.conv1', 'features.0.conv2', 'features.1.conv1', 'features.1.conv2', 'features.1.conv3', 'features.2.conv1', 'features.2.conv2', 'features.2.conv3', 'features.3.conv1', 'features.3.conv2', 'features.3.conv3', 'features.4.conv1', 'features.4.conv2', 'features.4.conv3', 'features.5.conv1', 'features.5.conv2', 'features.5.conv3', 'features.6.conv1', 'features.6.conv2', 'features.6.conv3', 'features.7.conv1', 'features.7.conv2', 'features.7.conv3', 'features.8.conv1', 'features.8.conv2', 'features.8.conv3', 'features.9.conv1', 'features.9.conv2', 'features.9.conv3', 'features.10.conv1', 'features.10.conv2', 'features.10.conv3', 'features.11.conv1', 'features.11.conv2', 'features.11.conv3', 'features.12.conv1', 'features.12.conv2', 'features.12.conv3', 'features.13.conv1', 'features.13.conv2', 'features.13.conv3', 'features.14.conv1', 'features.14.conv2', 'features.14.conv3', 'features.15.conv1', 'features.15.conv2', 'features.15.conv3', 'features.16.conv1', 'features.16.conv2', 'features.16.conv3', 'conv2', 'deconv_layers0', 'deconv_layers1', 'deconv_layers2', 'deconv_layers3', 'deconv_layers4', 'deconv_layers5', 'final_layer']\n",
    "\n",
    "''' ******************************************************************  计算M0 *************************************************************** '''\n",
    "BIT=16\n",
    "Mscale_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M0_float_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M0_int_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "ascale_load = np.load(parent_path+'ascale.npy', allow_pickle=True) \n",
    "# oscale_load = np.load('../output/weights_quan/oscale.npy', allow_pickle=True) \n",
    "wscale_load = np.load(parent_path+'wscale.npy', allow_pickle=True) \n",
    "M_list = np.load('../output/weights_quan/M_refactor.npy', allow_pickle=True) #量化感知训练得到的scale   有relu的量化反量化，因此需要转换\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    if(n==58): # 最后一层没有output_scale\n",
    "        Mscale=wscale_load.item()[key]*ascale_load.item()[key]\n",
    "    else:\n",
    "        key_post = Mkey_load[Mkey_load.index(key)+1] \n",
    "        Mscale=wscale_load.item()[key]*ascale_load.item()[key]/ascale_load.item()[key_post]  #不用relu的量化反量化，重新计算M值\n",
    "    Mscale_list[key] = Mscale #torch.squeeze(Mscale, dim=-1) \n",
    "    # print(n, key, M_list.item()[key].flatten(), M_list.item()[key].shape)\n",
    "    # print(ascale_load.item()[key], ascale_load.item()[key_post], oscale_load.item()[key])\n",
    "    # print(wscale_load.item()[key].flatten())\n",
    "    if(n!=58):\n",
    "        M0=(Mscale*2**BIT).type(torch.int32)\n",
    "        M0_float=M0*2**(-1*BIT)\n",
    "    else:  #此时还没有管最后一层的M0（最后一层应该不用*M0）\n",
    "        M0_float=Mscale\n",
    "        M0=torch.tensor(2**BIT) #乘以2**16,再除以2**16是原值\n",
    "    M0_float_list[key] = M0_float\n",
    "    M0_int_list[key] = M0\n",
    "    # print('M=',Mscale.flatten(),'\\nM0_float=',M0_float.flatten(),'\\nM0=',M0.flatten(),'\\nerror=',(Mscale-M0_float).flatten())\n",
    "    # print(n, key, Mscale.flatten(), Mscale_list[key].shape)\n",
    "\n",
    "# np.save('../output/weights_quan/mscale_norelu_quant.npy', Mscale_list)\n",
    "np.save(parent_path+'M0_quant_requant.npy', M0_float_list)\n",
    "np.save(parent_path+'M0_int.npy', M0_int_list)\n",
    "# # 读取M结果\n",
    "# M_load = np.load('../output/weights_quan/mscale_norelu_quant.npy', allow_pickle=True) #M_list\n",
    "# # M_load = np.load('../output/weights_quan/M0_int.npy', allow_pickle=True) #M_list\n",
    "# # print('M_list=',M_load.item())\n",
    "# print('conv1', M_load.item()['conv1'])#, M_load.item()[key])\n",
    "# for n,key in enumerate(M_key):\n",
    "#     if(n<=150):\n",
    "#         print(n, key, M_load.item()[key].shape)#, M_load.item()[key])\n",
    "#         M0=(M*2**16).type(torch.int32)\n",
    "#         print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['conv1.weight', 'conv1.bias', 'features.0.conv1.weight', 'features.0.conv1.bias', 'features.0.conv2.weight', 'features.0.conv2.bias', 'features.1.conv1.weight', 'features.1.conv1.bias', 'features.1.conv2.weight', 'features.1.conv2.bias', 'features.1.conv3.weight', 'features.1.conv3.bias', 'features.2.conv1.weight', 'features.2.conv1.bias', 'features.2.conv2.weight', 'features.2.conv2.bias', 'features.2.conv3.weight', 'features.2.conv3.bias', 'features.3.conv1.weight', 'features.3.conv1.bias', 'features.3.conv2.weight', 'features.3.conv2.bias', 'features.3.conv3.weight', 'features.3.conv3.bias', 'features.4.conv1.weight', 'features.4.conv1.bias', 'features.4.conv2.weight', 'features.4.conv2.bias', 'features.4.conv3.weight', 'features.4.conv3.bias', 'features.5.conv1.weight', 'features.5.conv1.bias', 'features.5.conv2.weight', 'features.5.conv2.bias', 'features.5.conv3.weight', 'features.5.conv3.bias', 'features.6.conv1.weight', 'features.6.conv1.bias', 'features.6.conv2.weight', 'features.6.conv2.bias', 'features.6.conv3.weight', 'features.6.conv3.bias', 'features.7.conv1.weight', 'features.7.conv1.bias', 'features.7.conv2.weight', 'features.7.conv2.bias', 'features.7.conv3.weight', 'features.7.conv3.bias', 'features.8.conv1.weight', 'features.8.conv1.bias', 'features.8.conv2.weight', 'features.8.conv2.bias', 'features.8.conv3.weight', 'features.8.conv3.bias', 'features.9.conv1.weight', 'features.9.conv1.bias', 'features.9.conv2.weight', 'features.9.conv2.bias', 'features.9.conv3.weight', 'features.9.conv3.bias', 'features.10.conv1.weight', 'features.10.conv1.bias', 'features.10.conv2.weight', 'features.10.conv2.bias', 'features.10.conv3.weight', 'features.10.conv3.bias', 'features.11.conv1.weight', 'features.11.conv1.bias', 'features.11.conv2.weight', 'features.11.conv2.bias', 'features.11.conv3.weight', 'features.11.conv3.bias', 'features.12.conv1.weight', 'features.12.conv1.bias', 'features.12.conv2.weight', 'features.12.conv2.bias', 'features.12.conv3.weight', 'features.12.conv3.bias', 'features.13.conv1.weight', 'features.13.conv1.bias', 'features.13.conv2.weight', 'features.13.conv2.bias', 'features.13.conv3.weight', 'features.13.conv3.bias', 'features.14.conv1.weight', 'features.14.conv1.bias', 'features.14.conv2.weight', 'features.14.conv2.bias', 'features.14.conv3.weight', 'features.14.conv3.bias', 'features.15.conv1.weight', 'features.15.conv1.bias', 'features.15.conv2.weight', 'features.15.conv2.bias', 'features.15.conv3.weight', 'features.15.conv3.bias', 'features.16.conv1.weight', 'features.16.conv1.bias', 'features.16.conv2.weight', 'features.16.conv2.bias', 'features.16.conv3.weight', 'features.16.conv3.bias', 'conv2.weight', 'conv2.bias', 'deconv_layers0.weight', 'deconv_layers0.bias', 'deconv_layers1.weight', 'deconv_layers1.bias', 'deconv_layers2.weight', 'deconv_layers2.bias', 'deconv_layers3.weight', 'deconv_layers3.bias', 'deconv_layers4.weight', 'deconv_layers4.bias', 'deconv_layers5.weight', 'deconv_layers5.bias', 'final_layer.weight'])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "#导入量化后的int权重  但由于模型中的weight/bias参数仍是float32类型的，因此保存时还是会保存成float32.\n",
    "print(remapped_state.keys())\n",
    "bnfuse_model.load_state_dict(remapped_state)\n",
    "#修改权重数据的类型为int32\n",
    "# for n,param_key in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_key,'\\t',bnfuse_model.state_dict()[param_key].size())\n",
    "#     eval('bnfuse_model.'+param_key+'.data.type(torch.int32)')\n",
    "\n",
    "# for name, module in bnfuse_model.named_modules():\n",
    "#     if type(module) in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n",
    "#         print(name, module)\n",
    "#         if(name=='final_layer'): #没有bias\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#         else: #conv2d, ConvTranspose2d\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#             eval('bnfuse_model.'+name+'.bias.data.type(torch.int32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {'model': bnfuse_model.module.state_dict() if hasattr(bnfuse_model, 'module') else bnfuse_model.state_dict()}\n",
    "parent_path='../output/weights_quan_deconv3/'\n",
    "# torch.save(ckpt, parent_path+'float_mobilenetpose_nobn_refactor_deconv3.pt')\n",
    "# torch.save(ckpt, '../output/weights_quan/post_int_mobilenetpose_nobn_refactor.pt')\n",
    "# torch.save(ckpt, '../output/weights_quan/int_mobilenetpose_nobn_refactor.pt')\n",
    "torch.save(ckpt, parent_path+'int_mobilenetpose_nobn_refactor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 conv1.weight \t torch.Size([16, 3, 3, 3])\n1 conv1.bias \t torch.Size([16])\n2 features.0.conv1.weight \t torch.Size([16, 1, 3, 3])\n3 features.0.conv1.bias \t torch.Size([16])\n4 features.0.conv2.weight \t torch.Size([8, 16, 1, 1])\n5 features.0.conv2.bias \t torch.Size([8])\n6 features.1.conv1.weight \t torch.Size([48, 8, 1, 1])\n7 features.1.conv1.bias \t torch.Size([48])\n8 features.1.conv2.weight \t torch.Size([48, 1, 3, 3])\n9 features.1.conv2.bias \t torch.Size([48])\n10 features.1.conv3.weight \t torch.Size([16, 48, 1, 1])\n11 features.1.conv3.bias \t torch.Size([16])\n12 features.2.conv1.weight \t torch.Size([96, 16, 1, 1])\n13 features.2.conv1.bias \t torch.Size([96])\n14 features.2.conv2.weight \t torch.Size([96, 1, 3, 3])\n15 features.2.conv2.bias \t torch.Size([96])\n16 features.2.conv3.weight \t torch.Size([16, 96, 1, 1])\n17 features.2.conv3.bias \t torch.Size([16])\n18 features.3.conv1.weight \t torch.Size([96, 16, 1, 1])\n19 features.3.conv1.bias \t torch.Size([96])\n20 features.3.conv2.weight \t torch.Size([96, 1, 3, 3])\n21 features.3.conv2.bias \t torch.Size([96])\n22 features.3.conv3.weight \t torch.Size([16, 96, 1, 1])\n23 features.3.conv3.bias \t torch.Size([16])\n24 features.4.conv1.weight \t torch.Size([96, 16, 1, 1])\n25 features.4.conv1.bias \t torch.Size([96])\n26 features.4.conv2.weight \t torch.Size([96, 1, 3, 3])\n27 features.4.conv2.bias \t torch.Size([96])\n28 features.4.conv3.weight \t torch.Size([16, 96, 1, 1])\n29 features.4.conv3.bias \t torch.Size([16])\n30 features.5.conv1.weight \t torch.Size([96, 16, 1, 1])\n31 features.5.conv1.bias \t torch.Size([96])\n32 features.5.conv2.weight \t torch.Size([96, 1, 3, 3])\n33 features.5.conv2.bias \t torch.Size([96])\n34 features.5.conv3.weight \t torch.Size([16, 96, 1, 1])\n35 features.5.conv3.bias \t torch.Size([16])\n36 features.6.conv1.weight \t torch.Size([96, 16, 1, 1])\n37 features.6.conv1.bias \t torch.Size([96])\n38 features.6.conv2.weight \t torch.Size([96, 1, 3, 3])\n39 features.6.conv2.bias \t torch.Size([96])\n40 features.6.conv3.weight \t torch.Size([32, 96, 1, 1])\n41 features.6.conv3.bias \t torch.Size([32])\n42 features.7.conv1.weight \t torch.Size([192, 32, 1, 1])\n43 features.7.conv1.bias \t torch.Size([192])\n44 features.7.conv2.weight \t torch.Size([192, 1, 3, 3])\n45 features.7.conv2.bias \t torch.Size([192])\n46 features.7.conv3.weight \t torch.Size([32, 192, 1, 1])\n47 features.7.conv3.bias \t torch.Size([32])\n48 features.8.conv1.weight \t torch.Size([192, 32, 1, 1])\n49 features.8.conv1.bias \t torch.Size([192])\n50 features.8.conv2.weight \t torch.Size([192, 1, 3, 3])\n51 features.8.conv2.bias \t torch.Size([192])\n52 features.8.conv3.weight \t torch.Size([32, 192, 1, 1])\n53 features.8.conv3.bias \t torch.Size([32])\n54 features.9.conv1.weight \t torch.Size([192, 32, 1, 1])\n55 features.9.conv1.bias \t torch.Size([192])\n56 features.9.conv2.weight \t torch.Size([192, 1, 3, 3])\n57 features.9.conv2.bias \t torch.Size([192])\n58 features.9.conv3.weight \t torch.Size([32, 192, 1, 1])\n59 features.9.conv3.bias \t torch.Size([32])\n60 features.10.conv1.weight \t torch.Size([192, 32, 1, 1])\n61 features.10.conv1.bias \t torch.Size([192])\n62 features.10.conv2.weight \t torch.Size([192, 1, 3, 3])\n63 features.10.conv2.bias \t torch.Size([192])\n64 features.10.conv3.weight \t torch.Size([48, 192, 1, 1])\n65 features.10.conv3.bias \t torch.Size([48])\n66 features.11.conv1.weight \t torch.Size([288, 48, 1, 1])\n67 features.11.conv1.bias \t torch.Size([288])\n68 features.11.conv2.weight \t torch.Size([288, 1, 3, 3])\n69 features.11.conv2.bias \t torch.Size([288])\n70 features.11.conv3.weight \t torch.Size([48, 288, 1, 1])\n71 features.11.conv3.bias \t torch.Size([48])\n72 features.12.conv1.weight \t torch.Size([288, 48, 1, 1])\n73 features.12.conv1.bias \t torch.Size([288])\n74 features.12.conv2.weight \t torch.Size([288, 1, 3, 3])\n75 features.12.conv2.bias \t torch.Size([288])\n76 features.12.conv3.weight \t torch.Size([48, 288, 1, 1])\n77 features.12.conv3.bias \t torch.Size([48])\n78 features.13.conv1.weight \t torch.Size([288, 48, 1, 1])\n79 features.13.conv1.bias \t torch.Size([288])\n80 features.13.conv2.weight \t torch.Size([288, 1, 3, 3])\n81 features.13.conv2.bias \t torch.Size([288])\n82 features.13.conv3.weight \t torch.Size([80, 288, 1, 1])\n83 features.13.conv3.bias \t torch.Size([80])\n84 features.14.conv1.weight \t torch.Size([480, 80, 1, 1])\n85 features.14.conv1.bias \t torch.Size([480])\n86 features.14.conv2.weight \t torch.Size([480, 1, 3, 3])\n87 features.14.conv2.bias \t torch.Size([480])\n88 features.14.conv3.weight \t torch.Size([80, 480, 1, 1])\n89 features.14.conv3.bias \t torch.Size([80])\n90 features.15.conv1.weight \t torch.Size([480, 80, 1, 1])\n91 features.15.conv1.bias \t torch.Size([480])\n92 features.15.conv2.weight \t torch.Size([480, 1, 3, 3])\n93 features.15.conv2.bias \t torch.Size([480])\n94 features.15.conv3.weight \t torch.Size([80, 480, 1, 1])\n95 features.15.conv3.bias \t torch.Size([80])\n96 features.16.conv1.weight \t torch.Size([480, 80, 1, 1])\n97 features.16.conv1.bias \t torch.Size([480])\n98 features.16.conv2.weight \t torch.Size([480, 1, 3, 3])\n99 features.16.conv2.bias \t torch.Size([480])\n100 features.16.conv3.weight \t torch.Size([160, 480, 1, 1])\n101 features.16.conv3.bias \t torch.Size([160])\n102 conv2.weight \t torch.Size([128, 160, 1, 1])\n103 conv2.bias \t torch.Size([128])\n104 deconv_layers0.weight \t torch.Size([128, 1, 3, 3])\n105 deconv_layers0.bias \t torch.Size([128])\n106 deconv_layers1.weight \t torch.Size([128, 128, 1, 1])\n107 deconv_layers1.bias \t torch.Size([128])\n108 deconv_layers2.weight \t torch.Size([128, 1, 3, 3])\n109 deconv_layers2.bias \t torch.Size([128])\n110 deconv_layers3.weight \t torch.Size([128, 128, 1, 1])\n111 deconv_layers3.bias \t torch.Size([128])\n112 deconv_layers4.weight \t torch.Size([128, 1, 3, 3])\n113 deconv_layers4.bias \t torch.Size([128])\n114 deconv_layers5.weight \t torch.Size([128, 128, 1, 1])\n115 deconv_layers5.bias \t torch.Size([128])\n116 final_layer.weight \t torch.Size([17, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "####################################### bnfuse float model ############################################ \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "parent_path='../output/weights_quan_deconv3/'\n",
    "float_nobn_dict=torch.load(parent_path+'float_mobilenetpose_nobn_refactor_deconv3.pt')['model'] #可以直接导入权重，可以不需要加载模型\n",
    "# print(float_nobn_dict.keys())\n",
    "\n",
    "for n,param_key in enumerate(float_nobn_dict):\n",
    "    #打印 key value字典\n",
    "    print(n, param_key,'\\t',float_nobn_dict[param_key].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n",
      "conv1\n",
      "2\n",
      "features.0.conv1\n",
      "2 features.2.conv1 : tensor([0.0739]) features.3.conv1 : tensor([0.1108]) tensor([0.6670])\n",
      "4 features.4.conv1 : tensor([0.0533]) features.6.conv1 : tensor([0.1001]) tensor([0.5321])\n",
      "5 features.5.conv1 : tensor([0.0737]) features.6.conv1 : tensor([0.1001]) tensor([0.7364])\n",
      "7 features.7.conv1 : tensor([0.0673]) features.10.conv1 : tensor([0.0809]) tensor([0.8318])\n",
      "8 features.8.conv1 : tensor([0.0694]) features.10.conv1 : tensor([0.0809]) tensor([0.8582])\n",
      "9 features.9.conv1 : tensor([0.0741]) features.10.conv1 : tensor([0.0809]) tensor([0.9159])\n",
      "11 features.11.conv1 : tensor([0.0504]) features.13.conv1 : tensor([0.1270]) tensor([0.3967])\n",
      "12 features.12.conv1 : tensor([0.0649]) features.13.conv1 : tensor([0.1270]) tensor([0.5110])\n",
      "14 features.14.conv1 : tensor([0.0561]) features.16.conv1 : tensor([0.0963]) tensor([0.5824])\n",
      "15 features.15.conv1 : tensor([0.0672]) features.16.conv1 : tensor([0.0963]) tensor([0.6979])\n",
      "0 conv1 tensor([0.0207]) tensor([0.0207])\n",
      "1 features.0.conv1 tensor([0.6154]) tensor([0.6154])\n",
      "2 features.0.conv2 tensor([0.4402]) tensor([0.4402])\n",
      "3 features.1.conv1 tensor([0.1598]) tensor([0.1598])\n",
      "4 features.1.conv2 tensor([0.3061]) tensor([0.3061])\n",
      "5 features.1.conv3 tensor([0.1691]) tensor([0.1691])\n",
      "6 features.2.conv1 tensor([0.1108]) tensor([0.0739])\n",
      "7 features.2.conv2 tensor([0.1171]) tensor([0.1171])\n",
      "8 features.2.conv3 tensor([0.0965]) tensor([0.0965])\n",
      "9 features.3.conv1 tensor([0.1108]) tensor([0.1108])\n",
      "10 features.3.conv2 tensor([0.1461]) tensor([0.1461])\n",
      "11 features.3.conv3 tensor([0.0586]) tensor([0.0586])\n",
      "12 features.4.conv1 tensor([0.1001]) tensor([0.0533])\n",
      "13 features.4.conv2 tensor([0.0547]) tensor([0.0547])\n",
      "14 features.4.conv3 tensor([0.0491]) tensor([0.0491])\n",
      "15 features.5.conv1 tensor([0.1001]) tensor([0.0737])\n",
      "16 features.5.conv2 tensor([0.0684]) tensor([0.0684])\n",
      "17 features.5.conv3 tensor([0.0606]) tensor([0.0606])\n",
      "18 features.6.conv1 tensor([0.1001]) tensor([0.1001])\n",
      "19 features.6.conv2 tensor([0.0964]) tensor([0.0964])\n",
      "20 features.6.conv3 tensor([0.1224]) tensor([0.1224])\n",
      "21 features.7.conv1 tensor([0.0809]) tensor([0.0673])\n",
      "22 features.7.conv2 tensor([0.0506]) tensor([0.0506])\n",
      "23 features.7.conv3 tensor([0.0470]) tensor([0.0470])\n",
      "24 features.8.conv1 tensor([0.0809]) tensor([0.0694])\n",
      "25 features.8.conv2 tensor([0.0583]) tensor([0.0583])\n",
      "26 features.8.conv3 tensor([0.0574]) tensor([0.0574])\n",
      "27 features.9.conv1 tensor([0.0809]) tensor([0.0741])\n",
      "28 features.9.conv2 tensor([0.0537]) tensor([0.0537])\n",
      "29 features.9.conv3 tensor([0.0466]) tensor([0.0466])\n",
      "30 features.10.conv1 tensor([0.0809]) tensor([0.0809])\n",
      "31 features.10.conv2 tensor([0.1077]) tensor([0.1077])\n",
      "32 features.10.conv3 tensor([0.0798]) tensor([0.0798])\n",
      "33 features.11.conv1 tensor([0.1270]) tensor([0.0504])\n",
      "34 features.11.conv2 tensor([0.0887]) tensor([0.0887])\n",
      "35 features.11.conv3 tensor([0.0865]) tensor([0.0865])\n",
      "36 features.12.conv1 tensor([0.1270]) tensor([0.0649])\n",
      "37 features.12.conv2 tensor([0.0801]) tensor([0.0801])\n",
      "38 features.12.conv3 tensor([0.2134]) tensor([0.2134])\n",
      "39 features.13.conv1 tensor([0.1270]) tensor([0.1270])\n",
      "40 features.13.conv2 tensor([0.0731]) tensor([0.0731])\n",
      "41 features.13.conv3 tensor([0.1596]) tensor([0.1596])\n",
      "42 features.14.conv1 tensor([0.0963]) tensor([0.0561])\n",
      "43 features.14.conv2 tensor([0.1439]) tensor([0.1439])\n",
      "44 features.14.conv3 tensor([0.0714]) tensor([0.0714])\n",
      "45 features.15.conv1 tensor([0.0963]) tensor([0.0672])\n",
      "46 features.15.conv2 tensor([0.0942]) tensor([0.0942])\n",
      "47 features.15.conv3 tensor([0.1027]) tensor([0.1027])\n",
      "48 features.16.conv1 tensor([0.0963]) tensor([0.0963])\n",
      "49 features.16.conv2 tensor([0.1005]) tensor([0.1005])\n",
      "50 features.16.conv3 tensor([0.2435]) tensor([0.2435])\n",
      "51 conv2 tensor([0.0605]) tensor([0.0605])\n",
      "52 deconv_layers0 tensor([0.2622]) tensor([0.2622])\n",
      "53 deconv_layers1 tensor([0.6318]) tensor([0.6318])\n",
      "54 deconv_layers2 tensor([0.3525]) tensor([0.3525])\n",
      "55 deconv_layers3 tensor([0.6455]) tensor([0.6455])\n",
      "56 deconv_layers4 tensor([0.4271]) tensor([0.4271])\n",
      "57 deconv_layers5 tensor([1.2943]) tensor([1.2943])\n",
      "58 final_layer tensor([0.0115]) tensor([0.0115])\n",
      "M0_float: tensor([3.5848e-05, 4.7000e-05, 4.3831e-05, 5.8180e-05, 5.3682e-05, 1.2144e-04,\n",
      "        3.9237e-05, 4.1856e-05, 3.9902e-05, 3.6905e-05, 4.3240e-05, 4.1776e-05,\n",
      "        3.4622e-05, 4.6382e-05, 4.4576e-05, 8.6324e-05, 6.8203e-05])\n"
     ]
    }
   ],
   "source": [
    "''' ******************************************************************  shortcut处的另一种计算方式 *************************************************************** '''\n",
    "'''shortcut处使用同一scale，则x处的表示范围会变小，由此带来误差；   直接用输出的的scale计算？ \n",
    "    因为ascale改变了，所以除了M需要改变，bias也需要改变\n",
    "但如果使用不同scale, 则需要硬件上进行*M0并移位的操作，操作更多，而且是整型计算，同样会带来误差'''\n",
    "Mkey_load = np.load('../output/weights_quan/M_key.npy', allow_pickle=True)  #type:<class 'numpy.ndarray'>\n",
    "Mkey_load=list(Mkey_load)\n",
    "print(type(Mkey_load))\n",
    "print(Mkey_load[0])\n",
    "print(Mkey_load.index('features.0.conv2'))\n",
    "print(Mkey_load[Mkey_load.index('features.0.conv2')-1])\n",
    "\n",
    "shortcut_list=['features.2.conv1','features.4.conv1','features.5.conv1','features.7.conv1','features.8.conv1','features.9.conv1','features.11.conv1','features.12.conv1','features.14.conv1','features.15.conv1']\n",
    "shortcut_layer=[2,4,5,7,8,9,11,12,14,15]\n",
    "# shortcut_index=[6, 12, 15, 21, 24, 27, 33, 36, 42, 45]\n",
    "# shortcut_corr_index=[9, 18, 18, 30, 30, 30, 39, 39, 48, 48]\n",
    "shortcut_dict={6:9, 12:18, 15:18,21:30, 24:30, 27:30, 33:39, 36:39, 42:48, 45:48}\n",
    "\n",
    "# print(shortcut_index)\n",
    "int_weight_nobn_dict = {}\n",
    "ascale_shortcut_same_list = {}  \n",
    "Mscale_shortcut_same_list = {}  \n",
    "M0_float_list = {}\n",
    "M0_int_list = {}\n",
    "ascale_load = np.load(parent_path+'ascale.npy', allow_pickle=True) \n",
    "# oscale_load = np.load(parent_path+'oscale.npy', allow_pickle=True) \n",
    "wscale_load = np.load(parent_path+'wscale.npy', allow_pickle=True) \n",
    "#重设ascale  shortcut处使用同一scale\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    ascale=ascale_load.item()[key]\n",
    "    if(n in shortcut_dict.keys()):\n",
    "        index=shortcut_dict[n]\n",
    "        ascale_pre=ascale\n",
    "        ascale=ascale_load.item()[Mkey_load[index]]\n",
    "        # print(n//3, key, ':', ascale_pre, Mkey_load[index], ':', ascale, ascale_pre/ascale)\n",
    "    ascale_shortcut_same_list[key] = ascale \n",
    "np.save(parent_path+'ascale_shortcut0.npy', ascale_shortcut_same_list)\n",
    "\n",
    "\n",
    "# 读取ascale结果\n",
    "ascale_shortcut_load = np.load(parent_path+'ascale_shortcut0.npy', allow_pickle=True) #M_list\n",
    "# print('conv1', ascale_shortcut_load.item()['conv1'])#, M_load.item()[key])\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    ascale0=ascale_load.item()[key]\n",
    "    ascale=ascale_shortcut_load.item()[key]\n",
    "    wscale=wscale_load.item()[key]\n",
    "    # print(n, key, ascale_shortcut_load.item()[key], ascale_load.item()[key])\n",
    "    # print(n, key, ascale, ascale0)\n",
    "    \n",
    "    #计算权重和偏置int量化结果\n",
    "    conv_weight=float_nobn_dict[key+'.weight']\n",
    "    if(n!=58):#不是final_layer\n",
    "        #计算q_weight, q_bias\n",
    "        conv_bias=float_nobn_dict[key+'.bias']\n",
    "        # print(conv_weight.shape, conv_bias.shape, wscale.shape)\n",
    "        q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale.unsqueeze(dim=-1), ascale, num_bits=8) \n",
    "        int_weight_nobn_dict[key+'.weight'] = q_weight\n",
    "        int_weight_nobn_dict[key+'.bias'] = q_bias\n",
    "        #计算Mscale\n",
    "        key_post = Mkey_load[Mkey_load.index(key)+1] \n",
    "        Mscale=wscale*ascale/ascale_shortcut_load.item()[key_post]  #不用relu的量化反量化，重新计算M值\n",
    "        M0=(Mscale*2**16).type(torch.int32)\n",
    "        M0_float=M0*2**(-16)\n",
    "    else: #final_layer\n",
    "        #计算q_weight, q_bias\n",
    "        q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale.unsqueeze(dim=-1), ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "        int_weight_nobn_dict[key+'.weight'] = q_weight\n",
    "        #计算Mscale\n",
    "        Mscale=wscale*ascale\n",
    "        M0_float=Mscale\n",
    "        print('M0_float:',M0_float.flatten())\n",
    "        '''M0_float: tensor([3.9311e-05, 5.1540e-05, 4.8065e-05, 5.7022e-05, 5.8868e-05, 1.3317e-04,\n",
    "        4.3028e-05, 4.5899e-05, 4.3756e-05, 4.0470e-05, 4.7417e-05, 4.5811e-05,\n",
    "        3.7967e-05, 5.0862e-05, 4.8882e-05, 9.4663e-05, 7.4792e-05])'''\n",
    "        M0=torch.tensor(2**16) #乘以2**16,再除以2**16是原值\n",
    "    \n",
    "    # print(n, q_weight, q_bias)\n",
    "    Mscale_shortcut_same_list[key] = Mscale #torch.squeeze(Mscale, dim=-1)\n",
    "    M0_float_list[key] = M0_float\n",
    "    M0_int_list[key] = M0 \n",
    "\n",
    "torch.save(int_weight_nobn_dict, parent_path+'int_mobilenetpose_shortcut0.pt')\n",
    "np.save(parent_path+'Mscale_shortcut0.npy', Mscale_shortcut_same_list)\n",
    "np.save(parent_path+'M0_float_shortcut0.npy', M0_float_list)\n",
    "np.save(parent_path+'M0_int_shortcut0.npy', M0_int_list)\n",
    "# 读取ascale结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab342d38c9089b27086ef2ff31e6c6863533cd1c8642810b45a542962dd3699"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}