{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import _init_paths\n",
    "from core.config import config\n",
    "from core.config import update_config\n",
    "from core.config import update_dir\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models\n",
    "\n",
    "import quantize_dorefa\n",
    "# from quantize_iao import *\n",
    "from quantize_iao_deconv3 import *\n",
    "# from quantize_iao_uint import *  #对feature map进行uint对称量化\n",
    "\n",
    "import numpy as np\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "np.set_printoptions(threshold = np.inf) \n",
    "\n",
    "# # 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_conv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConv2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv\n",
    "\n",
    "def bn_fuse_deconv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConvTranspose2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         output_padding=bn_conv.output_padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_module(module, device):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, QuantBNFuseConv2d):\n",
    "            bn_fused_conv = bn_fuse_conv(child, device)\n",
    "            module._modules[name] = bn_fused_conv\n",
    "        elif isinstance(child, QuantBNFuseConvTranspose2d):\n",
    "            bn_fused_deconv = bn_fuse_deconv(child, device)\n",
    "            module._modules[name] = bn_fused_deconv\n",
    "        else:\n",
    "            bn_fuse_module(child, device)\n",
    "\n",
    "\n",
    "def model_bn_fuse(model, inplace=False):\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "    device = next(model.parameters()).device\n",
    "    bn_fuse_module(model,device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device='', apex=False, batch_size=None):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    cpu_request = device.lower() == 'cpu'\n",
    "    if device and not cpu_request:  # if device requested other than 'cpu'\n",
    "        # os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity\n",
    "\n",
    "    cuda = False if cpu_request else torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        c = 1024 ** 2  # bytes to MB\n",
    "        ng = torch.cuda.device_count()\n",
    "        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
    "            assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)\n",
    "        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n",
    "        s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex\n",
    "        for i in range(0, ng):\n",
    "            if i == 1:\n",
    "                s = ' ' * len(s)\n",
    "            print(\"%sdevice%g _CudaDeviceProperties(name='%s', total_memory=%dMB)\" %\n",
    "                  (s, i, x[i].name, x[i].total_memory / c))\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    print('')  # skip a line\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models.pose_mobilenet_relu.get_pose_net\n"
     ]
    }
   ],
   "source": [
    "# cfg='../experiments/coco/resnet50/mobile_quant_relu_int.yaml'   #MODEL_FILE:'output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt' 生成 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "cfg='../experiments/coco/resnet50/mobile_quant_relu_int_deconv3.yaml'   #MODEL_FILE:'output/weights_quan_deconv3/int8_mobilenet8_relu_bnfuse_deconv3_float.pt' 生成 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "update_config(cfg)\n",
    "# cudnn related setting\n",
    "cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n",
    "\n",
    "# for shufflenetv2\n",
    "shufflenetv2_spec = {'0.5': ([4, 8, 4], [24, 48, 96, 192, 1024]),\n",
    "                        '1.0': ([4, 8, 4], [24, 116, 232, 464, 1024]),\n",
    "                        '1.5': ([4, 8, 4], [24, 176, 352, 704, 1024]),\n",
    "                        '2.0': ([4, 8, 4], [24, 244, 488, 976, 2048])}\n",
    "stages_repeats, stages_out_channels = shufflenetv2_spec['1.0']\n",
    "print('models.'+config.MODEL.NAME+'.get_pose_net')\n",
    "model = eval('models.'+config.MODEL.NAME+'.get_pose_net')(\n",
    "        config, \n",
    "        stages_repeats, stages_out_channels,\n",
    "        is_train=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################### bnfuse model ############################################\n",
    "bnfuse_model = eval('models.pose_mobilenet_relu_bnfuse.get_pose_net')(\n",
    "    config, \n",
    "    stages_repeats, stages_out_channels,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32510MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = [int(i) for i in config.GPUS.split(',')]\n",
    "device = select_device(config.GPUS, batch_size=config.TEST.BATCH_SIZE*len(gpus))\n",
    "\n",
    "model = model.to(device)\n",
    "# summary(model,input_size=(3, 256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_bits= 8 \tw_bits= 8 \tq_type= 0 \tq_level= 0 \tdevice= cuda:0 \tweight_observer= 0 \tbn_fuse= 1 \tquant_inference= False\n"
     ]
    }
   ],
   "source": [
    "#print('*******************ori_model*******************\\n', model)\n",
    "if(config.QUANTIZATION.QUANT_METHOD == 1): # DoReFa\n",
    "    quantize_dorefa.prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS, quant_inference=config.QUANTIZATION.QUANT_INFERENCE, is_activate=False)\n",
    "else: #default quant_method == 0   IAO\n",
    "    prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS,q_type=config.QUANTIZATION.Q_TYPE, q_level=config.QUANTIZATION.Q_LEVEL, device=device,#device=next(model.parameters()).device, \n",
    "                        weight_observer=config.QUANTIZATION.WEIGHT_OBSERVER, bn_fuse=config.QUANTIZATION.BN_FUSE, quant_inference=config.QUANTIZATION.QUANT_INFERENCE)\n",
    "#print('\\n*******************quant_model*******************\\n', model)\n",
    "# print('\\n*******************Using quant_model in test*******************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.TEST.MODEL_FILE:\n",
    "#     # logger.info('=> loading model from {}'.format(config.TEST.MODEL_FILE))\n",
    "#     if(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint.pth.tar'):\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         #model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=torch.device('cuda'))['state_dict'])\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device)['state_dict'])\n",
    "#         #torch.save(model.module.state_dict(), 'output/coco_quan/mobile_quant_relu_w8a8_bnfuse0/checkpoint_nomodule.pth.tar')\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='model_best.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint_resave.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     else:  #final_state.pth.tar\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************For inference bn_fuse quant_model*******************\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# ********************* quant_bn_fused_model_inference **********************\n",
    "model.to(device)\n",
    "model_bn_fuse(model, inplace=True)  # bn融合\n",
    "# print('\\n*******************For inference bn_fuse quant_model*******************\\n', model)\n",
    "# ckpt = {'model': model.module.state_dict() if hasattr(model, 'module') else model.state_dict()}\n",
    "# torch.save(ckpt, '../output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt')\n",
    "print('*******************For inference bn_fuse quant_model*******************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/weights_quan_deconv3/int8_mobilenet8_relu_bnfuse_deconv3_float.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(config.TEST.MODEL_FILE)\n",
    "model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device)['model'])  ##为什么还在'model'里面呀？\n",
    "# model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device))  ##为什么还在'model'里面呀？\n",
    "# model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model.state_dict:\n"
     ]
    }
   ],
   "source": [
    "remapped_state = {}\n",
    "print('Model.state_dict:')\n",
    "# ######################################## before #######################################\n",
    "# for n,param_tensor in enumerate(model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     # if(n<5):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     # print(model.state_dict()[param_tensor])\n",
    "\n",
    "# ######################################### after #######################################\n",
    "# for n,param_tensor in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',bnfuse_model.state_dict()[param_tensor].size())\n",
    "#     # if(n<4):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     print(model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor后 #############################\n",
    "# import re\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]=='features'):\n",
    "#         k[1]=str(int(k[1])+1)\n",
    "#         k[-2]=str((int(k[2][-1])-1)*3)\n",
    "#         k[2]='conv'\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     elif(k[0].startswith('deconv_layers')): #final_layer\n",
    "#         number=3*int(k[0][-1])\n",
    "#         remapped_state_key='deconv_layers.'+str(number)+'.'+k[-1] #weight/bias\n",
    "#     elif(k[0]=='conv1'): #final_layer\n",
    "#         remapped_state_key='features.0.0.'+k[-1] #weight/bias\n",
    "#     else: #final_layer  conv2\n",
    "#         remapped_state_key=state_key\n",
    "#     # print(n, state_key, remapped_state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################# 生成浮点权重 refactor前 #############################\n",
    "# remapped_state = {}\n",
    "# for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "#     k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#     if(k[0]!='final_layer'):\n",
    "#         number = int(k[-2])//2*3 \n",
    "#         # print(number)\n",
    "#         k[-2]=str(number)\n",
    "#         # print(k)\n",
    "#         remapped_state_key=('.').join(k) #进行重映射\n",
    "#     else: #final_layer\n",
    "#         remapped_state_key=state_key\n",
    "#     print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#     remapped_state[state_key]= model.state_dict()[remapped_state_key]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### 量化步骤 ##############################\n",
    "## 对称量化，因此zero_point均为0\n",
    "def quantize_tensor(weight, bias, wscale, ascale, num_bits=8):\n",
    "    qmin = -2**(num_bits-1)  #8bit [-128,127]\n",
    "    qmax = 2**(num_bits-1) - 1\n",
    "    q_weight= torch.round(weight/wscale).clamp_(qmin, qmax).type(torch.int32)\n",
    "    # print('0:', torch.max(torch.round(weight/wscale)), torch.min(torch.round(weight/wscale)))\n",
    "    q_bias= torch.round(bias/wscale.flatten()/ascale).type(torch.int32)  #bias没有进行截断\n",
    "    return q_weight, q_bias\n",
    "\n",
    "#反量化回浮点结果\n",
    "def dequantize_tensor(q_weight, q_bias, wscale, ascale):\n",
    "    return wscale*q_weight.float(), wscale.flatten()*ascale*q_bias.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model.state_dict:\n",
      "It is final_layer\n",
      "['conv1', 'features.0.conv1', 'features.0.conv2', 'features.1.conv1', 'features.1.conv2', 'features.1.conv3', 'features.2.conv1', 'features.2.conv2', 'features.2.conv3', 'features.3.conv1', 'features.3.conv2', 'features.3.conv3', 'features.4.conv1', 'features.4.conv2', 'features.4.conv3', 'features.5.conv1', 'features.5.conv2', 'features.5.conv3', 'features.6.conv1', 'features.6.conv2', 'features.6.conv3', 'features.7.conv1', 'features.7.conv2', 'features.7.conv3', 'features.8.conv1', 'features.8.conv2', 'features.8.conv3', 'features.9.conv1', 'features.9.conv2', 'features.9.conv3', 'features.10.conv1', 'features.10.conv2', 'features.10.conv3', 'features.11.conv1', 'features.11.conv2', 'features.11.conv3', 'features.12.conv1', 'features.12.conv2', 'features.12.conv3', 'features.13.conv1', 'features.13.conv2', 'features.13.conv3', 'features.14.conv1', 'features.14.conv2', 'features.14.conv3', 'features.15.conv1', 'features.15.conv2', 'features.15.conv3', 'features.16.conv1', 'features.16.conv2', 'features.16.conv3', 'conv2', 'deconv_layers0', 'deconv_layers1', 'deconv_layers2', 'deconv_layers3', 'deconv_layers4', 'deconv_layers5', 'final_layer']\n"
     ]
    }
   ],
   "source": [
    "############################################################## 定点结果导出 类似后训练量化，直接从权重中得到最大值最小值 ##############################################\n",
    "remapped_state = {}\n",
    "M_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M_key=[]  #存储M_list中的索引键值\n",
    "oscale_list={} #储存 浮点和整型的oscale\n",
    "ascale_list={} #储存 浮点和整型的ascale\n",
    "wscale_list={} #储存 浮点和整型的wscale\n",
    "\n",
    "count=0\n",
    "#导出权重和偏置至二进制文件中\n",
    "print('Model.state_dict:')\n",
    "for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "    #打印 key value字典\n",
    "    # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "    if(n<=434): #if(n<4*2): if(n<=434):  n==16 or n==8\n",
    "        # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "        ############################################## 以下得到浮点权重值 ###################################################\n",
    "        if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "            layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "            next_layer=''\n",
    "            # print(layer_name+'.weight')\n",
    "            conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "            ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "            ######################################## 使用量化感知训练的权重 #########################################\n",
    "            wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "            ######################################## 直接后训练量化得到权重 #########################################\n",
    "            # inter = torch.flatten(conv_weight, start_dim=1)  #变成两维 [out_channel,-1]\n",
    "            # frange=torch.max(torch.abs(inter), 1)[0].reshape(-1,1,1,1)\n",
    "            # wscale=torch.clamp(frange/127.5, min=1.1920928955078125e-7)  #1.1920928955078125e-7*127.5=1.519918441772461e-05\n",
    "            # print('frange=',frange.flatten(),'\\nwscale=',wscale.flatten())\n",
    "\n",
    "            if(param_key.split('.')[0]!='final_layer'):\n",
    "                conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "                tmp=param_key.split('.')[0:-1]\n",
    "                tmp[-1]=str(int(tmp[-1])+2)\n",
    "                next_layer=('.').join(tmp)\n",
    "                # print(param_key, next_layer)\n",
    "            else:\n",
    "                print('It is final_layer')\n",
    "                # print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况 包括网络第一层、InvertedResidual中包含relu的层、conv2以及deconv_layers\n",
    "#                 # print('0:',next_layer, oscale)\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet 每个InvertedResidual模块的末尾（线性直通，无relu)   但features.17的oscale应该是conv2的ascale\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('1:',next_layer,oscale)\n",
    "#             elif(param_key.split('.')[1]=='17'): #features.17的oscale应该是final_layer的ascale\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('2: conv2.0',oscale)\n",
    "#             else:  #final_layer 不需要oscale    next_layer=='' 没有relu的都不需要oscale\n",
    "#                 oscale=torch.tensor([1.])\n",
    "#                 # print('3: final_layer',oscale)\n",
    "#             # print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(), wscale.shape, '\\noscale=',oscale,oscale.shape) #wscale.flatten()\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 # if(param_key.split('.')[0]=='features'and param_key.split('.')[1]=='0'): #第一层的M需要特殊处理，将输入ascale放入图像预处理中实现，输入网络的数据直接是[-128,127] 似乎又不需要...\n",
    "#                 #     M=wscale/oscale\n",
    "#                 # else:\n",
    "#                 #     M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M=wscale*ascale/oscale\n",
    "#                 # M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n",
    "#                 # print('###: M=',M.flatten())\n",
    "\n",
    "#                 # print(conv_weight.shape, conv_bias.shape)\n",
    "#                 # #*********************************************************************************************************************************\n",
    "#                 # print('conv_weight:',conv_weight.squeeze(), 'conv_bias:',conv_bias.flatten())\n",
    "                \n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 # print(q_weight.shape, q_bias.shape)\n",
    "#                 # print('q_weight:',q_weight.flatten(), 'q_bias:',q_bias.flatten())\n",
    "                \n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 # print('**量化误差**   weight:', torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight),'  bias', torch.mean(conv_bias-dq_bias), torch.max(conv_bias-dq_bias))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "#                 # print('final_layer: M=',M.flatten())\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 # print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "            ################################################### 浮点权重导出 ################################################################\n",
    "            q_weight=conv_weight #浮点权重的导出，需要控制变量呀！！！\n",
    "            q_bias=conv_bias\n",
    "            ################################################################################################################################\n",
    "\n",
    "            ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "            k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "            if(k[0]=='features' and k[1]=='0'): # 网络第一层  features.0. -> conv1\n",
    "                remapped_state_key = 'conv1.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            elif(k[0]=='features'): #除了第一层外的其他层\n",
    "                k[1]=str(int(k[1])-1)\n",
    "                number = int(k[-2])//3 + 1 \n",
    "                # print(number)\n",
    "                k[2]=k[2]+str(number)\n",
    "                # k[3]='0' #现在不需要了\n",
    "                # print(k)\n",
    "                remapped_state_key=('.').join(k[0:3])+'.weight' #进行重映射\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='deconv_layers'): #deconv_layers0-5\n",
    "                number = int(k[-2])//3\n",
    "                remapped_state_key=k[0]+str(number)+'.weight' #进行重映射\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[k[0]+str(number)+'.bias']=q_bias\n",
    "                # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "            elif(k[0]=='final_layer'): #final_layer\n",
    "                remapped_state_key=param_key\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "            else: #conv2 无需进行重映射\n",
    "                remapped_state_key = 'conv2.weight'\n",
    "                remapped_state[remapped_state_key]=q_weight\n",
    "                remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "            # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "            # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "            if(remapped_state_key.split('.')[0]=='features'):\n",
    "                M_key_name = '.'.join(remapped_state_key.split('.')[0:-1])\n",
    "            else:\n",
    "                M_key_name = remapped_state_key.split('.')[0]\n",
    "            M_key.append(M_key_name)\n",
    "#             M_list[M_key_name] = torch.squeeze(M, dim=-1) #M.flatten() torch.Size([16]);  M通道量化torch.Size([16, 1, 1, 1]) -> 需要转换为torch.Size([16, 1, 1])\n",
    "#             oscale_list[M_key_name]=oscale  #储存 浮点和整形的Oscale 是一个值  类似这样oscale= tensor([0.9272]) torch.Size([1])\n",
    "            ascale_list[M_key_name]=ascale  \n",
    "            wscale_list[M_key_name]=torch.squeeze(wscale, dim=-1)  \n",
    "            count += 1\n",
    "\n",
    "#保存M结果\n",
    "print(M_key)\n",
    "parent_path='../output/weights_quan_deconv3/'\n",
    "# np.save('../output/weights_quan/M_key.npy', M_key) #这才真的是list\n",
    "\n",
    "#其实下面的都是假的list,实际是字典\n",
    "# np.save(parent_path+'M_refactor_noreluq.npy', M_list)\n",
    "# np.save(parent_path+'oscale.npy', oscale_list)\n",
    "np.save(parent_path+'ascale.npy', ascale_list)\n",
    "\n",
    "# np.save(parent_path+'M_refactor.npy', M_list)\n",
    "np.save(parent_path+'wscale.npy', wscale_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#127.0 **量化误差**   weight: tensor(0.0004) tensor(0.0275)   bias tensor(7.7337e-06) tensor(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################################## 定点结果导出 refactor后 ##############################################\n",
    "# remapped_state = {}\n",
    "# M_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "# M_key=[]  #存储M_list中的索引键值\n",
    "# oscale_list={} #储存 浮点和整型的oscale\n",
    "# ascale_list={} #储存 浮点和整型的ascale\n",
    "# wscale_list={} #储存 浮点和整型的wscale\n",
    "\n",
    "# count=0\n",
    "# #导出权重和偏置至二进制文件中\n",
    "# print('Model.state_dict:')\n",
    "# for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "#     #打印 key value字典\n",
    "#     # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "#     if(n<=434): #if(n<4*2): if(n<=434):  n==16 or n==8\n",
    "#         # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "#         ############################################## 以下得到浮点权重值 ###################################################\n",
    "#         if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "#             layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "#             next_layer=''\n",
    "#             # print(layer_name+'.weight')\n",
    "#             conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "#             ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "#             wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "#                 tmp=param_key.split('.')[0:-1]\n",
    "#                 tmp[-1]=str(int(tmp[-1])+2)\n",
    "#                 next_layer=('.').join(tmp)\n",
    "#                 print(param_key, next_layer)\n",
    "#             else:\n",
    "#                 print('It is final_layer')\n",
    "#                 print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况 包括网络第一层、InvertedResidual中包含relu的层、conv2以及deconv_layers\n",
    "#                 # print('0:',next_layer, oscale)\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet 每个InvertedResidual模块的末尾（线性直通，无relu)   但features.17的oscale应该是conv2的ascale\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('1:',next_layer,oscale)\n",
    "#             elif(param_key.split('.')[1]=='17'): #features.17的oscale应该是final_layer的ascale\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#                 # print('2: conv2.0',oscale)\n",
    "#             else:  #final_layer 不需要oscale    next_layer=='' 没有relu的都不需要oscale\n",
    "#                 oscale=torch.tensor([1.])\n",
    "#                 # print('3: final_layer',oscale)\n",
    "#             print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(), wscale.shape, '\\noscale=',oscale,oscale.shape) #wscale.flatten()\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 # if(param_key.split('.')[0]=='features'and param_key.split('.')[1]=='0'): #第一层的M需要特殊处理，将输入ascale放入图像预处理中实现，输入网络的数据直接是[-128,127] 似乎又不需要...\n",
    "#                 #     M=wscale/oscale\n",
    "#                 # else:\n",
    "#                 #     M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M=wscale*ascale/oscale\n",
    "#                 # M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n",
    "#                 print('###: M=',M.flatten())\n",
    "\n",
    "#                 print(conv_weight.shape, conv_bias.shape)\n",
    "#                 #*********************************************************************************************************************************\n",
    "#                 print('conv_weight:',conv_weight.squeeze(), 'conv_bias:',conv_bias.flatten())\n",
    "                \n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 print(q_weight.shape, q_bias.shape)\n",
    "#                 print(q_weight.flatten(), q_bias.flatten())\n",
    "                \n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print('weight:', torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight),'  bias', torch.mean(conv_bias-dq_bias), torch.max(conv_bias-dq_bias))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "#                 print('final_layer: M=',M.flatten())\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "#             # ################################################### 浮点权重导出 ################################################################\n",
    "#             # q_weight=conv_weight #浮点权重的导出，需要控制变量呀！！！\n",
    "#             # q_bias=conv_bias\n",
    "#             # ################################################################################################################################\n",
    "\n",
    "#             ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "#             k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#             if(k[0]=='features' and k[1]=='0'): # 网络第一层  features.0. -> conv1\n",
    "#                 remapped_state_key = 'conv1.weight'\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#             elif(k[0]=='features'): #除了第一层外的其他层\n",
    "#                 k[1]=str(int(k[1])-1)\n",
    "#                 number = int(k[-2])//3 + 1 \n",
    "#                 # print(number)\n",
    "#                 k[2]=k[2]+str(number)\n",
    "#                 # k[3]='0' #现在不需要了\n",
    "#                 # print(k)\n",
    "#                 remapped_state_key=('.').join(k[0:3])+'.weight' #进行重映射\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             elif(k[0]=='deconv_layers'): #deconv_layers0-5\n",
    "#                 number = int(k[-2])//3\n",
    "#                 remapped_state_key=k[0]+str(number)+'.weight' #进行重映射\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[k[0]+str(number)+'.bias']=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             elif(k[0]=='final_layer'): #final_layer\n",
    "#                 remapped_state_key=param_key\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#             else: #conv2 无需进行重映射\n",
    "#                 remapped_state_key = 'conv2.weight'\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#             # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#             # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "#             if(remapped_state_key.split('.')[0]=='features'):\n",
    "#                 M_key_name = '.'.join(remapped_state_key.split('.')[0:-1])\n",
    "#             else:\n",
    "#                 M_key_name = remapped_state_key.split('.')[0]\n",
    "#             M_key.append(M_key_name)\n",
    "#             M_list[M_key_name] = torch.squeeze(M, dim=-1) #M.flatten() torch.Size([16]);  M通道量化torch.Size([16, 1, 1, 1]) -> 需要转换为torch.Size([16, 1, 1])\n",
    "#             oscale_list[M_key_name]=oscale  #储存 浮点和整形的Oscale 是一个值  类似这样oscale= tensor([0.9272]) torch.Size([1])\n",
    "#             ascale_list[M_key_name]=ascale  \n",
    "#             wscale_list[M_key_name]=torch.squeeze(M, dim=-1)  \n",
    "#             count += 1\n",
    "\n",
    "# #保存M结果\n",
    "# print(M_key)\n",
    "# np.save('../output/weights_quan/M_refactor.npy', M_list)\n",
    "# np.save('../output/weights_quan/oscale.npy', oscale_list)\n",
    "# np.save('../output/weights_quan/ascale.npy', ascale_list)\n",
    "# np.save('../output/weights_quan/wscale.npy', wscale_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## 定点结果导出 refactor前 ##############################################\n",
    "# remapped_state = {}\n",
    "# M_list = {}\n",
    "# count=0\n",
    "# #导出权重和偏置至二进制文件中\n",
    "# print('Model.state_dict:')\n",
    "# for n,param_key in enumerate(model.state_dict()): # AttributeError: 'collections.OrderedDict' object has no attribute 'key',所以这里不使用model.state_dict().keys()\n",
    "#     #打印 key value字典\n",
    "#     # print(n, param_key,'\\t',model.state_dict()[param_key].size())\n",
    "#     if(n<=434): #if(n<4*2): if(n<=434):\n",
    "#         # print(n, param_key,'\\t',model.state_dict()[param_key].size()) #param_key: features.0.0.activation_quantizer.scale \n",
    "#         ############################################## 以下得到浮点权重值 ###################################################\n",
    "#         if(param_key.split('.')[-1]=='weight'):  #最后一层final_layer没有bias,但最后一层本来就应该要单独设计的？\n",
    "#             layer_name=('.').join(param_key.split('.')[0:-1])\n",
    "#             next_layer=''\n",
    "#             # print(layer_name+'.weight')\n",
    "#             conv_weight=model.state_dict()[layer_name+'.weight'].detach().cpu()\n",
    "#             ascale=model.state_dict()[layer_name+'.activation_quantizer.scale'].detach().cpu()  #对称量化，因此zero_point为0\n",
    "#             wscale=model.state_dict()[layer_name+'.weight_quantizer.scale'].detach().cpu()  # 通道量化  torch.Size([16, 1, 1, 1])\n",
    "\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 conv_bias=model.state_dict()[layer_name+'.bias'].detach().cpu()  # final_layer其实是没有bias的\n",
    "#                 tmp=param_key.split('.')[0:-1]\n",
    "#                 tmp[-1]=str(int(tmp[-1])+2)\n",
    "#                 next_layer=('.').join(tmp)\n",
    "#             else:\n",
    "#                 print('It is final_layer')\n",
    "#                 print(conv_weight.shape)\n",
    "            \n",
    "#             if((next_layer+'.activation_quantizer.scale') in model.state_dict()):\n",
    "#                 # print('0:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()  #存在relu的情况\n",
    "#             elif(param_key.split('.')[1]!='17' and next_layer!=''): #不存在relu的情况  此时是mobilenet InvertedResidual模块的末尾\n",
    "#                 tmp2=param_key.split('.')[0:-1]\n",
    "#                 tmp2[1]=str(int(tmp[1])+1)\n",
    "#                 tmp2[-1]='0'\n",
    "#                 next_layer=('.').join(tmp2)\n",
    "#                 # print('1:',next_layer)\n",
    "#                 oscale=model.state_dict()[next_layer+'.activation_quantizer.scale'].detach().cpu()\n",
    "#             elif(param_key.split('.')[1]!='17'):\n",
    "#                 oscale=model.state_dict()['conv2.0.activation_quantizer.scale'].detach().cpu()\n",
    "#             # print(conv_weight.shape, 'conv_bias=',conv_bias.shape,'\\nascale=',ascale, ascale.shape, '\\nwscale=',wscale.flatten(),wscale.shape, '\\noscale=',oscale,oscale.shape)\n",
    "\n",
    "#             ############################################## 以下进行浮点权重的量化，得到int权重和M ###################################################\n",
    "#             if(param_key.split('.')[0]!='final_layer'):\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale/oscale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 M0=(M*2**16).type(torch.int32)\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale, ascale, num_bits=8)\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "#             else: #final_layer\n",
    "#                 #计算移位值  不同通道的wscale还相差蛮大的（e-02,e-07) 2**16好像还不够  所以先使用M来验证\n",
    "#                 M=wscale*ascale  #一开始的不同通道间M差距很大（e-03,e-09)， 后面就挺均匀的  \n",
    "#                 # M0=(M*2**16).type(torch.int32) #如果最后一层直接使用浮点数进行运算，则不用计算M0\n",
    "#                 # print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(16))/M).flatten()) #\n",
    "\n",
    "#                 #计算权重和偏置int量化结果\n",
    "#                 q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale, ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "#                 # print(q_weight, q_bias)\n",
    "#                 # print(q_bias.shape)\n",
    "#                 #反量化回浮点数的结果\n",
    "#                 dq_weight, dq_bias = dequantize_tensor(q_weight, q_bias, wscale, ascale)\n",
    "#                 # print(dq_weight, dq_bias)\n",
    "#                 print(torch.mean(conv_weight-dq_weight), torch.max(conv_weight-dq_weight))\n",
    "#                 # print(conv_bias-dq_bias)\n",
    "\n",
    "                \n",
    "#             ############################################## 以下将int权重映射给bnfuse_model ###################################################\n",
    "#             k = param_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "#             if(k[0]!='final_layer'):\n",
    "#                 number = int(k[-2])//3*2 \n",
    "#                 # print(number)\n",
    "#                 k[-2]=str(number)\n",
    "#                 # print(k)\n",
    "#                 remapped_state_key=('.').join(k) #进行重映射\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 remapped_state[remapped_state_key.replace('weight','bias')]=q_bias\n",
    "#                 # print(count, n, remapped_state_key.replace('weight','bias'), bnfuse_model.state_dict()[remapped_state_key.replace('weight','bias')].shape)\n",
    "#             else: #final_layer\n",
    "#                 remapped_state_key=param_key\n",
    "#                 remapped_state[remapped_state_key]=q_weight\n",
    "#                 # print(count, n, remapped_state_key, bnfuse_model.state_dict()[remapped_state_key].shape)\n",
    "#             # print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "#             # remapped_state[state_key]= model.state_dict()[remapped_state_key]   \n",
    "#             M_list[remapped_state_key] = M.flatten()\n",
    "#             count += 1\n",
    "\n",
    "# #保存M结果\n",
    "# np.save('../output/weights_quan/M.npy', M_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "conv1\n",
      "2\n",
      "features.0.conv1\n"
     ]
    }
   ],
   "source": [
    "# print(len(M_key), type(M_key), M_key)\n",
    "import numpy as np\n",
    "import torch\n",
    "Mkey_load = np.load('../output/weights_quan/M_key.npy', allow_pickle=True)  #type:<class 'numpy.ndarray'>\n",
    "Mkey_load=list(Mkey_load)\n",
    "print(type(Mkey_load))\n",
    "print(Mkey_load[0])\n",
    "print(Mkey_load.index('features.0.conv2'))\n",
    "print(Mkey_load[Mkey_load.index('features.0.conv2')-1])\n",
    "\n",
    "\n",
    "# 59 ['conv1', 'features.0.conv1', 'features.0.conv2', 'features.1.conv1', 'features.1.conv2', 'features.1.conv3', 'features.2.conv1', 'features.2.conv2', 'features.2.conv3', 'features.3.conv1', 'features.3.conv2', 'features.3.conv3', 'features.4.conv1', 'features.4.conv2', 'features.4.conv3', 'features.5.conv1', 'features.5.conv2', 'features.5.conv3', 'features.6.conv1', 'features.6.conv2', 'features.6.conv3', 'features.7.conv1', 'features.7.conv2', 'features.7.conv3', 'features.8.conv1', 'features.8.conv2', 'features.8.conv3', 'features.9.conv1', 'features.9.conv2', 'features.9.conv3', 'features.10.conv1', 'features.10.conv2', 'features.10.conv3', 'features.11.conv1', 'features.11.conv2', 'features.11.conv3', 'features.12.conv1', 'features.12.conv2', 'features.12.conv3', 'features.13.conv1', 'features.13.conv2', 'features.13.conv3', 'features.14.conv1', 'features.14.conv2', 'features.14.conv3', 'features.15.conv1', 'features.15.conv2', 'features.15.conv3', 'features.16.conv1', 'features.16.conv2', 'features.16.conv3', 'conv2', 'deconv_layers0', 'deconv_layers1', 'deconv_layers2', 'deconv_layers3', 'deconv_layers4', 'deconv_layers5', 'final_layer']\n",
    "\n",
    "''' ******************************************************************  计算M0 *************************************************************** '''\n",
    "BIT=24\n",
    "Mscale_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M0_float_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "M0_int_list = {}  #存储int推断时每个conv之后的*M scale\n",
    "ascale_load = np.load(parent_path+'ascale.npy', allow_pickle=True) \n",
    "# oscale_load = np.load('../output/weights_quan/oscale.npy', allow_pickle=True) \n",
    "wscale_load = np.load(parent_path+'wscale.npy', allow_pickle=True) \n",
    "M_list = np.load('../output/weights_quan/M_refactor.npy', allow_pickle=True) #量化感知训练得到的scale   有relu的量化反量化，因此需要转换\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    if(n==58): # 最后一层没有output_scale\n",
    "        Mscale=wscale_load.item()[key]*ascale_load.item()[key]\n",
    "    else:\n",
    "        key_post = Mkey_load[Mkey_load.index(key)+1] \n",
    "        Mscale=wscale_load.item()[key]*ascale_load.item()[key]/ascale_load.item()[key_post]  #不用relu的量化反量化，重新计算M值\n",
    "    Mscale_list[key] = Mscale #torch.squeeze(Mscale, dim=-1) \n",
    "    # print(n, key, M_list.item()[key].flatten(), M_list.item()[key].shape)\n",
    "    # print(ascale_load.item()[key], ascale_load.item()[key_post], oscale_load.item()[key])\n",
    "    # print(wscale_load.item()[key].flatten())\n",
    "    if(n!=58):\n",
    "        M0=(Mscale*2**BIT).type(torch.int32)\n",
    "        M0_float=M0*2**(-1*BIT)\n",
    "    else:  #此时还没有管最后一层的M0（最后一层应该不用*M0）\n",
    "        M0_float=Mscale\n",
    "        M0=torch.tensor(2**BIT) #乘以2**16,再除以2**16是原值\n",
    "    M0_float_list[key] = M0_float\n",
    "    M0_int_list[key] = M0\n",
    "    # print('M=',Mscale.flatten(),'\\nM0_float=',M0_float.flatten(),'\\nM0=',M0.flatten(),'\\nerror=',(Mscale-M0_float).flatten())\n",
    "    # print(n, key, Mscale.flatten(), Mscale_list[key].shape)\n",
    "\n",
    "# np.save('../output/weights_quan/mscale_norelu_quant.npy', Mscale_list)\n",
    "np.save(parent_path+'M0_quant_requant.npy', M0_float_list)\n",
    "np.save(parent_path+'M0_int.npy', M0_int_list)\n",
    "# # 读取M结果\n",
    "# M_load = np.load('../output/weights_quan/mscale_norelu_quant.npy', allow_pickle=True) #M_list\n",
    "# # M_load = np.load('../output/weights_quan/M0_int.npy', allow_pickle=True) #M_list\n",
    "# # print('M_list=',M_load.item())\n",
    "# print('conv1', M_load.item()['conv1'])#, M_load.item()[key])\n",
    "# for n,key in enumerate(M_key):\n",
    "#     if(n<=150):\n",
    "#         print(n, key, M_load.item()[key].shape)#, M_load.item()[key])\n",
    "#         M0=(M*2**16).type(torch.int32)\n",
    "#         print('M=',M.flatten(),'  M0=',M0.flatten(),'   error=',((M-M0*2**(-16))/M).flatten()) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv1.weight', 'conv1.bias', 'features.0.conv1.weight', 'features.0.conv1.bias', 'features.0.conv2.weight', 'features.0.conv2.bias', 'features.1.conv1.weight', 'features.1.conv1.bias', 'features.1.conv2.weight', 'features.1.conv2.bias', 'features.1.conv3.weight', 'features.1.conv3.bias', 'features.2.conv1.weight', 'features.2.conv1.bias', 'features.2.conv2.weight', 'features.2.conv2.bias', 'features.2.conv3.weight', 'features.2.conv3.bias', 'features.3.conv1.weight', 'features.3.conv1.bias', 'features.3.conv2.weight', 'features.3.conv2.bias', 'features.3.conv3.weight', 'features.3.conv3.bias', 'features.4.conv1.weight', 'features.4.conv1.bias', 'features.4.conv2.weight', 'features.4.conv2.bias', 'features.4.conv3.weight', 'features.4.conv3.bias', 'features.5.conv1.weight', 'features.5.conv1.bias', 'features.5.conv2.weight', 'features.5.conv2.bias', 'features.5.conv3.weight', 'features.5.conv3.bias', 'features.6.conv1.weight', 'features.6.conv1.bias', 'features.6.conv2.weight', 'features.6.conv2.bias', 'features.6.conv3.weight', 'features.6.conv3.bias', 'features.7.conv1.weight', 'features.7.conv1.bias', 'features.7.conv2.weight', 'features.7.conv2.bias', 'features.7.conv3.weight', 'features.7.conv3.bias', 'features.8.conv1.weight', 'features.8.conv1.bias', 'features.8.conv2.weight', 'features.8.conv2.bias', 'features.8.conv3.weight', 'features.8.conv3.bias', 'features.9.conv1.weight', 'features.9.conv1.bias', 'features.9.conv2.weight', 'features.9.conv2.bias', 'features.9.conv3.weight', 'features.9.conv3.bias', 'features.10.conv1.weight', 'features.10.conv1.bias', 'features.10.conv2.weight', 'features.10.conv2.bias', 'features.10.conv3.weight', 'features.10.conv3.bias', 'features.11.conv1.weight', 'features.11.conv1.bias', 'features.11.conv2.weight', 'features.11.conv2.bias', 'features.11.conv3.weight', 'features.11.conv3.bias', 'features.12.conv1.weight', 'features.12.conv1.bias', 'features.12.conv2.weight', 'features.12.conv2.bias', 'features.12.conv3.weight', 'features.12.conv3.bias', 'features.13.conv1.weight', 'features.13.conv1.bias', 'features.13.conv2.weight', 'features.13.conv2.bias', 'features.13.conv3.weight', 'features.13.conv3.bias', 'features.14.conv1.weight', 'features.14.conv1.bias', 'features.14.conv2.weight', 'features.14.conv2.bias', 'features.14.conv3.weight', 'features.14.conv3.bias', 'features.15.conv1.weight', 'features.15.conv1.bias', 'features.15.conv2.weight', 'features.15.conv2.bias', 'features.15.conv3.weight', 'features.15.conv3.bias', 'features.16.conv1.weight', 'features.16.conv1.bias', 'features.16.conv2.weight', 'features.16.conv2.bias', 'features.16.conv3.weight', 'features.16.conv3.bias', 'conv2.weight', 'conv2.bias', 'deconv_layers0.weight', 'deconv_layers0.bias', 'deconv_layers1.weight', 'deconv_layers1.bias', 'deconv_layers2.weight', 'deconv_layers2.bias', 'deconv_layers3.weight', 'deconv_layers3.bias', 'deconv_layers4.weight', 'deconv_layers4.bias', 'deconv_layers5.weight', 'deconv_layers5.bias', 'final_layer.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入量化后的int权重  但由于模型中的weight/bias参数仍是float32类型的，因此保存时还是会保存成float32.\n",
    "print(remapped_state.keys())\n",
    "bnfuse_model.load_state_dict(remapped_state)\n",
    "#修改权重数据的类型为int32\n",
    "# for n,param_key in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_key,'\\t',bnfuse_model.state_dict()[param_key].size())\n",
    "#     eval('bnfuse_model.'+param_key+'.data.type(torch.int32)')\n",
    "\n",
    "# for name, module in bnfuse_model.named_modules():\n",
    "#     if type(module) in [torch.nn.Conv2d, torch.nn.ConvTranspose2d]:\n",
    "#         print(name, module)\n",
    "#         if(name=='final_layer'): #没有bias\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#         else: #conv2d, ConvTranspose2d\n",
    "#             eval('bnfuse_model.'+name+'.weight.data.type(torch.int32)')\n",
    "#             eval('bnfuse_model.'+name+'.bias.data.type(torch.int32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {'model': bnfuse_model.module.state_dict() if hasattr(bnfuse_model, 'module') else bnfuse_model.state_dict()}\n",
    "# parent_path='../output/weights_quan_deconv3/'\n",
    "# torch.save(ckpt, parent_path+'float_mobilenetpose_nobn_refactor_deconv3.pt')\n",
    "# torch.save(ckpt, '../output/weights_quan/post_int_mobilenetpose_nobn_refactor.pt')\n",
    "# torch.save(ckpt, '../output/weights_quan/int_mobilenetpose_nobn_refactor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'features.0.conv1.weight', 'features.0.conv1.bias', 'features.0.conv2.weight', 'features.0.conv2.bias', 'features.1.conv1.weight', 'features.1.conv1.bias', 'features.1.conv2.weight', 'features.1.conv2.bias', 'features.1.conv3.weight', 'features.1.conv3.bias', 'features.2.conv1.weight', 'features.2.conv1.bias', 'features.2.conv2.weight', 'features.2.conv2.bias', 'features.2.conv3.weight', 'features.2.conv3.bias', 'features.3.conv1.weight', 'features.3.conv1.bias', 'features.3.conv2.weight', 'features.3.conv2.bias', 'features.3.conv3.weight', 'features.3.conv3.bias', 'features.4.conv1.weight', 'features.4.conv1.bias', 'features.4.conv2.weight', 'features.4.conv2.bias', 'features.4.conv3.weight', 'features.4.conv3.bias', 'features.5.conv1.weight', 'features.5.conv1.bias', 'features.5.conv2.weight', 'features.5.conv2.bias', 'features.5.conv3.weight', 'features.5.conv3.bias', 'features.6.conv1.weight', 'features.6.conv1.bias', 'features.6.conv2.weight', 'features.6.conv2.bias', 'features.6.conv3.weight', 'features.6.conv3.bias', 'features.7.conv1.weight', 'features.7.conv1.bias', 'features.7.conv2.weight', 'features.7.conv2.bias', 'features.7.conv3.weight', 'features.7.conv3.bias', 'features.8.conv1.weight', 'features.8.conv1.bias', 'features.8.conv2.weight', 'features.8.conv2.bias', 'features.8.conv3.weight', 'features.8.conv3.bias', 'features.9.conv1.weight', 'features.9.conv1.bias', 'features.9.conv2.weight', 'features.9.conv2.bias', 'features.9.conv3.weight', 'features.9.conv3.bias', 'features.10.conv1.weight', 'features.10.conv1.bias', 'features.10.conv2.weight', 'features.10.conv2.bias', 'features.10.conv3.weight', 'features.10.conv3.bias', 'features.11.conv1.weight', 'features.11.conv1.bias', 'features.11.conv2.weight', 'features.11.conv2.bias', 'features.11.conv3.weight', 'features.11.conv3.bias', 'features.12.conv1.weight', 'features.12.conv1.bias', 'features.12.conv2.weight', 'features.12.conv2.bias', 'features.12.conv3.weight', 'features.12.conv3.bias', 'features.13.conv1.weight', 'features.13.conv1.bias', 'features.13.conv2.weight', 'features.13.conv2.bias', 'features.13.conv3.weight', 'features.13.conv3.bias', 'features.14.conv1.weight', 'features.14.conv1.bias', 'features.14.conv2.weight', 'features.14.conv2.bias', 'features.14.conv3.weight', 'features.14.conv3.bias', 'features.15.conv1.weight', 'features.15.conv1.bias', 'features.15.conv2.weight', 'features.15.conv2.bias', 'features.15.conv3.weight', 'features.15.conv3.bias', 'features.16.conv1.weight', 'features.16.conv1.bias', 'features.16.conv2.weight', 'features.16.conv2.bias', 'features.16.conv3.weight', 'features.16.conv3.bias', 'conv2.weight', 'conv2.bias', 'deconv_layers0.weight', 'deconv_layers0.bias', 'deconv_layers1.weight', 'deconv_layers1.bias', 'deconv_layers2.weight', 'deconv_layers2.bias', 'deconv_layers3.weight', 'deconv_layers3.bias', 'deconv_layers4.weight', 'deconv_layers4.bias', 'deconv_layers5.weight', 'deconv_layers5.bias', 'final_layer.weight'])\n"
     ]
    }
   ],
   "source": [
    "####################################### bnfuse float model ############################################ \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "parent_path='../output/weights_quan_deconv3/'\n",
    "float_nobn_dict=torch.load(parent_path+'float_mobilenetpose_nobn_refactor_deconv3.pt')['model'] #可以直接导入权重，可以不需要加载模型\n",
    "print(float_nobn_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "conv1\n",
      "2\n",
      "features.0.conv1\n",
      "2 features.2.conv1 : tensor([0.0657]) features.3.conv1 : tensor([0.0973]) tensor([0.6754])\n",
      "4 features.4.conv1 : tensor([0.0530]) features.6.conv1 : tensor([0.0955]) tensor([0.5550])\n",
      "5 features.5.conv1 : tensor([0.0700]) features.6.conv1 : tensor([0.0955]) tensor([0.7329])\n",
      "7 features.7.conv1 : tensor([0.0640]) features.10.conv1 : tensor([0.0825]) tensor([0.7761])\n",
      "8 features.8.conv1 : tensor([0.0651]) features.10.conv1 : tensor([0.0825]) tensor([0.7887])\n",
      "9 features.9.conv1 : tensor([0.0706]) features.10.conv1 : tensor([0.0825]) tensor([0.8559])\n",
      "11 features.11.conv1 : tensor([0.0478]) features.13.conv1 : tensor([0.1110]) tensor([0.4304])\n",
      "12 features.12.conv1 : tensor([0.0594]) features.13.conv1 : tensor([0.1110]) tensor([0.5354])\n",
      "14 features.14.conv1 : tensor([0.0525]) features.16.conv1 : tensor([0.0831]) tensor([0.6319])\n",
      "15 features.15.conv1 : tensor([0.0588]) features.16.conv1 : tensor([0.0831]) tensor([0.7069])\n",
      "0 conv1 tensor([0.0207]) tensor([0.0207])\n",
      "1 features.0.conv1 tensor([0.3417]) tensor([0.3417])\n",
      "2 features.0.conv2 tensor([0.2705]) tensor([0.2705])\n",
      "3 features.1.conv1 tensor([0.1153]) tensor([0.1153])\n",
      "4 features.1.conv2 tensor([0.2389]) tensor([0.2389])\n",
      "5 features.1.conv3 tensor([0.1275]) tensor([0.1275])\n",
      "6 features.2.conv1 tensor([0.0973]) tensor([0.0657])\n",
      "7 features.2.conv2 tensor([0.0992]) tensor([0.0992])\n",
      "8 features.2.conv3 tensor([0.0855]) tensor([0.0855])\n",
      "9 features.3.conv1 tensor([0.0973]) tensor([0.0973])\n",
      "10 features.3.conv2 tensor([0.1167]) tensor([0.1167])\n",
      "11 features.3.conv3 tensor([0.0536]) tensor([0.0536])\n",
      "12 features.4.conv1 tensor([0.0955]) tensor([0.0530])\n",
      "13 features.4.conv2 tensor([0.0480]) tensor([0.0480])\n",
      "14 features.4.conv3 tensor([0.0462]) tensor([0.0462])\n",
      "15 features.5.conv1 tensor([0.0955]) tensor([0.0700])\n",
      "16 features.5.conv2 tensor([0.0515]) tensor([0.0515])\n",
      "17 features.5.conv3 tensor([0.0540]) tensor([0.0540])\n",
      "18 features.6.conv1 tensor([0.0955]) tensor([0.0955])\n",
      "19 features.6.conv2 tensor([0.0830]) tensor([0.0830])\n",
      "20 features.6.conv3 tensor([0.0975]) tensor([0.0975])\n",
      "21 features.7.conv1 tensor([0.0825]) tensor([0.0640])\n",
      "22 features.7.conv2 tensor([0.0481]) tensor([0.0481])\n",
      "23 features.7.conv3 tensor([0.0467]) tensor([0.0467])\n",
      "24 features.8.conv1 tensor([0.0825]) tensor([0.0651])\n",
      "25 features.8.conv2 tensor([0.0536]) tensor([0.0536])\n",
      "26 features.8.conv3 tensor([0.0538]) tensor([0.0538])\n",
      "27 features.9.conv1 tensor([0.0825]) tensor([0.0706])\n",
      "28 features.9.conv2 tensor([0.0466]) tensor([0.0466])\n",
      "29 features.9.conv3 tensor([0.0444]) tensor([0.0444])\n",
      "30 features.10.conv1 tensor([0.0825]) tensor([0.0825])\n",
      "31 features.10.conv2 tensor([0.0756]) tensor([0.0756])\n",
      "32 features.10.conv3 tensor([0.0702]) tensor([0.0702])\n",
      "33 features.11.conv1 tensor([0.1110]) tensor([0.0478])\n",
      "34 features.11.conv2 tensor([0.0731]) tensor([0.0731])\n",
      "35 features.11.conv3 tensor([0.0773]) tensor([0.0773])\n",
      "36 features.12.conv1 tensor([0.1110]) tensor([0.0594])\n",
      "37 features.12.conv2 tensor([0.0679]) tensor([0.0679])\n",
      "38 features.12.conv3 tensor([0.1439]) tensor([0.1439])\n",
      "39 features.13.conv1 tensor([0.1110]) tensor([0.1110])\n",
      "40 features.13.conv2 tensor([0.0623]) tensor([0.0623])\n",
      "41 features.13.conv3 tensor([0.1197]) tensor([0.1197])\n",
      "42 features.14.conv1 tensor([0.0831]) tensor([0.0525])\n",
      "43 features.14.conv2 tensor([0.1068]) tensor([0.1068])\n",
      "44 features.14.conv3 tensor([0.0580]) tensor([0.0580])\n",
      "45 features.15.conv1 tensor([0.0831]) tensor([0.0588])\n",
      "46 features.15.conv2 tensor([0.0735]) tensor([0.0735])\n",
      "47 features.15.conv3 tensor([0.0943]) tensor([0.0943])\n",
      "48 features.16.conv1 tensor([0.0831]) tensor([0.0831])\n",
      "49 features.16.conv2 tensor([0.0942]) tensor([0.0942])\n",
      "50 features.16.conv3 tensor([0.1881]) tensor([0.1881])\n",
      "51 conv2 tensor([0.0567]) tensor([0.0567])\n",
      "52 deconv_layers0 tensor([0.2661]) tensor([0.2661])\n",
      "53 deconv_layers1 tensor([0.6707]) tensor([0.6707])\n",
      "54 deconv_layers2 tensor([0.4037]) tensor([0.4037])\n",
      "55 deconv_layers3 tensor([0.6812]) tensor([0.6812])\n",
      "56 deconv_layers4 tensor([0.4040]) tensor([0.4040])\n",
      "57 deconv_layers5 tensor([1.3697]) tensor([1.3697])\n",
      "58 final_layer tensor([0.0126]) tensor([0.0126])\n",
      "M0_float: tensor([3.9311e-05, 5.1540e-05, 4.8065e-05, 5.7022e-05, 5.8868e-05, 1.3317e-04,\n",
      "        4.3028e-05, 4.5899e-05, 4.3756e-05, 4.0470e-05, 4.7417e-05, 4.5811e-05,\n",
      "        3.7967e-05, 5.0862e-05, 4.8882e-05, 9.4663e-05, 7.4792e-05])\n"
     ]
    }
   ],
   "source": [
    "''' ******************************************************************  shortcut处的另一种计算方式 *************************************************************** '''\n",
    "'''shortcut处使用同一scale，则x处的表示范围会变小，由此带来误差；   直接用输出的的scale计算？ \n",
    "    因为ascale改变了，所以除了M需要改变，bias也需要改变\n",
    "但如果使用不同scale, 则需要硬件上进行*M0并移位的操作，操作更多，而且是整型计算，同样会带来误差'''\n",
    "Mkey_load = np.load('../output/weights_quan/M_key.npy', allow_pickle=True)  #type:<class 'numpy.ndarray'>\n",
    "Mkey_load=list(Mkey_load)\n",
    "print(type(Mkey_load))\n",
    "print(Mkey_load[0])\n",
    "print(Mkey_load.index('features.0.conv2'))\n",
    "print(Mkey_load[Mkey_load.index('features.0.conv2')-1])\n",
    "\n",
    "shortcut_list=['features.2.conv1','features.4.conv1','features.5.conv1','features.7.conv1','features.8.conv1','features.9.conv1','features.11.conv1','features.12.conv1','features.14.conv1','features.15.conv1']\n",
    "shortcut_layer=[2,4,5,7,8,9,11,12,14,15]\n",
    "# shortcut_index=[6, 12, 15, 21, 24, 27, 33, 36, 42, 45]\n",
    "# shortcut_corr_index=[9, 18, 18, 30, 30, 30, 39, 39, 48, 48]\n",
    "shortcut_dict={6:9, 12:18, 15:18,21:30, 24:30, 27:30, 33:39, 36:39, 42:48, 45:48}\n",
    "\n",
    "# print(shortcut_index)\n",
    "int_weight_nobn_dict = {}\n",
    "ascale_shortcut_same_list = {}  \n",
    "Mscale_shortcut_same_list = {}  \n",
    "M0_float_list = {}\n",
    "M0_int_list = {}\n",
    "ascale_load = np.load(parent_path+'ascale.npy', allow_pickle=True) \n",
    "# oscale_load = np.load(parent_path+'oscale.npy', allow_pickle=True) \n",
    "wscale_load = np.load(parent_path+'wscale.npy', allow_pickle=True) \n",
    "#重设ascale  shortcut处使用同一scale\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    ascale=ascale_load.item()[key]\n",
    "    if(n in shortcut_dict.keys()):\n",
    "        index=shortcut_dict[n]\n",
    "        ascale_pre=ascale\n",
    "        ascale=ascale_load.item()[Mkey_load[index]]\n",
    "        print(n//3, key, ':', ascale_pre, Mkey_load[index], ':', ascale, ascale_pre/ascale)\n",
    "    ascale_shortcut_same_list[key] = ascale \n",
    "np.save(parent_path+'ascale_shortcut0.npy', ascale_shortcut_same_list)\n",
    "\n",
    "\n",
    "# 读取ascale结果\n",
    "ascale_shortcut_load = np.load(parent_path+'ascale_shortcut0.npy', allow_pickle=True) #M_list\n",
    "# print('conv1', ascale_shortcut_load.item()['conv1'])#, M_load.item()[key])\n",
    "for n,key in enumerate(Mkey_load):\n",
    "    ascale0=ascale_load.item()[key]\n",
    "    ascale=ascale_shortcut_load.item()[key]\n",
    "    wscale=wscale_load.item()[key]\n",
    "    # print(n, key, ascale_shortcut_load.item()[key], ascale_load.item()[key])\n",
    "    print(n, key, ascale, ascale0)\n",
    "    \n",
    "    #计算权重和偏置int量化结果\n",
    "    conv_weight=float_nobn_dict[key+'.weight']\n",
    "    if(n!=58):#不是final_layer\n",
    "        #计算q_weight, q_bias\n",
    "        conv_bias=float_nobn_dict[key+'.bias']\n",
    "        # print(conv_weight.shape, conv_bias.shape, wscale.shape)\n",
    "        q_weight,q_bias = quantize_tensor(conv_weight, conv_bias, wscale.unsqueeze(dim=-1), ascale, num_bits=8) \n",
    "        int_weight_nobn_dict[key+'.weight'] = q_weight\n",
    "        int_weight_nobn_dict[key+'.bias'] = q_bias\n",
    "        #计算Mscale\n",
    "        key_post = Mkey_load[Mkey_load.index(key)+1] \n",
    "        Mscale=wscale*ascale/ascale_shortcut_load.item()[key_post]  #不用relu的量化反量化，重新计算M值\n",
    "        M0=(Mscale*2**16).type(torch.int32)\n",
    "        M0_float=M0*2**(-16)\n",
    "    else: #final_layer\n",
    "        #计算q_weight, q_bias\n",
    "        q_weight,q_bias = quantize_tensor(conv_weight, torch.zeros([17]), wscale.unsqueeze(dim=-1), ascale, num_bits=8) #这儿的bias是上一层的bias,是没有用的\n",
    "        int_weight_nobn_dict[key+'.weight'] = q_weight\n",
    "        #计算Mscale\n",
    "        Mscale=wscale*ascale\n",
    "        M0_float=Mscale\n",
    "        print('M0_float:',M0_float.flatten())\n",
    "        '''M0_float: tensor([3.9311e-05, 5.1540e-05, 4.8065e-05, 5.7022e-05, 5.8868e-05, 1.3317e-04,\n",
    "        4.3028e-05, 4.5899e-05, 4.3756e-05, 4.0470e-05, 4.7417e-05, 4.5811e-05,\n",
    "        3.7967e-05, 5.0862e-05, 4.8882e-05, 9.4663e-05, 7.4792e-05])'''\n",
    "        M0=torch.tensor(2**16) #乘以2**16,再除以2**16是原值\n",
    "    \n",
    "    # print(n, q_weight, q_bias)\n",
    "    Mscale_shortcut_same_list[key] = Mscale #torch.squeeze(Mscale, dim=-1)\n",
    "    M0_float_list[key] = M0_float\n",
    "    M0_int_list[key] = M0 \n",
    "\n",
    "torch.save(int_weight_nobn_dict, parent_path+'int_mobilenetpose_shortcut0.pt')\n",
    "np.save(parent_path+'Mscale_shortcut0.npy', Mscale_shortcut_same_list)\n",
    "np.save(parent_path+'M0_float_shortcut0.npy', M0_float_list)\n",
    "np.save(parent_path+'M0_int_shortcut0.npy', M0_int_list)\n",
    "# 读取ascale结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aab342d38c9089b27086ef2ff31e6c6863533cd1c8642810b45a542962dd3699"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}