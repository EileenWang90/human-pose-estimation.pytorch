PoseMobileNet(
  (conv1): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (features): Sequential(
    (0): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(12, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
        (4): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Sequential(
    (0): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (deconv_layers): Sequential(
    (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): ReLU(inplace=True)
    (15): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (16): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
  )
  (final_layer): Conv2d(128, 17, kernel_size=(1, 1), stride=(1, 1))
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 16, 128, 96]             432
       BatchNorm2d-2          [-1, 16, 128, 96]              32
             ReLU6-3          [-1, 16, 128, 96]               0
            Conv2d-4          [-1, 16, 128, 96]             144
       BatchNorm2d-5          [-1, 16, 128, 96]              32
             ReLU6-6          [-1, 16, 128, 96]               0
            Conv2d-7           [-1, 8, 128, 96]             128
       BatchNorm2d-8           [-1, 8, 128, 96]              16
  InvertedResidual-9           [-1, 8, 128, 96]               0
           Conv2d-10          [-1, 48, 128, 96]             384
      BatchNorm2d-11          [-1, 48, 128, 96]              96
            ReLU6-12          [-1, 48, 128, 96]               0
           Conv2d-13           [-1, 48, 64, 48]             432
      BatchNorm2d-14           [-1, 48, 64, 48]              96
            ReLU6-15           [-1, 48, 64, 48]               0
           Conv2d-16           [-1, 12, 64, 48]             576
      BatchNorm2d-17           [-1, 12, 64, 48]              24
 InvertedResidual-18           [-1, 12, 64, 48]               0
           Conv2d-19           [-1, 72, 64, 48]             864
      BatchNorm2d-20           [-1, 72, 64, 48]             144
            ReLU6-21           [-1, 72, 64, 48]               0
           Conv2d-22           [-1, 72, 64, 48]             648
      BatchNorm2d-23           [-1, 72, 64, 48]             144
            ReLU6-24           [-1, 72, 64, 48]               0
           Conv2d-25           [-1, 12, 64, 48]             864
      BatchNorm2d-26           [-1, 12, 64, 48]              24
 InvertedResidual-27           [-1, 12, 64, 48]               0
           Conv2d-28           [-1, 72, 64, 48]             864
      BatchNorm2d-29           [-1, 72, 64, 48]             144
            ReLU6-30           [-1, 72, 64, 48]               0
           Conv2d-31           [-1, 72, 32, 24]             648
      BatchNorm2d-32           [-1, 72, 32, 24]             144
            ReLU6-33           [-1, 72, 32, 24]               0
           Conv2d-34           [-1, 16, 32, 24]           1,152
      BatchNorm2d-35           [-1, 16, 32, 24]              32
 InvertedResidual-36           [-1, 16, 32, 24]               0
           Conv2d-37           [-1, 96, 32, 24]           1,536
      BatchNorm2d-38           [-1, 96, 32, 24]             192
            ReLU6-39           [-1, 96, 32, 24]               0
           Conv2d-40           [-1, 96, 32, 24]             864
      BatchNorm2d-41           [-1, 96, 32, 24]             192
            ReLU6-42           [-1, 96, 32, 24]               0
           Conv2d-43           [-1, 16, 32, 24]           1,536
      BatchNorm2d-44           [-1, 16, 32, 24]              32
 InvertedResidual-45           [-1, 16, 32, 24]               0
           Conv2d-46           [-1, 96, 32, 24]           1,536
      BatchNorm2d-47           [-1, 96, 32, 24]             192
            ReLU6-48           [-1, 96, 32, 24]               0
           Conv2d-49           [-1, 96, 32, 24]             864
      BatchNorm2d-50           [-1, 96, 32, 24]             192
            ReLU6-51           [-1, 96, 32, 24]               0
           Conv2d-52           [-1, 16, 32, 24]           1,536
      BatchNorm2d-53           [-1, 16, 32, 24]              32
 InvertedResidual-54           [-1, 16, 32, 24]               0
           Conv2d-55           [-1, 96, 32, 24]           1,536
      BatchNorm2d-56           [-1, 96, 32, 24]             192
            ReLU6-57           [-1, 96, 32, 24]               0
           Conv2d-58           [-1, 96, 16, 12]             864
      BatchNorm2d-59           [-1, 96, 16, 12]             192
            ReLU6-60           [-1, 96, 16, 12]               0
           Conv2d-61           [-1, 32, 16, 12]           3,072
      BatchNorm2d-62           [-1, 32, 16, 12]              64
 InvertedResidual-63           [-1, 32, 16, 12]               0
           Conv2d-64          [-1, 192, 16, 12]           6,144
      BatchNorm2d-65          [-1, 192, 16, 12]             384
            ReLU6-66          [-1, 192, 16, 12]               0
           Conv2d-67          [-1, 192, 16, 12]           1,728
      BatchNorm2d-68          [-1, 192, 16, 12]             384
            ReLU6-69          [-1, 192, 16, 12]               0
           Conv2d-70           [-1, 32, 16, 12]           6,144
      BatchNorm2d-71           [-1, 32, 16, 12]              64
 InvertedResidual-72           [-1, 32, 16, 12]               0
           Conv2d-73          [-1, 192, 16, 12]           6,144
      BatchNorm2d-74          [-1, 192, 16, 12]             384
            ReLU6-75          [-1, 192, 16, 12]               0
           Conv2d-76          [-1, 192, 16, 12]           1,728
      BatchNorm2d-77          [-1, 192, 16, 12]             384
            ReLU6-78          [-1, 192, 16, 12]               0
           Conv2d-79           [-1, 32, 16, 12]           6,144
      BatchNorm2d-80           [-1, 32, 16, 12]              64
 InvertedResidual-81           [-1, 32, 16, 12]               0
           Conv2d-82          [-1, 192, 16, 12]           6,144
      BatchNorm2d-83          [-1, 192, 16, 12]             384
            ReLU6-84          [-1, 192, 16, 12]               0
           Conv2d-85          [-1, 192, 16, 12]           1,728
      BatchNorm2d-86          [-1, 192, 16, 12]             384
            ReLU6-87          [-1, 192, 16, 12]               0
           Conv2d-88           [-1, 32, 16, 12]           6,144
      BatchNorm2d-89           [-1, 32, 16, 12]              64
 InvertedResidual-90           [-1, 32, 16, 12]               0
           Conv2d-91          [-1, 192, 16, 12]           6,144
      BatchNorm2d-92          [-1, 192, 16, 12]             384
            ReLU6-93          [-1, 192, 16, 12]               0
           Conv2d-94          [-1, 192, 16, 12]           1,728
      BatchNorm2d-95          [-1, 192, 16, 12]             384
            ReLU6-96          [-1, 192, 16, 12]               0
           Conv2d-97           [-1, 48, 16, 12]           9,216
      BatchNorm2d-98           [-1, 48, 16, 12]              96
 InvertedResidual-99           [-1, 48, 16, 12]               0
          Conv2d-100          [-1, 288, 16, 12]          13,824
     BatchNorm2d-101          [-1, 288, 16, 12]             576
           ReLU6-102          [-1, 288, 16, 12]               0
          Conv2d-103          [-1, 288, 16, 12]           2,592
     BatchNorm2d-104          [-1, 288, 16, 12]             576
           ReLU6-105          [-1, 288, 16, 12]               0
          Conv2d-106           [-1, 48, 16, 12]          13,824
     BatchNorm2d-107           [-1, 48, 16, 12]              96
InvertedResidual-108           [-1, 48, 16, 12]               0
          Conv2d-109          [-1, 288, 16, 12]          13,824
     BatchNorm2d-110          [-1, 288, 16, 12]             576
           ReLU6-111          [-1, 288, 16, 12]               0
          Conv2d-112          [-1, 288, 16, 12]           2,592
     BatchNorm2d-113          [-1, 288, 16, 12]             576
           ReLU6-114          [-1, 288, 16, 12]               0
          Conv2d-115           [-1, 48, 16, 12]          13,824
     BatchNorm2d-116           [-1, 48, 16, 12]              96
InvertedResidual-117           [-1, 48, 16, 12]               0
          Conv2d-118          [-1, 288, 16, 12]          13,824
     BatchNorm2d-119          [-1, 288, 16, 12]             576
           ReLU6-120          [-1, 288, 16, 12]               0
          Conv2d-121            [-1, 288, 8, 6]           2,592
     BatchNorm2d-122            [-1, 288, 8, 6]             576
           ReLU6-123            [-1, 288, 8, 6]               0
          Conv2d-124             [-1, 80, 8, 6]          23,040
     BatchNorm2d-125             [-1, 80, 8, 6]             160
InvertedResidual-126             [-1, 80, 8, 6]               0
          Conv2d-127            [-1, 480, 8, 6]          38,400
     BatchNorm2d-128            [-1, 480, 8, 6]             960
           ReLU6-129            [-1, 480, 8, 6]               0
          Conv2d-130            [-1, 480, 8, 6]           4,320
     BatchNorm2d-131            [-1, 480, 8, 6]             960
           ReLU6-132            [-1, 480, 8, 6]               0
          Conv2d-133             [-1, 80, 8, 6]          38,400
     BatchNorm2d-134             [-1, 80, 8, 6]             160
InvertedResidual-135             [-1, 80, 8, 6]               0
          Conv2d-136            [-1, 480, 8, 6]          38,400
     BatchNorm2d-137            [-1, 480, 8, 6]             960
           ReLU6-138            [-1, 480, 8, 6]               0
          Conv2d-139            [-1, 480, 8, 6]           4,320
     BatchNorm2d-140            [-1, 480, 8, 6]             960
           ReLU6-141            [-1, 480, 8, 6]               0
          Conv2d-142             [-1, 80, 8, 6]          38,400
     BatchNorm2d-143             [-1, 80, 8, 6]             160
InvertedResidual-144             [-1, 80, 8, 6]               0
          Conv2d-145            [-1, 480, 8, 6]          38,400
     BatchNorm2d-146            [-1, 480, 8, 6]             960
           ReLU6-147            [-1, 480, 8, 6]               0
          Conv2d-148            [-1, 480, 8, 6]           4,320
     BatchNorm2d-149            [-1, 480, 8, 6]             960
           ReLU6-150            [-1, 480, 8, 6]               0
          Conv2d-151            [-1, 160, 8, 6]          76,800
     BatchNorm2d-152            [-1, 160, 8, 6]             320
InvertedResidual-153            [-1, 160, 8, 6]               0
          Conv2d-154            [-1, 128, 8, 6]          20,480
     BatchNorm2d-155            [-1, 128, 8, 6]             256
            ReLU-156            [-1, 128, 8, 6]               0
 ConvTranspose2d-157          [-1, 128, 16, 12]           2,048
     BatchNorm2d-158          [-1, 128, 16, 12]             256
            ReLU-159          [-1, 128, 16, 12]               0
          Conv2d-160          [-1, 128, 16, 12]          16,384
     BatchNorm2d-161          [-1, 128, 16, 12]             256
            ReLU-162          [-1, 128, 16, 12]               0
 ConvTranspose2d-163          [-1, 128, 32, 24]           2,048
     BatchNorm2d-164          [-1, 128, 32, 24]             256
            ReLU-165          [-1, 128, 32, 24]               0
          Conv2d-166          [-1, 128, 32, 24]          16,384
     BatchNorm2d-167          [-1, 128, 32, 24]             256
            ReLU-168          [-1, 128, 32, 24]               0
 ConvTranspose2d-169          [-1, 128, 64, 48]           2,048
     BatchNorm2d-170          [-1, 128, 64, 48]             256
            ReLU-171          [-1, 128, 64, 48]               0
          Conv2d-172          [-1, 128, 64, 48]          16,384
     BatchNorm2d-173          [-1, 128, 64, 48]             256
            ReLU-174          [-1, 128, 64, 48]               0
          Conv2d-175           [-1, 17, 64, 48]           2,193
================================================================
Total params: 556,849
Trainable params: 556,849
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.56
Forward/backward pass size (MB): 98.33
Params size (MB): 2.12
Estimated Total Size (MB): 101.01
----------------------------------------------------------------