{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import _init_paths\n",
    "from core.config import config\n",
    "from core.config import update_config\n",
    "from core.config import update_dir\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import validate\n",
    "from utils.utils import create_logger\n",
    "\n",
    "import dataset\n",
    "import models\n",
    "\n",
    "import quantize_dorefa\n",
    "from quantize_iao import *\n",
    "# from quantize_iao_uint import *  #对feature map进行uint对称量化\n",
    "\n",
    "import numpy as np\n",
    "# 保证所有数据能够显示，而不是用省略号表示，np.inf表示一个足够大的数\n",
    "np.set_printoptions(threshold = np.inf) \n",
    "\n",
    "# # 若想不以科学计数显示:\n",
    "# np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_conv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConv2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv\n",
    "\n",
    "def bn_fuse_deconv(bn_conv,device):\n",
    "    # ******************** bn参数 *********************\n",
    "    mean = bn_conv.running_mean\n",
    "    std = torch.sqrt(bn_conv.running_var + bn_conv.eps)\n",
    "    gamma = bn_conv.gamma\n",
    "    beta = bn_conv.beta\n",
    "    # ******************* conv参数 ********************\n",
    "    w = bn_conv.weight\n",
    "    w_fused = w.clone()\n",
    "    if bn_conv.bias is not None:\n",
    "        b = bn_conv.bias\n",
    "    else:\n",
    "        b = mean.new_zeros(mean.shape)\n",
    "    b_fused = b.clone()\n",
    "    # ******************* bn融合 *******************\n",
    "    w_fused = w * (gamma / std).reshape([bn_conv.out_channels, 1, 1, 1])\n",
    "    b_fused = beta + (b - mean) * (gamma / std)\n",
    "    bn_fused_conv = QuantConvTranspose2d(bn_conv.in_channels,\n",
    "                                         bn_conv.out_channels,\n",
    "                                         bn_conv.kernel_size,\n",
    "                                         stride=bn_conv.stride,\n",
    "                                         padding=bn_conv.padding,\n",
    "                                         output_padding=bn_conv.output_padding,\n",
    "                                         dilation=bn_conv.dilation,\n",
    "                                         groups=bn_conv.groups,\n",
    "                                         bias=True,\n",
    "                                         padding_mode=bn_conv.padding_mode,\n",
    "                                         a_bits=config.QUANTIZATION.A_BITS,\n",
    "                                         w_bits=config.QUANTIZATION.W_BITS,\n",
    "                                         q_type=config.QUANTIZATION.Q_TYPE,\n",
    "                                         q_level=config.QUANTIZATION.Q_LEVEL,\n",
    "                                         device=device,\n",
    "                                         quant_inference=True)\n",
    "    bn_fused_conv.weight.data = w_fused\n",
    "    bn_fused_conv.bias.data = b_fused\n",
    "    bn_fused_conv.activation_quantizer.scale.copy_(bn_conv.activation_quantizer.scale)\n",
    "    bn_fused_conv.activation_quantizer.zero_point.copy_(bn_conv.activation_quantizer.zero_point)\n",
    "    bn_fused_conv.activation_quantizer.eps = bn_conv.activation_quantizer.eps\n",
    "    bn_fused_conv.weight_quantizer.scale.copy_(bn_conv.weight_quantizer.scale)\n",
    "    bn_fused_conv.weight_quantizer.zero_point.copy_(bn_conv.weight_quantizer.zero_point)\n",
    "    bn_fused_conv.weight_quantizer.eps = bn_conv.weight_quantizer.eps\n",
    "    return bn_fused_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_fuse_module(module, device):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, QuantBNFuseConv2d):\n",
    "            bn_fused_conv = bn_fuse_conv(child, device)\n",
    "            module._modules[name] = bn_fused_conv\n",
    "        elif isinstance(child, QuantBNFuseConvTranspose2d):\n",
    "            bn_fused_deconv = bn_fuse_deconv(child, device)\n",
    "            module._modules[name] = bn_fused_deconv\n",
    "        else:\n",
    "            bn_fuse_module(child, device)\n",
    "\n",
    "\n",
    "def model_bn_fuse(model, inplace=False):\n",
    "    if not inplace:\n",
    "        model = copy.deepcopy(model)\n",
    "    device = next(model.parameters()).device\n",
    "    bn_fuse_module(model,device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device='', apex=False, batch_size=None):\n",
    "    # device = 'cpu' or '0' or '0,1,2,3'\n",
    "    cpu_request = device.lower() == 'cpu'\n",
    "    if device and not cpu_request:  # if device requested other than 'cpu'\n",
    "        # os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
    "        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity\n",
    "\n",
    "    cuda = False if cpu_request else torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        c = 1024 ** 2  # bytes to MB\n",
    "        ng = torch.cuda.device_count()\n",
    "        if ng > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
    "            assert batch_size % ng == 0, 'batch-size %g not multiple of GPU count %g' % (batch_size, ng)\n",
    "        x = [torch.cuda.get_device_properties(i) for i in range(ng)]\n",
    "        s = 'Using CUDA ' + ('Apex ' if apex else '')  # apex for mixed precision https://github.com/NVIDIA/apex\n",
    "        for i in range(0, ng):\n",
    "            if i == 1:\n",
    "                s = ' ' * len(s)\n",
    "            print(\"%sdevice%g _CudaDeviceProperties(name='%s', total_memory=%dMB)\" %\n",
    "                  (s, i, x[i].name, x[i].total_memory / c))\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    print('')  # skip a line\n",
    "    return torch.device('cuda:0' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models.pose_mobilenet_relu.get_pose_net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytwang/wyt_workspace/quantization/human-pose-estimation.pytorch/pose_estimation/../lib/core/config.py:196: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  exp_config = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "cfg='../experiments/coco/resnet50/mobile_quant_relu_int.yaml' #MODEL_FILE: 'output/weights_quan/float_mobilenetpose_nobn.pt'\n",
    "update_config(cfg)\n",
    "# cudnn related setting\n",
    "cudnn.benchmark = config.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n",
    "\n",
    "# for shufflenetv2\n",
    "shufflenetv2_spec = {'0.5': ([4, 8, 4], [24, 48, 96, 192, 1024]),\n",
    "                        '1.0': ([4, 8, 4], [24, 116, 232, 464, 1024]),\n",
    "                        '1.5': ([4, 8, 4], [24, 176, 352, 704, 1024]),\n",
    "                        '2.0': ([4, 8, 4], [24, 244, 488, 976, 2048])}\n",
    "stages_repeats, stages_out_channels = shufflenetv2_spec['1.0']\n",
    "print('models.'+config.MODEL.NAME+'.get_pose_net')\n",
    "model = eval('models.'+config.MODEL.NAME+'.get_pose_net')(\n",
    "        config, \n",
    "        stages_repeats, stages_out_channels,\n",
    "        is_train=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################################### bnfuse model ############################################\n",
    "bnfuse_model = eval('models.pose_mobilenet_relu_bnfuse.get_pose_net')(\n",
    "    config, \n",
    "    stages_repeats, stages_out_channels,\n",
    "    is_train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-PCIE-32GB', total_memory=32510MB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpus = [int(i) for i in config.GPUS.split(',')]\n",
    "device = select_device(config.GPUS, batch_size=config.TEST.BATCH_SIZE*len(gpus))\n",
    "\n",
    "model = model.to(device)\n",
    "# summary(model,input_size=(3, 256, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_bits= 8 \tw_bits= 8 \tq_type= 0 \tq_level= 0 \tdevice= cuda:0 \tweight_observer= 0 \tbn_fuse= 1 \tquant_inference= False\n"
     ]
    }
   ],
   "source": [
    "#print('*******************ori_model*******************\\n', model)\n",
    "if(config.QUANTIZATION.QUANT_METHOD == 1): # DoReFa\n",
    "    quantize_dorefa.prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS, quant_inference=config.QUANTIZATION.QUANT_INFERENCE, is_activate=False)\n",
    "else: #default quant_method == 0   IAO\n",
    "    prepare(model, inplace=True, a_bits=config.QUANTIZATION.A_BITS, w_bits=config.QUANTIZATION.W_BITS,q_type=config.QUANTIZATION.Q_TYPE, q_level=config.QUANTIZATION.Q_LEVEL, device=device,#device=next(model.parameters()).device, \n",
    "                        weight_observer=config.QUANTIZATION.WEIGHT_OBSERVER, bn_fuse=config.QUANTIZATION.BN_FUSE, quant_inference=config.QUANTIZATION.QUANT_INFERENCE)\n",
    "#print('\\n*******************quant_model*******************\\n', model)\n",
    "# print('\\n*******************Using quant_model in test*******************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.TEST.MODEL_FILE:\n",
    "#     # logger.info('=> loading model from {}'.format(config.TEST.MODEL_FILE))\n",
    "#     if(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint.pth.tar'):\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         #model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=torch.device('cuda'))['state_dict'])\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device)['state_dict'])\n",
    "#         #torch.save(model.module.state_dict(), 'output/coco_quan/mobile_quant_relu_w8a8_bnfuse0/checkpoint_nomodule.pth.tar')\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='model_best.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     elif(config.TEST.MODEL_FILE.split('/')[-1]=='checkpoint_resave.pth.tar'):  #multiGPU has model.module.\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#     else:  #final_state.pth.tar\n",
    "#         model.load_state_dict(torch.load(config.TEST.MODEL_FILE,map_location=device))\n",
    "#         model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************For inference bn_fuse quant_model*******************\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "# ********************* quant_bn_fused_model_inference **********************\n",
    "model.to(device)\n",
    "model_bn_fuse(model, inplace=True)  # bn融合\n",
    "# print('\\n*******************For inference bn_fuse quant_model*******************\\n', model)\n",
    "# ckpt = {'model': model.module.state_dict() if hasattr(model, 'module') else model.state_dict()}\n",
    "# torch.save(ckpt, '../output/weights_quan/int8_mobilenet8_relu_bnfuse_inference.pt')\n",
    "print('*******************For inference bn_fuse quant_model*******************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../'+config.TEST.MODEL_FILE,map_location=device)['model'])  ##为什么还在'model'里面呀？\n",
    "# model = torch.nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model.state_dict:\n"
     ]
    }
   ],
   "source": [
    "remapped_state = {}\n",
    "print('Model.state_dict:')\n",
    "# ######################################## before #######################################\n",
    "# for n,param_tensor in enumerate(model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     # if(n<5):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     # print(model.state_dict()[param_tensor])\n",
    "\n",
    "# ######################################### after #######################################\n",
    "# for n,param_tensor in enumerate(bnfuse_model.state_dict()):\n",
    "#     #打印 key value字典\n",
    "#     print(n, param_tensor,'\\t',bnfuse_model.state_dict()[param_tensor].size())\n",
    "#     # if(n<4):\n",
    "#     #     print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "#     #     print(model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features.0.0.weight torch.Size([16, 3, 3, 3])\n",
      "1 features.0.0.bias torch.Size([16])\n",
      "2 features.1.conv.0.weight torch.Size([16, 1, 3, 3])\n",
      "3 features.1.conv.0.bias torch.Size([16])\n",
      "4 features.1.conv.2.weight torch.Size([8, 16, 1, 1])\n",
      "5 features.1.conv.2.bias torch.Size([8])\n",
      "6 features.2.conv.0.weight torch.Size([48, 8, 1, 1])\n",
      "7 features.2.conv.0.bias torch.Size([48])\n",
      "8 features.2.conv.2.weight torch.Size([48, 1, 3, 3])\n",
      "9 features.2.conv.2.bias torch.Size([48])\n",
      "10 features.2.conv.4.weight torch.Size([16, 48, 1, 1])\n",
      "11 features.2.conv.4.bias torch.Size([16])\n",
      "12 features.3.conv.0.weight torch.Size([96, 16, 1, 1])\n",
      "13 features.3.conv.0.bias torch.Size([96])\n",
      "14 features.3.conv.2.weight torch.Size([96, 1, 3, 3])\n",
      "15 features.3.conv.2.bias torch.Size([96])\n",
      "16 features.3.conv.4.weight torch.Size([16, 96, 1, 1])\n",
      "17 features.3.conv.4.bias torch.Size([16])\n",
      "18 features.4.conv.0.weight torch.Size([96, 16, 1, 1])\n",
      "19 features.4.conv.0.bias torch.Size([96])\n",
      "20 features.4.conv.2.weight torch.Size([96, 1, 3, 3])\n",
      "21 features.4.conv.2.bias torch.Size([96])\n",
      "22 features.4.conv.4.weight torch.Size([16, 96, 1, 1])\n",
      "23 features.4.conv.4.bias torch.Size([16])\n",
      "24 features.5.conv.0.weight torch.Size([96, 16, 1, 1])\n",
      "25 features.5.conv.0.bias torch.Size([96])\n",
      "26 features.5.conv.2.weight torch.Size([96, 1, 3, 3])\n",
      "27 features.5.conv.2.bias torch.Size([96])\n",
      "28 features.5.conv.4.weight torch.Size([16, 96, 1, 1])\n",
      "29 features.5.conv.4.bias torch.Size([16])\n",
      "30 features.6.conv.0.weight torch.Size([96, 16, 1, 1])\n",
      "31 features.6.conv.0.bias torch.Size([96])\n",
      "32 features.6.conv.2.weight torch.Size([96, 1, 3, 3])\n",
      "33 features.6.conv.2.bias torch.Size([96])\n",
      "34 features.6.conv.4.weight torch.Size([16, 96, 1, 1])\n",
      "35 features.6.conv.4.bias torch.Size([16])\n",
      "36 features.7.conv.0.weight torch.Size([96, 16, 1, 1])\n",
      "37 features.7.conv.0.bias torch.Size([96])\n",
      "38 features.7.conv.2.weight torch.Size([96, 1, 3, 3])\n",
      "39 features.7.conv.2.bias torch.Size([96])\n",
      "40 features.7.conv.4.weight torch.Size([32, 96, 1, 1])\n",
      "41 features.7.conv.4.bias torch.Size([32])\n",
      "42 features.8.conv.0.weight torch.Size([192, 32, 1, 1])\n",
      "43 features.8.conv.0.bias torch.Size([192])\n",
      "44 features.8.conv.2.weight torch.Size([192, 1, 3, 3])\n",
      "45 features.8.conv.2.bias torch.Size([192])\n",
      "46 features.8.conv.4.weight torch.Size([32, 192, 1, 1])\n",
      "47 features.8.conv.4.bias torch.Size([32])\n",
      "48 features.9.conv.0.weight torch.Size([192, 32, 1, 1])\n",
      "49 features.9.conv.0.bias torch.Size([192])\n",
      "50 features.9.conv.2.weight torch.Size([192, 1, 3, 3])\n",
      "51 features.9.conv.2.bias torch.Size([192])\n",
      "52 features.9.conv.4.weight torch.Size([32, 192, 1, 1])\n",
      "53 features.9.conv.4.bias torch.Size([32])\n",
      "54 features.10.conv.0.weight torch.Size([192, 32, 1, 1])\n",
      "55 features.10.conv.0.bias torch.Size([192])\n",
      "56 features.10.conv.2.weight torch.Size([192, 1, 3, 3])\n",
      "57 features.10.conv.2.bias torch.Size([192])\n",
      "58 features.10.conv.4.weight torch.Size([32, 192, 1, 1])\n",
      "59 features.10.conv.4.bias torch.Size([32])\n",
      "60 features.11.conv.0.weight torch.Size([192, 32, 1, 1])\n",
      "61 features.11.conv.0.bias torch.Size([192])\n",
      "62 features.11.conv.2.weight torch.Size([192, 1, 3, 3])\n",
      "63 features.11.conv.2.bias torch.Size([192])\n",
      "64 features.11.conv.4.weight torch.Size([48, 192, 1, 1])\n",
      "65 features.11.conv.4.bias torch.Size([48])\n",
      "66 features.12.conv.0.weight torch.Size([288, 48, 1, 1])\n",
      "67 features.12.conv.0.bias torch.Size([288])\n",
      "68 features.12.conv.2.weight torch.Size([288, 1, 3, 3])\n",
      "69 features.12.conv.2.bias torch.Size([288])\n",
      "70 features.12.conv.4.weight torch.Size([48, 288, 1, 1])\n",
      "71 features.12.conv.4.bias torch.Size([48])\n",
      "72 features.13.conv.0.weight torch.Size([288, 48, 1, 1])\n",
      "73 features.13.conv.0.bias torch.Size([288])\n",
      "74 features.13.conv.2.weight torch.Size([288, 1, 3, 3])\n",
      "75 features.13.conv.2.bias torch.Size([288])\n",
      "76 features.13.conv.4.weight torch.Size([48, 288, 1, 1])\n",
      "77 features.13.conv.4.bias torch.Size([48])\n",
      "78 features.14.conv.0.weight torch.Size([288, 48, 1, 1])\n",
      "79 features.14.conv.0.bias torch.Size([288])\n",
      "80 features.14.conv.2.weight torch.Size([288, 1, 3, 3])\n",
      "81 features.14.conv.2.bias torch.Size([288])\n",
      "82 features.14.conv.4.weight torch.Size([80, 288, 1, 1])\n",
      "83 features.14.conv.4.bias torch.Size([80])\n",
      "84 features.15.conv.0.weight torch.Size([480, 80, 1, 1])\n",
      "85 features.15.conv.0.bias torch.Size([480])\n",
      "86 features.15.conv.2.weight torch.Size([480, 1, 3, 3])\n",
      "87 features.15.conv.2.bias torch.Size([480])\n",
      "88 features.15.conv.4.weight torch.Size([80, 480, 1, 1])\n",
      "89 features.15.conv.4.bias torch.Size([80])\n",
      "90 features.16.conv.0.weight torch.Size([480, 80, 1, 1])\n",
      "91 features.16.conv.0.bias torch.Size([480])\n",
      "92 features.16.conv.2.weight torch.Size([480, 1, 3, 3])\n",
      "93 features.16.conv.2.bias torch.Size([480])\n",
      "94 features.16.conv.4.weight torch.Size([80, 480, 1, 1])\n",
      "95 features.16.conv.4.bias torch.Size([80])\n",
      "96 features.17.conv.0.weight torch.Size([480, 80, 1, 1])\n",
      "97 features.17.conv.0.bias torch.Size([480])\n",
      "98 features.17.conv.2.weight torch.Size([480, 1, 3, 3])\n",
      "99 features.17.conv.2.bias torch.Size([480])\n",
      "100 features.17.conv.4.weight torch.Size([160, 480, 1, 1])\n",
      "101 features.17.conv.4.bias torch.Size([160])\n",
      "102 conv2.0.weight torch.Size([128, 160, 1, 1])\n",
      "103 conv2.0.bias torch.Size([128])\n",
      "104 deconv_layers.0.weight torch.Size([128, 1, 4, 4])\n",
      "105 deconv_layers.0.bias torch.Size([128])\n",
      "106 deconv_layers.2.weight torch.Size([128, 128, 1, 1])\n",
      "107 deconv_layers.2.bias torch.Size([128])\n",
      "108 deconv_layers.4.weight torch.Size([128, 1, 4, 4])\n",
      "109 deconv_layers.4.bias torch.Size([128])\n",
      "110 deconv_layers.6.weight torch.Size([128, 128, 1, 1])\n",
      "111 deconv_layers.6.bias torch.Size([128])\n",
      "112 deconv_layers.8.weight torch.Size([128, 1, 4, 4])\n",
      "113 deconv_layers.8.bias torch.Size([128])\n",
      "114 deconv_layers.10.weight torch.Size([128, 128, 1, 1])\n",
      "115 deconv_layers.10.bias torch.Size([128])\n",
      "116 final_layer.weight torch.Size([17, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "remapped_state = {}\n",
    "for n,state_key in enumerate(bnfuse_model.state_dict().keys()):\n",
    "    k = state_key.split('.') # pytorch  ['features', '0', '0', 'weight']\n",
    "    if(k[0]!='final_layer'):\n",
    "        number = int(k[-2])//2*3 \n",
    "        # print(number)\n",
    "        k[-2]=str(number)\n",
    "        # print(k)\n",
    "        remapped_state_key=('.').join(k) #进行重映射\n",
    "    else: #final_layer\n",
    "        remapped_state_key=state_key\n",
    "    print(n, state_key, model.state_dict()[remapped_state_key].shape)\n",
    "    remapped_state[state_key]= model.state_dict()[remapped_state_key]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnfuse_model.load_state_dict(remapped_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {'model': bnfuse_model.module.state_dict() if hasattr(bnfuse_model, 'module') else bnfuse_model.state_dict()}\n",
    "torch.save(ckpt, '../output/weights_quan/float_mobilenetpose_nobn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model.state_dict:\n",
      "2 conv1.activation_quantizer.zero_point \t torch.Size([1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d655ebb9007f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(a.shape)  # [out_channel,in_channel,kernel_size,kernel_size] [8,3,3,3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#filename=('_').join(param_tensor.split('.')) + '.bin'  # module_list_0_0_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0_weight.bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#导出权重和偏置至二进制文件中\n",
    "result_path='scripts/exported_weights/'\n",
    "#result_path='scripts/before_bnfuse/'\n",
    "filename=''\n",
    "a=0.0\n",
    "print('Model.state_dict:')\n",
    "for n,param_tensor in enumerate(model.state_dict()):\n",
    "    #打印 key value字典\n",
    "    # print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "    if(n==2): #if(n<4*2):\n",
    "        print(n, param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "        a=model.state_dict()[param_tensor].numpy()  # module_list.0.0.weight   a:tensor->numpy dtype(float32) \n",
    "        #print(a.shape)  # [out_channel,in_channel,kernel_size,kernel_size] [8,3,3,3]\n",
    "        #filename=('_').join(param_tensor.split('.')) + '.bin'  # module_list_0_0_weight\n",
    "        print(filename)\n",
    "\n",
    "        if(param_tensor.split('.')[-1]=='weight'):\n",
    "            \n",
    "            # ############################################# needed: [outer for, ..., inner for] [kernel_size,kernel_size,out_channel,in_channel] [3,3,8,3]\n",
    "        elif(param_tensor.split('.')[-1]=='bias'):\n",
    "            a_reshape=a\n",
    "        else:\n",
    "            print(\"###################### WARNING ########################\\n\")\n",
    "            print(model.state_dict()[param_tensor])\n",
    "\n",
    "        print(a_reshape)\n",
    "        #a_reshape.astype(np.float32).tofile(result_path+filename+'.bin')\n",
    "        np.savetxt(result_path+filename+'.txt', a_reshape, fmt=\"%f\", delimiter='  ')\n",
    "        #b=np.loadtxt(result_path+filename, dtype=np.float32, delimiter='  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './0_weight.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-73bb369dc102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2_bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'3_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##一维的数据排列 ->[kernel_size1*kernel_size0*out_ch*in_ch]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#是weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './0_weight.bin'"
     ]
    }
   ],
   "source": [
    "###验证导出的权重是否正确\n",
    "######################以下是bnfuse后的权重，与bnfuse_model.pt对应######################\n",
    "###格式： [out_ch,in_ch,kernel_size1,kernel_size0]\n",
    "###第一个是普通卷积\n",
    "weight0_shape=[8,3,3,3]\n",
    "bias0_shape=[8]\n",
    "###接下来是深度可分离卷积\n",
    "weight1_shape=[8,8,1,1]\n",
    "bias1_shape=[8]\n",
    "weight2_shape=[8,1,3,3]\n",
    "bias2_shape=[8]\n",
    "weight3_shape=[4,8,1,1]\n",
    "bias3_shape=[4]\n",
    "\n",
    "list=[[8,3,3,3],[8],[8,8,1,1],[8],[8,1,3,3],[8],[4,8,1,1],[4]]\n",
    "\n",
    "#####################给的bin文件是权重重排后的结果######################\n",
    "###格式： [kernel_size1,kernel_size0,out_ch,in_ch]\n",
    "#以下可以将.bin文件中的权重返回.pt的数据排列，以便进行数据比对\n",
    "result_path='./'\n",
    "filename=['0_weight','0_bias','1_weight','1_bias','2_weight','2_bias','3_weight','3_bias']\n",
    "for i in range(4*2):\n",
    "    b=np.fromfile(result_path+filename[i]+'.bin',dtype=np.float32) ##一维的数据排列 ->[kernel_size1*kernel_size0*out_ch*in_ch]\n",
    "    if(i==1):\n",
    "        if(filename[i].split('_')[-1]=='weight'): #是weight\n",
    "            b=b.reshape(list[i][2]*list[i][3],list[i][0],list[i][1]) #->[kernel_size1*kernel_size0,out_ch,in_ch]\n",
    "            print(b.shape)\n",
    "            b=b.transpose(1,2,0) #->[out_ch,in_ch,kernel_size1*kernel_size0]\n",
    "            print(b.shape)\n",
    "            b=b.reshape(list[i][0],list[i][1],list[i][2],list[i][3]) #->[out_ch,in_ch,kernel_size1,kernel_size0]\n",
    "            print(b.shape)\n",
    "            print(b)\n",
    "        else: #否则是bias 无需转换，直接打印\n",
    "            print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 3)\n",
      "(9, 8, 3)\n",
      "(8, 3, 9)\n",
      "(8, 3, 3, 3)\n",
      "[[[[ 0.169188  0.189951  0.165524]\n",
      "   [ 0.15833   0.17425   0.18598 ]\n",
      "   [ 0.181422  0.20745   0.193455]]\n",
      "\n",
      "  [[ 0.127036  0.130175  0.13121 ]\n",
      "   [ 0.108236  0.123582  0.129688]\n",
      "   [ 0.135348  0.153904  0.160524]]\n",
      "\n",
      "  [[ 0.081796  0.095659  0.107188]\n",
      "   [ 0.101577  0.093322  0.11248 ]\n",
      "   [ 0.09426   0.111809  0.128836]]]\n",
      "\n",
      "\n",
      " [[[-0.560707 -0.90087  -0.724879]\n",
      "   [-0.370212 -0.85396  -0.942397]\n",
      "   [-0.189665 -0.430859 -0.813114]]\n",
      "\n",
      "  [[ 0.23858  -0.06389  -0.333687]\n",
      "   [ 0.172932 -0.257281 -0.48805 ]\n",
      "   [ 0.045519 -0.056772 -0.272473]]\n",
      "\n",
      "  [[ 0.176914  0.34184   0.036285]\n",
      "   [-0.111699  0.024917 -0.026201]\n",
      "   [-0.267103  0.064347  0.139831]]]\n",
      "\n",
      "\n",
      " [[[-0.286476 -0.299834 -0.234171]\n",
      "   [-0.238944 -0.192332 -0.181511]\n",
      "   [-0.090237  0.026358 -0.08187 ]]\n",
      "\n",
      "  [[-0.115784 -0.009535 -0.043332]\n",
      "   [ 0.017743  0.077202  0.13005 ]\n",
      "   [ 0.133964  0.113496  0.13871 ]]\n",
      "\n",
      "  [[-0.151784 -0.026175 -0.001373]\n",
      "   [ 0.069346  0.049721 -0.005715]\n",
      "   [ 0.032771  0.093534  0.071089]]]\n",
      "\n",
      "\n",
      " [[[ 0.65646   0.070615  0.447241]\n",
      "   [ 0.301191 -0.09385  -0.313102]\n",
      "   [ 0.161458  0.124143 -0.193338]]\n",
      "\n",
      "  [[-0.621784 -1.017329 -0.557274]\n",
      "   [-0.245178 -0.779021 -0.832824]\n",
      "   [ 0.253458 -0.009495 -0.080375]]\n",
      "\n",
      "  [[-0.519809 -0.735669 -0.466487]\n",
      "   [-0.314337 -0.520856 -0.483639]\n",
      "   [-0.043888 -0.139392 -0.081211]]]\n",
      "\n",
      "\n",
      " [[[ 0.408777  0.472209  0.443688]\n",
      "   [ 0.306948  0.392579  0.456176]\n",
      "   [ 0.114989  0.170054  0.325307]]\n",
      "\n",
      "  [[ 0.01624  -0.006682 -0.085873]\n",
      "   [-0.167402 -0.108801 -0.085709]\n",
      "   [-0.221746 -0.151885  0.015598]]\n",
      "\n",
      "  [[ 0.231438  0.244641  0.02627 ]\n",
      "   [ 0.494688  0.516982  0.268226]\n",
      "   [ 0.490856  0.513896  0.341222]]]\n",
      "\n",
      "\n",
      " [[[-0.056248 -0.007535  0.31402 ]\n",
      "   [-0.194984 -0.385092 -0.102816]\n",
      "   [-0.095269 -0.32088  -0.698012]]\n",
      "\n",
      "  [[-0.17167  -0.099648  0.279882]\n",
      "   [-0.260135 -0.503774 -0.089205]\n",
      "   [ 0.071912 -0.286569 -0.411847]]\n",
      "\n",
      "  [[-0.132016  0.007374  0.252612]\n",
      "   [-0.256214 -0.306343  0.04508 ]\n",
      "   [-0.030838 -0.118747 -0.175867]]]\n",
      "\n",
      "\n",
      " [[[-0.128127 -0.117846 -0.197385]\n",
      "   [ 0.07253   0.101487 -0.012231]\n",
      "   [ 0.266724  0.304413  0.260061]]\n",
      "\n",
      "  [[ 0.556348  0.5401    0.51026 ]\n",
      "   [ 0.70099   0.728856  0.620042]\n",
      "   [ 0.583963  0.620709  0.501264]]\n",
      "\n",
      "  [[ 0.03853   0.003123  0.18667 ]\n",
      "   [-0.230583 -0.172574  0.027114]\n",
      "   [-0.34658  -0.271247 -0.040977]]]\n",
      "\n",
      "\n",
      " [[[ 0.001119  0.099222  0.115309]\n",
      "   [-0.248692 -0.109929 -0.195281]\n",
      "   [-0.196144 -0.241387 -0.011245]]\n",
      "\n",
      "  [[-0.200019 -0.009245  0.03444 ]\n",
      "   [ 0.045661 -0.200817 -0.145634]\n",
      "   [-0.168949  0.058152 -0.095233]]\n",
      "\n",
      "  [[-0.00873  -0.028173 -0.041559]\n",
      "   [-0.150937 -0.158199 -0.062219]\n",
      "   [-0.160255 -0.121503 -0.034715]]]]\n"
     ]
    }
   ],
   "source": [
    "#b=np.fromfile(result_path+filename+'.bin',dtype=np.float32)\n",
    "b=np.loadtxt(result_path+filename+'.txt', dtype=np.float32, delimiter='  ')\n",
    "print(b.shape)\n",
    "b=b.reshape(9,8,3)\n",
    "print(b.shape)\n",
    "b=b.transpose(1,2,0) #[9,8,3]->[8,3,9]\n",
    "print(b.shape)\n",
    "b=b.reshape(8,3,3,3)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "# print(a.shape)\n",
    "# print(a.shape[0],a.shape[1],a.shape[2],a.shape[3])\n",
    "# #a_resize=a.reshape([8,3,-1])\n",
    "# a_reshape=a.reshape([a.shape[0],a.shape[1],-1])\n",
    "# print(a_reshape.shape)\n",
    "# a_reshape=a_reshape.transpose(2,0,1) #[8,3,9]->[9,8,3]\n",
    "# print(a_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.reshape(8,3,9).transpose(2,1,0).shape #一次可以置换三个\n",
    "#a.swapaxes(1,2)  #swapaxes只能两两置换 对于swapaxes来说，括号内的两参数，交换位置和不交换，实际结果相同。\n",
    "\n",
    "# import cv2\n",
    "# image=cv2.imread('/home/ytwang/dataset/VOC/images/val/000001.jpg')\n",
    "# cv2.imshow(\"image\",image)\n",
    "# cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 352, 256])\n",
      "(3, 352, 256)\n",
      "(352, 256, 3)\n",
      "(90112, 3)\n"
     ]
    }
   ],
   "source": [
    "##################################### 权重、feature map导出 ##################################\n",
    "x0=torch.randn(1,3,352,256)\n",
    "print(x0.shape) #[1,3,352,256]\n",
    "x=x0.numpy().squeeze()\n",
    "print(x.shape) #(3, 352, 256)\n",
    "x=x.transpose(1,2,0)\n",
    "print(x.shape) #(352,256,3)\n",
    "#x.astype(np.float32).tofile(result_path+'input000001_352x256.bin') # 二进制文件导出\n",
    "x=x.reshape([-1,x.shape[-1]]) #(90112,3)=(352*256,3)\n",
    "print(x.shape)\n",
    "#np.savetxt(result_path+'input.txt', x, fmt=\"%f\", delimiter=',\\t') # .txt文件导出\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./input.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5952c7f42c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# b=torch.tensor(b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# type(b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m',\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ./input.txt not found."
     ]
    }
   ],
   "source": [
    "#b=np.fromfile(result_path+'input000001_352x256.bin',dtype=np.float32)\n",
    "# b=np.loadtxt(result_path+'input.txt', dtype=np.float32, delimiter=',\\t')\n",
    "# print(b.shape)\n",
    "# b=b.reshape(352,256,3).transpose(2,0,1)\n",
    "# print(b.shape)\n",
    "# b=b.reshape(1,3,352,256)\n",
    "# print(b.shape)\n",
    "# b=torch.tensor(b)\n",
    "# type(b)\n",
    "b=np.loadtxt(result_path+'input.txt', dtype=np.float32, delimiter=',\\t')\n",
    "print(b.shape)\n",
    "print(x0.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 352, 256)\n",
      "(1, 3, 352, 256)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "b=b.reshape(x0.shape[2],x0.shape[3],x0.shape[1]).transpose(2,0,1)\n",
    "print(b.shape)\n",
    "b=np.expand_dims(b,0) #[3,352,256]->[1,3,352,256]\n",
    "print(b.shape)\n",
    "x=torch.tensor(b)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.81406e-01, -8.01516e-01,  3.28198e-01, -7.11460e-01, -9.71156e-01, -2.00248e-02, -1.63450e+00,  9.71171e-01, -7.00366e-01,  1.60246e+00, -1.78841e+00,  3.63713e-01,  9.74244e-01,  1.33411e+00, -3.02220e-01, -4.06555e-01,  1.37745e-01, -6.31540e-01, -1.46539e+00,  2.76620e-01,  1.23277e+00, -1.40946e+00,\n",
      "        -1.51452e+00,  1.19671e-01,  4.42624e-01, -7.22908e-01, -4.04740e-01, -4.34604e-01,  2.06584e-01, -1.40587e+00,  5.70870e-01,  1.07140e-01,  1.61050e+00,  1.02770e+00, -3.53327e-01,  2.04177e-01,  2.31896e-02,  1.28927e+00, -5.48752e-01,  1.42098e-01,  9.03911e-01, -1.20936e+00,  9.63919e-01, -1.01060e+00,\n",
      "         8.54119e-02, -1.53845e-01, -1.54839e+00, -6.02410e-02, -2.57630e+00,  3.93433e-01,  2.39229e-01, -1.21564e-01, -1.15327e+00,  1.26149e+00,  1.45040e+00, -9.07547e-01,  1.31109e+00, -1.65926e-02, -3.07028e-01,  6.83296e-01, -5.43891e-01,  6.85024e-01,  4.69522e-01,  1.76420e+00,  1.61510e+00,  1.25916e+00,\n",
      "        -1.77150e-01,  2.47641e+00, -1.46249e+00, -2.80018e-01,  6.05029e-02,  5.44458e-01,  2.67465e+00, -1.15849e+00, -5.81707e-01,  4.31597e-01,  2.43441e+00,  2.65851e-01, -1.14425e+00,  2.25560e+00,  1.77140e+00, -7.80207e-01,  6.53952e-02, -4.71407e-01,  1.57048e+00, -9.17719e-01,  8.72730e-01,  9.31447e-01,\n",
      "         1.77649e+00,  1.00942e+00, -7.18745e-01, -2.77709e-01,  4.57021e-01, -2.23519e-01,  2.16158e-01, -1.34371e+00, -4.63412e-01,  1.40616e+00, -8.71267e-01, -2.05477e+00,  1.82690e+00, -9.64664e-02, -1.37564e+00, -3.15296e-02,  5.58746e-01, -1.83691e-01, -6.04123e-01,  7.66482e-01, -4.03121e-01,  2.93691e-01,\n",
      "         8.41161e-01,  2.42523e+00,  6.69548e-01,  1.93673e-01, -1.15198e+00,  4.01391e-01,  8.17872e-02,  1.89599e-01, -6.67637e-01,  1.04472e+00,  2.86891e-01,  8.68945e-01, -1.60576e+00, -3.06741e-04,  2.12187e+00,  4.09882e-01, -1.14038e+00, -1.76595e+00, -1.55876e-01, -1.18096e+00, -4.73746e-02,  1.64630e-01,\n",
      "        -6.96033e-01,  1.18123e+00,  2.55200e+00, -1.55258e+00, -5.07378e-02,  5.74990e-02, -3.31153e-01,  4.79517e-01,  8.98099e-02,  1.49721e+00,  4.27276e-02,  2.24228e+00,  9.64584e-01,  1.06729e+00,  1.11198e+00,  1.89691e+00, -1.34978e+00,  1.04962e+00,  1.25740e+00,  4.83465e-01,  9.00648e-01,  6.93612e-01,\n",
      "        -1.32085e+00,  6.34273e-01,  7.02246e-02,  3.29396e-01,  1.19658e+00, -3.61629e-01,  1.82372e+00, -8.15059e-01,  5.58395e-02, -1.26954e+00, -2.69953e-01, -6.87627e-01, -8.86588e-01,  6.98368e-01, -1.29626e+00, -6.38218e-01,  6.82183e-01, -4.30375e-01, -2.36423e+00,  4.34401e-01,  1.94347e+00, -1.14078e-01,\n",
      "         4.15629e-01,  9.99400e-01,  9.41885e-01,  5.68806e-01, -2.00598e-01,  7.16138e-01, -9.69121e-01, -1.51415e-01,  4.58058e-01,  6.32497e-01,  1.45279e+00, -2.12141e-01,  8.35680e-01,  6.50394e-01, -5.15632e-01, -5.69040e-01,  3.28715e-01,  1.17557e+00,  1.30806e+00,  1.56861e+00, -2.23972e+00,  4.16258e-01,\n",
      "         4.63714e-02, -3.72965e-01, -8.45543e-01, -8.86618e-01,  2.44351e-01,  9.09960e-01, -3.21582e-01, -1.45642e+00, -1.95657e+00, -3.57767e-01, -1.44434e+00,  8.81572e-01,  3.95041e-01,  7.41678e-01,  1.47994e+00,  7.90388e-01, -6.65410e-01, -3.59384e-01,  6.13578e-01,  8.22294e-01, -5.26904e-01,  8.14695e-01,\n",
      "        -3.81759e-01,  2.36013e+00, -3.96305e-01,  1.66977e+00, -2.11435e-01,  5.07760e-01, -3.10217e-01,  4.52953e-01, -7.71852e-01, -3.66158e-01,  3.17449e-01, -2.57260e+00, -8.96794e-01,  9.62500e-02,  1.30173e+00, -7.75847e-01, -1.38639e+00, -5.68825e-01,  4.10178e-01,  5.41091e-01, -1.68622e-02,  6.18837e-01,\n",
      "        -8.45662e-01, -6.51488e-01, -6.27133e-01,  7.43690e-01,  2.00255e-01, -3.11761e-01,  1.16924e+00,  1.51702e+00,  1.01467e+00, -2.29272e+00, -5.90515e-01, -6.66745e-01,  1.29226e+00, -2.55041e+00])\n"
     ]
    }
   ],
   "source": [
    "# (x0==b).all()\n",
    "print(x0[0][0][0]) #torch.Size([1, 3, 352, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### 验证bnfuse后结果是否正确 #########################################\n",
    "bn_weight=torch.tensor([\n",
    "    0.9627532362937927,\n",
    "    1.6811193227767944,\n",
    "    1.0011911392211914,\n",
    "    1.6239477396011353,\n",
    "    1.3853583335876465,\n",
    "    0.9575594663619995,\n",
    "    1.325505256652832,\n",
    "    0.9901668429374695])\n",
    "bn_bias=torch.tensor([\n",
    "    -24.751123428344727,\n",
    "    16.553693771362305,\n",
    "    -21.58427619934082,\n",
    "    13.835719108581543,\n",
    "    7.6066741943359375,\n",
    "    12.691370010375977,\n",
    "    24.215431213378906,\n",
    "    -10.96066665649414])\n",
    "bn_running_mean=torch.tensor([\n",
    "    0.2264288365840912,\n",
    "    -1.438944935798645,\n",
    "    -0.012756695970892906,\n",
    "    -1.1514368057250977,\n",
    "    2.064279079437256,\n",
    "    -1.000322699546814,\n",
    "    1.4272160530090332,\n",
    "    -0.01157035119831562])\n",
    "bn_running_var=torch.tensor([\n",
    "    0.018602905794978142,\n",
    "    0.7236471176147461,\n",
    "    0.0006818491965532303,\n",
    "    0.586725652217865,\n",
    "    1.5904737710952759,\n",
    "    0.3611223101615906,\n",
    "    0.7421475648880005,\n",
    "    0.00004840613837586716])\n",
    "bn_eps=torch.tensor([1E-4]).expand([8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-26.34514,  19.39717, -21.12751,  16.27666,   5.33914,  14.28511,  22.01961, -10.02023])\n",
      "tensor([-26.34514,  19.39717, -21.12751,  16.27666,   5.33914,  14.28511,  22.01961, -10.02023])\n"
     ]
    }
   ],
   "source": [
    "# bn.bias=\n",
    "# bn.weight=\n",
    "# bn.running_mean=\n",
    "# bn.running_var=\n",
    "# bn.eps=\n",
    "afterfuse_b=bn_bias-bn_weight*bn_running_mean/(torch.sqrt(bn_running_var+bn_eps))\n",
    "print(afterfuse_b)\n",
    "\n",
    "b_bn = bn_bias - bn_weight.mul(bn_running_mean).div(torch.sqrt(bn_running_var + bn_eps))\n",
    "print(b_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### torch_utils.py中的bu_fuse实现 ##########################\n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "    # https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
    "    with torch.no_grad():\n",
    "        # init\n",
    "        fusedconv = torch.nn.Conv2d(conv.in_channels,\n",
    "                                    conv.out_channels,\n",
    "                                    kernel_size=conv.kernel_size,\n",
    "                                    stride=conv.stride,\n",
    "                                    padding=conv.padding,\n",
    "                                    groups=conv.groups,\n",
    "                                    bias=True)\n",
    "\n",
    "        # prepare filters\n",
    "        w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "        fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.size()))\n",
    "\n",
    "        # prepare spatial bias\n",
    "        if conv.bias is not None:\n",
    "            b_conv = conv.bias\n",
    "        else:\n",
    "            b_conv = torch.zeros(conv.weight.size(0))\n",
    "        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "        fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
    "\n",
    "        return fusedconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python383jvsc74a57bd0aab342d38c9089b27086ef2ff31e6c6863533cd1c8642810b45a542962dd3699"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 5
}