GPUS: '0,1'
DATA_DIR: ''
OUTPUT_DIR: 'output'
LOG_DIR: 'log'
WORKERS: 8
PRINT_FREQ: 100

DATASET:
  DATASET: 'coco'
  ROOT: '/home/ytwang/dataset/COCO2017'
  TEST_SET: 'val2017'
  TRAIN_SET: 'train2017'
  FLIP: true
  ROT_FACTOR: 40
  SCALE_FACTOR: 0.3
MODEL:
  NAME: 'pose_mobilenet_relu' #'pose_resnet' 
  PRETRAINED: 'output/coco_quan/mobile_quant_relu_lsq_w4a8_bnfuse0/final_state_280.pth.tar' #'output/coco/256x192_d256x3_adam_lr1e-3_mobile8_relu/final_state_560.pth.tar'  output/coco_quan/mobile_quant_relu_w8a8_bnfuse0_resume/final_state.pth.tar
  IMAGE_SIZE:                                                                           
  - 192
  - 256
  NUM_JOINTS: 17
  EXTRA:
    TARGET_TYPE: 'gaussian'
    HEATMAP_SIZE:
    - 48
    - 64
    SIGMA: 2
    FINAL_CONV_KERNEL: 1
    DECONV_WITH_BIAS: false
    NUM_DECONV_LAYERS: 3
    NUM_DECONV_FILTERS:
    - 128
    - 128
    - 128
    NUM_DECONV_KERNELS:
    - 4
    - 4
    - 4
    NUM_LAYERS: 50
LOSS:
  USE_TARGET_WEIGHT: true
TRAIN:
  BATCH_SIZE: 256
  SHUFFLE: true
  BEGIN_EPOCH: 0
  END_EPOCH: 140
  RESUME: false
  OPTIMIZER: 'adam'
  LR: 0.001
  LR_FACTOR: 0.1
  LR_STEP:
  - 90
  - 120 
  WD: 0.0001
  GAMMA1: 0.99
  GAMMA2: 0.0
  MOMENTUM: 0.9
  NESTEROV: false
TEST:
  BATCH_SIZE: 128
  COCO_BBOX_FILE: 'models/person_detection_results/COCO_val2017_detections_AP_H_56_person.json'
  BBOX_THRE: 1.0
  FLIP_TEST: false
  IMAGE_THRE: 0.0
  IN_VIS_THRE: 0.2
  MODEL_FILE: 'output/coco_quan/mobile_quant_relu_lsq_w4a8_bnfuse0/model_best.pth.tar'
  NMS_THRE: 1.0
  OKS_THRE: 0.9
  USE_GT_BBOX: true
DEBUG:
  DEBUG: true
  SAVE_BATCH_IMAGES_GT: true
  SAVE_BATCH_IMAGES_PRED: true
  SAVE_HEATMAPS_GT: true
  SAVE_HEATMAPS_PRED: true
QUANTIZATION:
  W_BITS: 4
  A_BITS: 8
  BN_FUSE: 0
  Q_TYPE: 0
  Q_LEVEL: 0
  WEIGHT_OBSERVER: 0
  QUANT_INFERENCE: false
  QUANT_METHOD: 2
LSQACT: # (default for all layers)
  # Quantizer type (choices: lsq)
  mode: lsq
  # Bit width of quantized activation
  bit: 8
  # Each output channel uses its own scaling factor
  per_channel: false
  # Whether to use symmetric quantization
  symmetric: false
  # Quantize all the numbers to non-negative
  all_positive: true
LSQWEIGHT: # (default for all layers)
  # Quantizer type (choices: lsq)
  mode: lsq
  # Bit width of quantized weight
  bit: 4
  # Each output channel uses its own scaling factor
  per_channel: true
  # Whether to use symmetric quantization
  symmetric: false
  # Whether to quantize all the numbers to non-negative
  all_positive: false
LSQEXPECTS:
  # Specify quantized bit width for some layers, like this:
  # features.0.0:
  #   act:
  #     bit:
  #     all_positive: false
  #   weight:
  #     bit:
  fc:
  #   act:
  #     bit:
  #   weight:
  #     bit:
